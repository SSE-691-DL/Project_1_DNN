{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup\n",
    "The following cell contains the necessary imports for the code present on this page. It also verifies that tensorflow is imported with the necessary version. the dataset to be used in the later cells is imported here as well. NOTE: Much of the code on this page is unnecessarily repeated. This was done for my benefit in learning the methodology of this process, as well as allowing for quick references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.8.0\n",
      "Extracting data\\train-images-idx3-ubyte.gz\n",
      "Extracting data\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\t10k-labels-idx1-ubyte.gz\n",
      "Setup successful!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "import tensorflow as tf\n",
    "import tensorflowvisu\n",
    "import math\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "print(\"Tensorflow version: \" + tf.__version__)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# download images and labels into mnist.test and mnist.train\n",
    "mnist = mnist_data.read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n",
    "\n",
    "print(\"Setup successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Notes for Code in Cell Below\n",
    "The following notes are related to code in the cell shown below. It uses a single layer neural network. With 2000 iterations, it maxes out with an accuracy of ~92%.\n",
    "\n",
    "## The Data\n",
    "The data consists of images. 60k images and labels are included for the training data. 10k images and labels are included for the test data.\n",
    "\n",
    "## The Model\n",
    "The model, represented by the function 'Y = softmax(X * W + b),' is described as follows:\n",
    "* X: matrix for 100 grayscale images of 28x28 pixels (100 images in a mini-batch)\n",
    "* W: weight matrix with 784 lines and 10 columns\n",
    "* b: bias vector with 10 dimensions\n",
    "* +: add with broadcasting -> adds the vector to each line of the matrix (numpy)\n",
    "* Y: output matrix with 100 lines and 10 columns\n",
    "Additional notes:\n",
    "* 'None' in X's definition corresponds to the number of images in the mini-batch -> known at training time\n",
    "* 28, 28, 1 corresponds to 28x28 greyscale images (color would be 3 for rgb)\n",
    "* softmax(matrix) applies softmax to each line\n",
    "* softmax(line) applies an exponential to each value and divides by the normal of the resulting line\n",
    "\n",
    "## The Loss Function\n",
    "The loss function used in the next example is cross-entropy, which is represented by 'CE = -sum(Y_i * log(Yi)).' It is defined as follows:\n",
    "* Y : computed output vector\n",
    "* Y_: desired output vector\n",
    "Additional notes:\n",
    "* log takes the log of each element\n",
    "* \\* multiplies the tensors element-by-element\n",
    "* reduce_mean adds all components in the tensor (sums all elements in vector)\n",
    "It results in the total cross-entropy for all images in a batch normalized for batches of 100 images multiplied by 10 because 'mean' included an unwanted division by 10.\n",
    "\n",
    "## Other Notes\n",
    "* in reshape, -1 is used to say \"the only possible dimension that will preserve the number of elements\"\n",
    "* tensorflow magic occurs with the optimizer used to minimize to cross-entropy loss. the gradient descent used here computes the formal partial derivatives of the loss function relative to the weights and biases (gradient). numerical derivation would be too time consuming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.12 loss: 230.25854\n",
      "0: ********* epoch 1 ********* test accuracy:0.098 test loss: 230.25717\n",
      "10: accuracy:0.81 loss: 101.18049\n",
      "20: accuracy:0.84 loss: 75.85964\n",
      "30: accuracy:0.81 loss: 66.60701\n",
      "40: accuracy:0.86 loss: 55.985138\n",
      "50: accuracy:0.87 loss: 52.318764\n",
      "50: ********* epoch 1 ********* test accuracy:0.8759 test loss: 50.732784\n",
      "60: accuracy:0.89 loss: 51.026382\n",
      "70: accuracy:0.88 loss: 53.434334\n",
      "80: accuracy:0.82 loss: 54.094975\n",
      "90: accuracy:0.88 loss: 50.260353\n",
      "100: accuracy:0.95 loss: 31.7305\n",
      "100: ********* epoch 1 ********* test accuracy:0.8909 test loss: 42.32413\n",
      "110: accuracy:0.89 loss: 47.37336\n",
      "120: accuracy:0.86 loss: 56.214287\n",
      "130: accuracy:0.93 loss: 31.851784\n",
      "140: accuracy:0.9 loss: 36.476578\n",
      "150: accuracy:0.84 loss: 44.125298\n",
      "150: ********* epoch 1 ********* test accuracy:0.8915 test loss: 39.337486\n",
      "160: accuracy:0.91 loss: 34.39393\n",
      "170: accuracy:0.86 loss: 50.572987\n",
      "180: accuracy:0.9 loss: 45.449863\n",
      "190: accuracy:0.86 loss: 49.707714\n",
      "200: accuracy:0.9 loss: 44.937355\n",
      "200: ********* epoch 1 ********* test accuracy:0.8971 test loss: 37.455647\n",
      "210: accuracy:0.9 loss: 34.24408\n",
      "220: accuracy:0.9 loss: 40.072956\n",
      "230: accuracy:0.85 loss: 49.187416\n",
      "240: accuracy:0.88 loss: 39.75328\n",
      "250: accuracy:0.92 loss: 29.214096\n",
      "250: ********* epoch 1 ********* test accuracy:0.9033 test loss: 34.79932\n",
      "260: accuracy:0.9 loss: 33.67328\n",
      "270: accuracy:0.83 loss: 55.441833\n",
      "280: accuracy:0.84 loss: 53.22944\n",
      "290: accuracy:0.88 loss: 32.1522\n",
      "300: accuracy:0.91 loss: 35.373905\n",
      "300: ********* epoch 1 ********* test accuracy:0.9056 test loss: 34.269802\n",
      "310: accuracy:0.93 loss: 34.841175\n",
      "320: accuracy:0.88 loss: 35.98198\n",
      "330: accuracy:0.92 loss: 35.70182\n",
      "340: accuracy:0.91 loss: 39.054893\n",
      "350: accuracy:0.96 loss: 20.202013\n",
      "350: ********* epoch 1 ********* test accuracy:0.9078 test loss: 33.78371\n",
      "360: accuracy:0.96 loss: 20.476528\n",
      "370: accuracy:0.93 loss: 32.635006\n",
      "380: accuracy:0.94 loss: 28.927212\n",
      "390: accuracy:0.9 loss: 32.126877\n",
      "400: accuracy:0.88 loss: 41.069893\n",
      "400: ********* epoch 1 ********* test accuracy:0.9063 test loss: 33.348007\n",
      "410: accuracy:0.89 loss: 38.20215\n",
      "420: accuracy:0.91 loss: 29.649961\n",
      "430: accuracy:0.9 loss: 33.101852\n",
      "440: accuracy:0.88 loss: 38.064533\n",
      "450: accuracy:0.92 loss: 41.69471\n",
      "450: ********* epoch 1 ********* test accuracy:0.9092 test loss: 32.480392\n",
      "460: accuracy:0.92 loss: 23.180128\n",
      "470: accuracy:0.88 loss: 53.51729\n",
      "480: accuracy:0.92 loss: 32.532227\n",
      "490: accuracy:0.85 loss: 41.45823\n",
      "500: accuracy:0.93 loss: 24.051704\n",
      "500: ********* epoch 1 ********* test accuracy:0.9078 test loss: 32.349926\n",
      "510: accuracy:0.9 loss: 31.264677\n",
      "520: accuracy:0.87 loss: 41.59049\n",
      "530: accuracy:0.93 loss: 27.467236\n",
      "540: accuracy:0.91 loss: 28.64323\n",
      "550: accuracy:0.92 loss: 23.996864\n",
      "550: ********* epoch 1 ********* test accuracy:0.9098 test loss: 32.090973\n",
      "560: accuracy:0.96 loss: 21.999298\n",
      "570: accuracy:0.86 loss: 34.59623\n",
      "580: accuracy:0.87 loss: 34.84525\n",
      "590: accuracy:0.85 loss: 43.740868\n",
      "600: accuracy:0.93 loss: 34.40101\n",
      "600: ********* epoch 2 ********* test accuracy:0.9142 test loss: 31.330317\n",
      "610: accuracy:0.93 loss: 24.154343\n",
      "620: accuracy:0.88 loss: 33.405834\n",
      "630: accuracy:0.89 loss: 36.126945\n",
      "640: accuracy:0.9 loss: 25.929548\n",
      "650: accuracy:0.89 loss: 34.535194\n",
      "650: ********* epoch 2 ********* test accuracy:0.9133 test loss: 31.318386\n",
      "660: accuracy:0.87 loss: 43.05815\n",
      "670: accuracy:0.93 loss: 21.619083\n",
      "680: accuracy:0.92 loss: 34.42497\n",
      "690: accuracy:0.9 loss: 35.046448\n",
      "700: accuracy:0.91 loss: 34.203495\n",
      "700: ********* epoch 2 ********* test accuracy:0.9126 test loss: 31.627514\n",
      "710: accuracy:0.95 loss: 14.8338375\n",
      "720: accuracy:0.91 loss: 32.22488\n",
      "730: accuracy:0.94 loss: 22.470718\n",
      "740: accuracy:0.87 loss: 34.003525\n",
      "750: accuracy:0.93 loss: 31.919214\n",
      "750: ********* epoch 2 ********* test accuracy:0.9144 test loss: 30.921417\n",
      "760: accuracy:0.86 loss: 46.265133\n",
      "770: accuracy:0.93 loss: 25.793056\n",
      "780: accuracy:0.95 loss: 23.774126\n",
      "790: accuracy:0.86 loss: 43.82674\n",
      "800: accuracy:0.88 loss: 42.755707\n",
      "800: ********* epoch 2 ********* test accuracy:0.9141 test loss: 30.475918\n",
      "810: accuracy:0.92 loss: 21.093626\n",
      "820: accuracy:0.91 loss: 26.410477\n",
      "830: accuracy:0.88 loss: 39.898636\n",
      "840: accuracy:0.93 loss: 36.708805\n",
      "850: accuracy:0.91 loss: 32.663914\n",
      "850: ********* epoch 2 ********* test accuracy:0.9171 test loss: 30.163036\n",
      "860: accuracy:0.95 loss: 18.653717\n",
      "870: accuracy:0.92 loss: 27.929197\n",
      "880: accuracy:0.94 loss: 20.24691\n",
      "890: accuracy:0.92 loss: 31.679798\n",
      "900: accuracy:0.92 loss: 23.961227\n",
      "900: ********* epoch 2 ********* test accuracy:0.9165 test loss: 29.759434\n",
      "910: accuracy:0.9 loss: 33.91369\n",
      "920: accuracy:0.94 loss: 25.885294\n",
      "930: accuracy:0.87 loss: 35.694603\n",
      "940: accuracy:0.91 loss: 36.651833\n",
      "950: accuracy:0.92 loss: 28.091667\n",
      "950: ********* epoch 2 ********* test accuracy:0.9135 test loss: 30.66754\n",
      "960: accuracy:0.89 loss: 33.97444\n",
      "970: accuracy:0.92 loss: 35.776375\n",
      "980: accuracy:0.9 loss: 33.949516\n",
      "990: accuracy:0.94 loss: 17.221348\n",
      "1000: accuracy:0.93 loss: 20.124361\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9156 test loss: 29.843472\n",
      "1010: accuracy:0.94 loss: 31.36982\n",
      "1020: accuracy:0.92 loss: 28.181843\n",
      "1030: accuracy:0.95 loss: 24.770996\n",
      "1040: accuracy:0.86 loss: 40.297447\n",
      "1050: accuracy:0.92 loss: 41.601776\n",
      "1050: ********* epoch 2 ********* test accuracy:0.9175 test loss: 29.48482\n",
      "1060: accuracy:0.95 loss: 18.646503\n",
      "1070: accuracy:0.88 loss: 39.2088\n",
      "1080: accuracy:0.83 loss: 56.28798\n",
      "1090: accuracy:0.92 loss: 27.610415\n",
      "1100: accuracy:0.9 loss: 26.36631\n",
      "1100: ********* epoch 2 ********* test accuracy:0.9186 test loss: 29.171213\n",
      "1110: accuracy:0.89 loss: 44.09578\n",
      "1120: accuracy:0.9 loss: 32.910255\n",
      "1130: accuracy:0.95 loss: 12.9239435\n",
      "1140: accuracy:0.87 loss: 38.931866\n",
      "1150: accuracy:0.92 loss: 28.9999\n",
      "1150: ********* epoch 2 ********* test accuracy:0.92 test loss: 29.160574\n",
      "1160: accuracy:0.95 loss: 22.975788\n",
      "1170: accuracy:0.92 loss: 28.654774\n",
      "1180: accuracy:0.93 loss: 22.112446\n",
      "1190: accuracy:0.91 loss: 32.422443\n",
      "1200: accuracy:0.96 loss: 17.68002\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9177 test loss: 29.114847\n",
      "1210: accuracy:0.96 loss: 18.063156\n",
      "1220: accuracy:0.89 loss: 44.434113\n",
      "1230: accuracy:0.92 loss: 25.972446\n",
      "1240: accuracy:0.91 loss: 40.533905\n",
      "1250: accuracy:0.92 loss: 28.855715\n",
      "1250: ********* epoch 3 ********* test accuracy:0.9169 test loss: 29.476416\n",
      "1260: accuracy:0.89 loss: 27.42957\n",
      "1270: accuracy:0.89 loss: 31.855635\n",
      "1280: accuracy:0.96 loss: 20.361889\n",
      "1290: accuracy:0.93 loss: 25.496542\n",
      "1300: accuracy:0.87 loss: 38.979244\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9157 test loss: 29.619179\n",
      "1310: accuracy:0.92 loss: 39.090572\n",
      "1320: accuracy:0.93 loss: 30.637066\n",
      "1330: accuracy:0.92 loss: 31.63276\n",
      "1340: accuracy:0.93 loss: 26.94212\n",
      "1350: accuracy:0.9 loss: 27.000185\n",
      "1350: ********* epoch 3 ********* test accuracy:0.9189 test loss: 28.75491\n",
      "1360: accuracy:0.92 loss: 20.21631\n",
      "1370: accuracy:0.91 loss: 29.947582\n",
      "1380: accuracy:0.94 loss: 27.06556\n",
      "1390: accuracy:0.96 loss: 20.698526\n",
      "1400: accuracy:0.94 loss: 21.581375\n",
      "1400: ********* epoch 3 ********* test accuracy:0.918 test loss: 28.758135\n",
      "1410: accuracy:0.96 loss: 21.746193\n",
      "1420: accuracy:0.92 loss: 24.906372\n",
      "1430: accuracy:0.89 loss: 33.509136\n",
      "1440: accuracy:0.91 loss: 35.707787\n",
      "1450: accuracy:0.94 loss: 28.790075\n",
      "1450: ********* epoch 3 ********* test accuracy:0.92 test loss: 28.850796\n",
      "1460: accuracy:0.92 loss: 22.723927\n",
      "1470: accuracy:0.92 loss: 32.75344\n",
      "1480: accuracy:0.92 loss: 31.487356\n",
      "1490: accuracy:0.94 loss: 26.380295\n",
      "1500: accuracy:0.9 loss: 36.895885\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9192 test loss: 28.857553\n",
      "1510: accuracy:0.92 loss: 27.122837\n",
      "1520: accuracy:0.98 loss: 19.439419\n",
      "1530: accuracy:0.9 loss: 34.018375\n",
      "1540: accuracy:0.94 loss: 34.758785\n",
      "1550: accuracy:0.91 loss: 29.720585\n",
      "1550: ********* epoch 3 ********* test accuracy:0.9202 test loss: 28.453547\n",
      "1560: accuracy:0.93 loss: 30.344276\n",
      "1570: accuracy:0.91 loss: 25.56697\n",
      "1580: accuracy:0.91 loss: 20.29097\n",
      "1590: accuracy:0.93 loss: 31.639591\n",
      "1600: accuracy:0.91 loss: 25.712399\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9166 test loss: 29.157345\n",
      "1610: accuracy:0.91 loss: 30.545284\n",
      "1620: accuracy:0.94 loss: 24.642391\n",
      "1630: accuracy:0.92 loss: 37.90057\n",
      "1640: accuracy:0.9 loss: 26.452938\n",
      "1650: accuracy:0.89 loss: 40.39128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1650: ********* epoch 3 ********* test accuracy:0.9176 test loss: 29.108196\n",
      "1660: accuracy:0.92 loss: 27.67024\n",
      "1670: accuracy:0.95 loss: 24.23737\n",
      "1680: accuracy:0.92 loss: 28.478054\n",
      "1690: accuracy:0.95 loss: 22.371464\n",
      "1700: accuracy:0.94 loss: 29.841877\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9214 test loss: 28.551722\n",
      "1710: accuracy:0.9 loss: 31.181847\n",
      "1720: accuracy:0.94 loss: 32.672237\n",
      "1730: accuracy:0.89 loss: 34.989006\n",
      "1740: accuracy:0.87 loss: 39.659737\n",
      "1750: accuracy:0.91 loss: 27.952007\n",
      "1750: ********* epoch 3 ********* test accuracy:0.921 test loss: 28.218616\n",
      "1760: accuracy:0.93 loss: 27.667946\n",
      "1770: accuracy:0.94 loss: 28.60889\n",
      "1780: accuracy:0.9 loss: 31.009468\n",
      "1790: accuracy:0.93 loss: 34.48891\n",
      "1800: accuracy:0.92 loss: 30.272251\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9207 test loss: 28.756298\n",
      "1810: accuracy:0.94 loss: 22.657793\n",
      "1820: accuracy:0.92 loss: 20.724173\n",
      "1830: accuracy:0.94 loss: 17.446026\n",
      "1840: accuracy:0.89 loss: 27.84233\n",
      "1850: accuracy:0.94 loss: 26.966812\n",
      "1850: ********* epoch 4 ********* test accuracy:0.9189 test loss: 28.368935\n",
      "1860: accuracy:0.92 loss: 22.8511\n",
      "1870: accuracy:0.92 loss: 22.696697\n",
      "1880: accuracy:0.95 loss: 37.285362\n",
      "1890: accuracy:0.94 loss: 22.885754\n",
      "1900: accuracy:0.93 loss: 28.87719\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9212 test loss: 28.314123\n",
      "1910: accuracy:0.93 loss: 30.575949\n",
      "1920: accuracy:0.95 loss: 15.396218\n",
      "1930: accuracy:0.91 loss: 40.35607\n",
      "1940: accuracy:0.92 loss: 20.716518\n",
      "1950: accuracy:0.91 loss: 35.197567\n",
      "1950: ********* epoch 4 ********* test accuracy:0.9222 test loss: 28.322739\n",
      "1960: accuracy:0.91 loss: 31.084496\n",
      "1970: accuracy:0.91 loss: 33.80564\n",
      "1980: accuracy:0.92 loss: 23.328556\n",
      "1990: accuracy:0.92 loss: 25.920677\n",
      "2000: accuracy:0.93 loss: 27.8783\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9202 test loss: 28.305176\n",
      "max test accuracy: 0.9222\n"
     ]
    }
   ],
   "source": [
    "# input X => 28x28 grayscale dimensions ('None' indexes images in mini-batch)\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "\n",
    "# correct answers\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights\n",
    "W = tf.Variable(tf.zeros([28*28, 10]))\n",
    "\n",
    "# biases\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# flatten images into single line\n",
    "XX = tf.reshape(X, [-1, 28*28])\n",
    "\n",
    "# model\n",
    "Y = tf.nn.softmax(tf.matmul(XX, W) + b)\n",
    "\n",
    "# cross entropy\n",
    "cross_entropy = -tf.reduce_mean(Y_ * tf.log(Y)) * 1000.0\n",
    "\n",
    "# accuracy of the trained model [0, 1] ([worst, best])\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# training (learning rate is 0.003)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.003).minimize(cross_entropy)\n",
    "\n",
    "# initialization\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# call in loop to train the model 100 images at a time\n",
    "max_test_accuracy = 0.0\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    # train on batches of 100 images w/ 100 labels\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    # compute training values for visualisation\n",
    "    if update_train_data:\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: batch_X, Y_: batch_Y})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c))\n",
    "    # compute test values for visualization\n",
    "    if update_test_data:\n",
    "        global max_test_accuracy\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: mnist.test.images, Y_: mnist.test.labels})\n",
    "        if (a > max_test_accuracy):\n",
    "            max_test_accuracy = a\n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \n",
    "              \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        \n",
    "    # back-propogation training step\n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y})\n",
    "\n",
    "# text visualization of process\n",
    "for i in range(2000+1):\n",
    "    training_step(i, i % 50 == 0, i % 10 == 0)\n",
    "    \n",
    "# display the final max accuracy\n",
    "print(\"max test accuracy: \" + str(max_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Intermediate Layers\n",
    "The following cell contains modifications to the code above to include two intermediate layers using the sigmoid function as an activation function for the same process. With 10k iterations, it maxes out with an accuracy of \n",
    "~97%.\n",
    "\n",
    "## Notes of Interest\n",
    "Some notes of interest regarding the modifications below are as follows:\n",
    "* weights are initialized with random values\n",
    " * prevents optimizer from getting stuck in initial position\n",
    " * truncated_normal produces random values following the normal/Gaussian distribution between +/-2*stddev\n",
    "* weights/biases are associated by the obvious input/output relationship shown by their initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.11 loss: 246.54544\n",
      "0: ********* epoch 1 ********* test accuracy:0.1135 test loss: 240.80122\n",
      "10: accuracy:0.2 loss: 228.13031\n",
      "20: accuracy:0.09 loss: 227.5014\n",
      "30: accuracy:0.13 loss: 223.10568\n",
      "40: accuracy:0.25 loss: 218.30746\n",
      "50: accuracy:0.42 loss: 210.21735\n",
      "50: ********* epoch 1 ********* test accuracy:0.3923 test loss: 211.19885\n",
      "60: accuracy:0.35 loss: 202.7474\n",
      "70: accuracy:0.27 loss: 202.99185\n",
      "80: accuracy:0.38 loss: 190.52643\n",
      "90: accuracy:0.48 loss: 173.39792\n",
      "100: accuracy:0.57 loss: 158.45883\n",
      "100: ********* epoch 1 ********* test accuracy:0.525 test loss: 158.98427\n",
      "110: accuracy:0.59 loss: 152.19601\n",
      "120: accuracy:0.66 loss: 134.73416\n",
      "130: accuracy:0.74 loss: 114.5004\n",
      "140: accuracy:0.64 loss: 121.38859\n",
      "150: accuracy:0.77 loss: 101.67596\n",
      "150: ********* epoch 1 ********* test accuracy:0.7656 test loss: 106.18748\n",
      "160: accuracy:0.7 loss: 105.86787\n",
      "170: accuracy:0.73 loss: 94.641525\n",
      "180: accuracy:0.87 loss: 79.510925\n",
      "190: accuracy:0.74 loss: 90.2095\n",
      "200: accuracy:0.73 loss: 87.66275\n",
      "200: ********* epoch 1 ********* test accuracy:0.801 test loss: 80.7829\n",
      "210: accuracy:0.84 loss: 71.13519\n",
      "220: accuracy:0.78 loss: 83.46062\n",
      "230: accuracy:0.76 loss: 72.29387\n",
      "240: accuracy:0.85 loss: 59.65125\n",
      "250: accuracy:0.8 loss: 62.48784\n",
      "250: ********* epoch 1 ********* test accuracy:0.804 test loss: 69.300865\n",
      "260: accuracy:0.78 loss: 74.58673\n",
      "270: accuracy:0.8 loss: 70.64858\n",
      "280: accuracy:0.79 loss: 64.401794\n",
      "290: accuracy:0.85 loss: 43.923332\n",
      "300: accuracy:0.82 loss: 63.66522\n",
      "300: ********* epoch 1 ********* test accuracy:0.839 test loss: 56.785664\n",
      "310: accuracy:0.82 loss: 55.14323\n",
      "320: accuracy:0.83 loss: 64.17186\n",
      "330: accuracy:0.87 loss: 52.74931\n",
      "340: accuracy:0.86 loss: 52.900566\n",
      "350: accuracy:0.88 loss: 46.663563\n",
      "350: ********* epoch 1 ********* test accuracy:0.857 test loss: 50.36208\n",
      "360: accuracy:0.85 loss: 50.95849\n",
      "370: accuracy:0.89 loss: 54.2794\n",
      "380: accuracy:0.82 loss: 65.1679\n",
      "390: accuracy:0.84 loss: 52.11953\n",
      "400: accuracy:0.83 loss: 49.351654\n",
      "400: ********* epoch 1 ********* test accuracy:0.8614 test loss: 47.662388\n",
      "410: accuracy:0.91 loss: 44.084846\n",
      "420: accuracy:0.85 loss: 42.947876\n",
      "430: accuracy:0.88 loss: 44.93008\n",
      "440: accuracy:0.83 loss: 54.66858\n",
      "450: accuracy:0.89 loss: 37.721306\n",
      "450: ********* epoch 1 ********* test accuracy:0.8744 test loss: 43.65794\n",
      "460: accuracy:0.82 loss: 57.512222\n",
      "470: accuracy:0.8 loss: 56.28814\n",
      "480: accuracy:0.88 loss: 49.809956\n",
      "490: accuracy:0.9 loss: 39.518196\n",
      "500: accuracy:0.82 loss: 52.749523\n",
      "500: ********* epoch 1 ********* test accuracy:0.882 test loss: 40.926476\n",
      "510: accuracy:0.94 loss: 30.378357\n",
      "520: accuracy:0.9 loss: 41.94436\n",
      "530: accuracy:0.87 loss: 39.423756\n",
      "540: accuracy:0.89 loss: 38.274605\n",
      "550: accuracy:0.91 loss: 32.464005\n",
      "550: ********* epoch 1 ********* test accuracy:0.8828 test loss: 40.53306\n",
      "560: accuracy:0.9 loss: 34.388325\n",
      "570: accuracy:0.87 loss: 36.971954\n",
      "580: accuracy:0.93 loss: 28.159653\n",
      "590: accuracy:0.89 loss: 40.37656\n",
      "600: accuracy:0.89 loss: 42.43609\n",
      "600: ********* epoch 2 ********* test accuracy:0.8891 test loss: 38.721413\n",
      "610: accuracy:0.83 loss: 42.713108\n",
      "620: accuracy:0.93 loss: 26.81782\n",
      "630: accuracy:0.86 loss: 40.52558\n",
      "640: accuracy:0.88 loss: 30.457275\n",
      "650: accuracy:0.91 loss: 34.724968\n",
      "650: ********* epoch 2 ********* test accuracy:0.8965 test loss: 36.425278\n",
      "660: accuracy:0.9 loss: 27.944166\n",
      "670: accuracy:0.86 loss: 41.36086\n",
      "680: accuracy:0.95 loss: 21.47767\n",
      "690: accuracy:0.9 loss: 36.844337\n",
      "700: accuracy:0.89 loss: 40.85308\n",
      "700: ********* epoch 2 ********* test accuracy:0.8981 test loss: 35.159126\n",
      "710: accuracy:0.9 loss: 36.12995\n",
      "720: accuracy:0.88 loss: 38.099644\n",
      "730: accuracy:0.94 loss: 19.059875\n",
      "740: accuracy:0.88 loss: 33.15642\n",
      "750: accuracy:0.91 loss: 25.60564\n",
      "750: ********* epoch 2 ********* test accuracy:0.9008 test loss: 34.722466\n",
      "760: accuracy:0.92 loss: 36.40525\n",
      "770: accuracy:0.89 loss: 42.77997\n",
      "780: accuracy:0.94 loss: 25.20532\n",
      "790: accuracy:0.93 loss: 23.857985\n",
      "800: accuracy:0.94 loss: 18.48545\n",
      "800: ********* epoch 2 ********* test accuracy:0.9026 test loss: 33.634644\n",
      "810: accuracy:0.88 loss: 43.06978\n",
      "820: accuracy:0.86 loss: 35.1156\n",
      "830: accuracy:0.93 loss: 26.618275\n",
      "840: accuracy:0.84 loss: 45.684937\n",
      "850: accuracy:0.87 loss: 51.47663\n",
      "850: ********* epoch 2 ********* test accuracy:0.9035 test loss: 32.46728\n",
      "860: accuracy:0.85 loss: 42.95163\n",
      "870: accuracy:0.88 loss: 28.766695\n",
      "880: accuracy:0.87 loss: 41.75581\n",
      "890: accuracy:0.93 loss: 28.893644\n",
      "900: accuracy:0.89 loss: 30.747904\n",
      "900: ********* epoch 2 ********* test accuracy:0.9043 test loss: 32.24875\n",
      "910: accuracy:0.93 loss: 28.494877\n",
      "920: accuracy:0.91 loss: 28.196526\n",
      "930: accuracy:0.92 loss: 26.895294\n",
      "940: accuracy:0.93 loss: 24.781965\n",
      "950: accuracy:0.95 loss: 22.0304\n",
      "950: ********* epoch 2 ********* test accuracy:0.9082 test loss: 31.345434\n",
      "960: accuracy:0.92 loss: 36.282104\n",
      "970: accuracy:0.91 loss: 32.536545\n",
      "980: accuracy:0.87 loss: 41.23787\n",
      "990: accuracy:0.93 loss: 30.307968\n",
      "1000: accuracy:0.93 loss: 27.545973\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9048 test loss: 31.6917\n",
      "1010: accuracy:0.91 loss: 28.316984\n",
      "1020: accuracy:0.9 loss: 33.28769\n",
      "1030: accuracy:0.91 loss: 31.572155\n",
      "1040: accuracy:0.92 loss: 38.217373\n",
      "1050: accuracy:0.9 loss: 29.62363\n",
      "1050: ********* epoch 2 ********* test accuracy:0.9057 test loss: 31.064074\n",
      "1060: accuracy:0.91 loss: 38.984577\n",
      "1070: accuracy:0.91 loss: 25.71622\n",
      "1080: accuracy:0.95 loss: 17.049887\n",
      "1090: accuracy:0.89 loss: 27.01305\n",
      "1100: accuracy:0.95 loss: 17.48618\n",
      "1100: ********* epoch 2 ********* test accuracy:0.909 test loss: 30.59685\n",
      "1110: accuracy:0.91 loss: 37.586372\n",
      "1120: accuracy:0.88 loss: 40.34213\n",
      "1130: accuracy:0.91 loss: 32.28914\n",
      "1140: accuracy:0.87 loss: 29.994396\n",
      "1150: accuracy:0.92 loss: 26.819668\n",
      "1150: ********* epoch 2 ********* test accuracy:0.9138 test loss: 29.519375\n",
      "1160: accuracy:0.91 loss: 37.129517\n",
      "1170: accuracy:0.95 loss: 18.615425\n",
      "1180: accuracy:0.9 loss: 35.027737\n",
      "1190: accuracy:0.93 loss: 30.566383\n",
      "1200: accuracy:0.87 loss: 36.026497\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9105 test loss: 30.37501\n",
      "1210: accuracy:0.93 loss: 16.55975\n",
      "1220: accuracy:0.89 loss: 32.71057\n",
      "1230: accuracy:0.94 loss: 24.34889\n",
      "1240: accuracy:0.9 loss: 40.43558\n",
      "1250: accuracy:0.95 loss: 19.324812\n",
      "1250: ********* epoch 3 ********* test accuracy:0.9148 test loss: 28.617426\n",
      "1260: accuracy:0.89 loss: 40.477936\n",
      "1270: accuracy:0.93 loss: 24.700424\n",
      "1280: accuracy:0.93 loss: 28.98502\n",
      "1290: accuracy:0.87 loss: 40.65426\n",
      "1300: accuracy:0.94 loss: 35.46904\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9134 test loss: 28.789164\n",
      "1310: accuracy:0.92 loss: 27.956997\n",
      "1320: accuracy:0.97 loss: 19.943954\n",
      "1330: accuracy:0.93 loss: 28.608112\n",
      "1340: accuracy:0.89 loss: 29.957153\n",
      "1350: accuracy:0.92 loss: 28.771667\n",
      "1350: ********* epoch 3 ********* test accuracy:0.9159 test loss: 28.434479\n",
      "1360: accuracy:0.95 loss: 25.520365\n",
      "1370: accuracy:0.88 loss: 38.16996\n",
      "1380: accuracy:0.91 loss: 34.16981\n",
      "1390: accuracy:0.86 loss: 40.13749\n",
      "1400: accuracy:0.87 loss: 46.7446\n",
      "1400: ********* epoch 3 ********* test accuracy:0.9179 test loss: 27.790348\n",
      "1410: accuracy:0.92 loss: 25.872234\n",
      "1420: accuracy:0.87 loss: 49.149918\n",
      "1430: accuracy:0.92 loss: 24.451366\n",
      "1440: accuracy:0.97 loss: 15.445566\n",
      "1450: accuracy:0.95 loss: 26.500097\n",
      "1450: ********* epoch 3 ********* test accuracy:0.9197 test loss: 27.27852\n",
      "1460: accuracy:0.91 loss: 41.97367\n",
      "1470: accuracy:0.93 loss: 24.69219\n",
      "1480: accuracy:0.95 loss: 17.316307\n",
      "1490: accuracy:0.87 loss: 32.146355\n",
      "1500: accuracy:0.93 loss: 21.075157\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9196 test loss: 26.715923\n",
      "1510: accuracy:0.89 loss: 36.348137\n",
      "1520: accuracy:0.89 loss: 24.58426\n",
      "1530: accuracy:0.92 loss: 23.238964\n",
      "1540: accuracy:0.87 loss: 45.87427\n",
      "1550: accuracy:0.9 loss: 40.924065\n",
      "1550: ********* epoch 3 ********* test accuracy:0.9172 test loss: 27.45556\n",
      "1560: accuracy:0.87 loss: 34.702034\n",
      "1570: accuracy:0.94 loss: 18.780704\n",
      "1580: accuracy:0.96 loss: 15.685709\n",
      "1590: accuracy:0.89 loss: 36.293278\n",
      "1600: accuracy:0.88 loss: 45.630714\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9224 test loss: 26.383368\n",
      "1610: accuracy:0.96 loss: 19.26714\n",
      "1620: accuracy:0.91 loss: 37.211105\n",
      "1630: accuracy:0.88 loss: 32.970264\n",
      "1640: accuracy:0.93 loss: 22.169949\n",
      "1650: accuracy:0.95 loss: 16.516983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1650: ********* epoch 3 ********* test accuracy:0.9235 test loss: 25.957006\n",
      "1660: accuracy:0.91 loss: 26.03305\n",
      "1670: accuracy:0.91 loss: 35.762695\n",
      "1680: accuracy:0.91 loss: 39.53366\n",
      "1690: accuracy:0.92 loss: 31.76802\n",
      "1700: accuracy:0.91 loss: 38.43365\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9224 test loss: 26.180092\n",
      "1710: accuracy:0.9 loss: 32.881824\n",
      "1720: accuracy:0.95 loss: 22.134523\n",
      "1730: accuracy:0.96 loss: 24.070728\n",
      "1740: accuracy:0.89 loss: 27.96839\n",
      "1750: accuracy:0.92 loss: 29.398855\n",
      "1750: ********* epoch 3 ********* test accuracy:0.9248 test loss: 25.868961\n",
      "1760: accuracy:0.91 loss: 35.31413\n",
      "1770: accuracy:0.91 loss: 32.893612\n",
      "1780: accuracy:0.93 loss: 29.095432\n",
      "1790: accuracy:0.92 loss: 33.002125\n",
      "1800: accuracy:0.94 loss: 22.503437\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9251 test loss: 25.258575\n",
      "1810: accuracy:0.95 loss: 16.123676\n",
      "1820: accuracy:0.94 loss: 25.88787\n",
      "1830: accuracy:0.92 loss: 22.962582\n",
      "1840: accuracy:0.95 loss: 21.098495\n",
      "1850: accuracy:0.93 loss: 18.347878\n",
      "1850: ********* epoch 4 ********* test accuracy:0.9248 test loss: 25.270756\n",
      "1860: accuracy:0.94 loss: 27.463032\n",
      "1870: accuracy:0.88 loss: 31.283274\n",
      "1880: accuracy:0.92 loss: 21.088728\n",
      "1890: accuracy:0.91 loss: 37.299004\n",
      "1900: accuracy:0.92 loss: 20.776474\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9252 test loss: 25.075886\n",
      "1910: accuracy:0.93 loss: 24.308315\n",
      "1920: accuracy:0.88 loss: 34.387165\n",
      "1930: accuracy:0.91 loss: 26.625626\n",
      "1940: accuracy:0.91 loss: 36.536354\n",
      "1950: accuracy:0.95 loss: 25.935585\n",
      "1950: ********* epoch 4 ********* test accuracy:0.9254 test loss: 24.79119\n",
      "1960: accuracy:0.95 loss: 13.755904\n",
      "1970: accuracy:0.93 loss: 21.004662\n",
      "1980: accuracy:0.92 loss: 21.540804\n",
      "1990: accuracy:0.92 loss: 21.950655\n",
      "2000: accuracy:0.94 loss: 23.187145\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9288 test loss: 23.987844\n",
      "2010: accuracy:0.93 loss: 21.520607\n",
      "2020: accuracy:0.94 loss: 23.76358\n",
      "2030: accuracy:0.89 loss: 30.45877\n",
      "2040: accuracy:0.9 loss: 25.254105\n",
      "2050: accuracy:0.9 loss: 39.34188\n",
      "2050: ********* epoch 4 ********* test accuracy:0.9278 test loss: 24.338404\n",
      "2060: accuracy:0.91 loss: 31.816706\n",
      "2070: accuracy:0.93 loss: 19.792936\n",
      "2080: accuracy:0.96 loss: 19.990158\n",
      "2090: accuracy:0.91 loss: 23.790632\n",
      "2100: accuracy:0.94 loss: 17.520203\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9278 test loss: 23.471285\n",
      "2110: accuracy:0.94 loss: 20.608751\n",
      "2120: accuracy:0.96 loss: 11.1700325\n",
      "2130: accuracy:0.89 loss: 30.557873\n",
      "2140: accuracy:0.91 loss: 26.427933\n",
      "2150: accuracy:0.91 loss: 32.049232\n",
      "2150: ********* epoch 4 ********* test accuracy:0.9299 test loss: 23.736853\n",
      "2160: accuracy:0.95 loss: 19.748173\n",
      "2170: accuracy:0.92 loss: 20.350845\n",
      "2180: accuracy:0.92 loss: 22.99044\n",
      "2190: accuracy:0.9 loss: 23.825777\n",
      "2200: accuracy:0.96 loss: 12.893187\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9305 test loss: 23.325478\n",
      "2210: accuracy:0.92 loss: 23.150614\n",
      "2220: accuracy:0.91 loss: 23.68132\n",
      "2230: accuracy:0.96 loss: 14.85055\n",
      "2240: accuracy:0.97 loss: 16.446136\n",
      "2250: accuracy:0.96 loss: 14.13286\n",
      "2250: ********* epoch 4 ********* test accuracy:0.9281 test loss: 23.979523\n",
      "2260: accuracy:0.97 loss: 19.066181\n",
      "2270: accuracy:0.91 loss: 22.672096\n",
      "2280: accuracy:0.95 loss: 15.337862\n",
      "2290: accuracy:0.95 loss: 13.434756\n",
      "2300: accuracy:0.94 loss: 20.439522\n",
      "2300: ********* epoch 4 ********* test accuracy:0.931 test loss: 23.277872\n",
      "2310: accuracy:0.95 loss: 16.755638\n",
      "2320: accuracy:0.94 loss: 15.201332\n",
      "2330: accuracy:0.93 loss: 25.26915\n",
      "2340: accuracy:0.94 loss: 19.023838\n",
      "2350: accuracy:0.96 loss: 16.374748\n",
      "2350: ********* epoch 4 ********* test accuracy:0.933 test loss: 22.40194\n",
      "2360: accuracy:0.93 loss: 27.96545\n",
      "2370: accuracy:0.94 loss: 18.067299\n",
      "2380: accuracy:0.92 loss: 24.184595\n",
      "2390: accuracy:0.88 loss: 35.899445\n",
      "2400: accuracy:0.93 loss: 19.728045\n",
      "2400: ********* epoch 5 ********* test accuracy:0.9286 test loss: 23.545723\n",
      "2410: accuracy:0.95 loss: 16.099154\n",
      "2420: accuracy:0.99 loss: 8.551511\n",
      "2430: accuracy:0.96 loss: 16.48576\n",
      "2440: accuracy:0.96 loss: 16.382423\n",
      "2450: accuracy:0.92 loss: 14.929311\n",
      "2450: ********* epoch 5 ********* test accuracy:0.9303 test loss: 22.5608\n",
      "2460: accuracy:0.88 loss: 38.97513\n",
      "2470: accuracy:0.95 loss: 18.328075\n",
      "2480: accuracy:0.94 loss: 20.852032\n",
      "2490: accuracy:0.88 loss: 42.983967\n",
      "2500: accuracy:0.97 loss: 13.724327\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9339 test loss: 22.03571\n",
      "2510: accuracy:0.9 loss: 30.943758\n",
      "2520: accuracy:0.95 loss: 19.827168\n",
      "2530: accuracy:0.89 loss: 28.951803\n",
      "2540: accuracy:0.89 loss: 29.256847\n",
      "2550: accuracy:0.91 loss: 27.17065\n",
      "2550: ********* epoch 5 ********* test accuracy:0.9336 test loss: 21.747757\n",
      "2560: accuracy:0.93 loss: 19.116388\n",
      "2570: accuracy:0.96 loss: 14.089878\n",
      "2580: accuracy:0.93 loss: 43.349163\n",
      "2590: accuracy:0.94 loss: 20.538803\n",
      "2600: accuracy:0.94 loss: 17.482332\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9371 test loss: 21.530605\n",
      "2610: accuracy:0.95 loss: 18.839218\n",
      "2620: accuracy:0.93 loss: 30.748463\n",
      "2630: accuracy:0.93 loss: 16.260735\n",
      "2640: accuracy:0.95 loss: 15.360505\n",
      "2650: accuracy:0.94 loss: 19.930481\n",
      "2650: ********* epoch 5 ********* test accuracy:0.9363 test loss: 20.834543\n",
      "2660: accuracy:0.91 loss: 27.051458\n",
      "2670: accuracy:0.96 loss: 18.495068\n",
      "2680: accuracy:0.95 loss: 22.473406\n",
      "2690: accuracy:0.95 loss: 27.658714\n",
      "2700: accuracy:0.95 loss: 23.52866\n",
      "2700: ********* epoch 5 ********* test accuracy:0.933 test loss: 21.743818\n",
      "2710: accuracy:0.92 loss: 22.86409\n",
      "2720: accuracy:0.95 loss: 18.710972\n",
      "2730: accuracy:0.95 loss: 16.932835\n",
      "2740: accuracy:0.95 loss: 13.197124\n",
      "2750: accuracy:0.94 loss: 16.828499\n",
      "2750: ********* epoch 5 ********* test accuracy:0.934 test loss: 21.381044\n",
      "2760: accuracy:0.97 loss: 17.929255\n",
      "2770: accuracy:0.94 loss: 17.173489\n",
      "2780: accuracy:0.96 loss: 15.1033125\n",
      "2790: accuracy:0.93 loss: 24.733932\n",
      "2800: accuracy:0.91 loss: 27.766289\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9329 test loss: 21.511417\n",
      "2810: accuracy:0.92 loss: 17.71408\n",
      "2820: accuracy:0.95 loss: 13.8867855\n",
      "2830: accuracy:0.93 loss: 15.231512\n",
      "2840: accuracy:0.92 loss: 23.354118\n",
      "2850: accuracy:0.92 loss: 29.838924\n",
      "2850: ********* epoch 5 ********* test accuracy:0.9375 test loss: 20.443195\n",
      "2860: accuracy:0.85 loss: 30.093536\n",
      "2870: accuracy:0.9 loss: 26.066769\n",
      "2880: accuracy:0.95 loss: 16.364449\n",
      "2890: accuracy:0.93 loss: 20.17997\n",
      "2900: accuracy:0.95 loss: 15.056587\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9403 test loss: 20.33644\n",
      "2910: accuracy:0.9 loss: 33.365585\n",
      "2920: accuracy:0.94 loss: 19.650337\n",
      "2930: accuracy:0.95 loss: 11.886219\n",
      "2940: accuracy:0.96 loss: 12.502148\n",
      "2950: accuracy:0.92 loss: 31.21138\n",
      "2950: ********* epoch 5 ********* test accuracy:0.9407 test loss: 20.148157\n",
      "2960: accuracy:0.9 loss: 28.096241\n",
      "2970: accuracy:0.91 loss: 24.432543\n",
      "2980: accuracy:0.92 loss: 25.225069\n",
      "2990: accuracy:0.97 loss: 13.619122\n",
      "3000: accuracy:0.93 loss: 21.31792\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9404 test loss: 19.878016\n",
      "3010: accuracy:0.97 loss: 12.467894\n",
      "3020: accuracy:0.94 loss: 15.027365\n",
      "3030: accuracy:0.93 loss: 23.490883\n",
      "3040: accuracy:0.97 loss: 12.125297\n",
      "3050: accuracy:0.97 loss: 9.782655\n",
      "3050: ********* epoch 6 ********* test accuracy:0.9408 test loss: 19.617933\n",
      "3060: accuracy:0.93 loss: 24.484262\n",
      "3070: accuracy:0.93 loss: 16.325314\n",
      "3080: accuracy:0.98 loss: 13.755335\n",
      "3090: accuracy:0.91 loss: 27.785908\n",
      "3100: accuracy:0.96 loss: 13.630885\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9421 test loss: 19.50568\n",
      "3110: accuracy:0.94 loss: 20.661497\n",
      "3120: accuracy:0.93 loss: 32.995945\n",
      "3130: accuracy:0.96 loss: 15.297888\n",
      "3140: accuracy:0.9 loss: 25.978289\n",
      "3150: accuracy:0.95 loss: 21.962845\n",
      "3150: ********* epoch 6 ********* test accuracy:0.9434 test loss: 19.186687\n",
      "3160: accuracy:0.97 loss: 7.9870353\n",
      "3170: accuracy:0.92 loss: 24.737274\n",
      "3180: accuracy:0.97 loss: 12.34287\n",
      "3190: accuracy:0.93 loss: 23.450499\n",
      "3200: accuracy:0.97 loss: 16.476746\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9432 test loss: 19.075792\n",
      "3210: accuracy:0.97 loss: 11.215866\n",
      "3220: accuracy:0.96 loss: 12.718839\n",
      "3230: accuracy:0.95 loss: 22.160936\n",
      "3240: accuracy:0.9 loss: 35.172974\n",
      "3250: accuracy:0.94 loss: 24.58893\n",
      "3250: ********* epoch 6 ********* test accuracy:0.9414 test loss: 19.408592\n",
      "3260: accuracy:0.95 loss: 24.787724\n",
      "3270: accuracy:0.95 loss: 15.667355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3280: accuracy:0.93 loss: 18.620403\n",
      "3290: accuracy:0.9 loss: 29.52423\n",
      "3300: accuracy:0.93 loss: 23.649658\n",
      "3300: ********* epoch 6 ********* test accuracy:0.9414 test loss: 19.269615\n",
      "3310: accuracy:0.96 loss: 11.645509\n",
      "3320: accuracy:0.96 loss: 20.17769\n",
      "3330: accuracy:0.95 loss: 15.550938\n",
      "3340: accuracy:0.96 loss: 19.350683\n",
      "3350: accuracy:0.93 loss: 18.671982\n",
      "3350: ********* epoch 6 ********* test accuracy:0.9447 test loss: 18.950352\n",
      "3360: accuracy:0.92 loss: 24.673332\n",
      "3370: accuracy:0.9 loss: 32.280853\n",
      "3380: accuracy:0.93 loss: 18.277746\n",
      "3390: accuracy:0.94 loss: 19.231333\n",
      "3400: accuracy:0.93 loss: 32.059998\n",
      "3400: ********* epoch 6 ********* test accuracy:0.9437 test loss: 18.531258\n",
      "3410: accuracy:0.88 loss: 46.63792\n",
      "3420: accuracy:0.95 loss: 15.576578\n",
      "3430: accuracy:0.96 loss: 14.413932\n",
      "3440: accuracy:0.93 loss: 23.447731\n",
      "3450: accuracy:0.93 loss: 12.50864\n",
      "3450: ********* epoch 6 ********* test accuracy:0.9455 test loss: 18.215158\n",
      "3460: accuracy:0.95 loss: 17.268719\n",
      "3470: accuracy:0.93 loss: 15.204305\n",
      "3480: accuracy:0.92 loss: 18.753555\n",
      "3490: accuracy:0.97 loss: 11.527925\n",
      "3500: accuracy:0.95 loss: 15.9601755\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9447 test loss: 18.73378\n",
      "3510: accuracy:0.94 loss: 18.191936\n",
      "3520: accuracy:0.93 loss: 22.31577\n",
      "3530: accuracy:0.94 loss: 19.97631\n",
      "3540: accuracy:0.96 loss: 23.2663\n",
      "3550: accuracy:0.98 loss: 11.235636\n",
      "3550: ********* epoch 6 ********* test accuracy:0.9454 test loss: 18.213749\n",
      "3560: accuracy:0.97 loss: 17.606052\n",
      "3570: accuracy:0.93 loss: 28.53632\n",
      "3580: accuracy:0.97 loss: 15.850745\n",
      "3590: accuracy:0.94 loss: 20.639252\n",
      "3600: accuracy:0.98 loss: 7.2765064\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9466 test loss: 17.940886\n",
      "3610: accuracy:0.93 loss: 16.572716\n",
      "3620: accuracy:0.96 loss: 11.991276\n",
      "3630: accuracy:0.93 loss: 24.857372\n",
      "3640: accuracy:0.91 loss: 23.694183\n",
      "3650: accuracy:0.93 loss: 15.722329\n",
      "3650: ********* epoch 7 ********* test accuracy:0.9465 test loss: 17.65471\n",
      "3660: accuracy:0.92 loss: 33.72473\n",
      "3670: accuracy:0.95 loss: 21.857193\n",
      "3680: accuracy:0.88 loss: 38.882545\n",
      "3690: accuracy:0.93 loss: 26.130007\n",
      "3700: accuracy:0.9 loss: 25.52209\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9472 test loss: 17.386574\n",
      "3710: accuracy:0.92 loss: 24.2693\n",
      "3720: accuracy:0.95 loss: 13.493757\n",
      "3730: accuracy:0.93 loss: 25.978266\n",
      "3740: accuracy:0.94 loss: 24.722843\n",
      "3750: accuracy:0.97 loss: 12.832455\n",
      "3750: ********* epoch 7 ********* test accuracy:0.9473 test loss: 17.645576\n",
      "3760: accuracy:0.98 loss: 9.198135\n",
      "3770: accuracy:0.97 loss: 11.715009\n",
      "3780: accuracy:0.96 loss: 14.285268\n",
      "3790: accuracy:0.89 loss: 24.262653\n",
      "3800: accuracy:0.97 loss: 15.565151\n",
      "3800: ********* epoch 7 ********* test accuracy:0.945 test loss: 18.071384\n",
      "3810: accuracy:0.95 loss: 14.998524\n",
      "3820: accuracy:0.99 loss: 5.141849\n",
      "3830: accuracy:0.97 loss: 13.061643\n",
      "3840: accuracy:0.91 loss: 20.967075\n",
      "3850: accuracy:0.94 loss: 18.838673\n",
      "3850: ********* epoch 7 ********* test accuracy:0.9433 test loss: 18.047134\n",
      "3860: accuracy:0.96 loss: 12.594439\n",
      "3870: accuracy:0.96 loss: 14.61899\n",
      "3880: accuracy:0.97 loss: 13.246963\n",
      "3890: accuracy:0.96 loss: 10.813686\n",
      "3900: accuracy:0.95 loss: 17.181154\n",
      "3900: ********* epoch 7 ********* test accuracy:0.9476 test loss: 17.118578\n",
      "3910: accuracy:0.95 loss: 24.68371\n",
      "3920: accuracy:0.97 loss: 12.017202\n",
      "3930: accuracy:0.94 loss: 17.663097\n",
      "3940: accuracy:0.98 loss: 12.983553\n",
      "3950: accuracy:0.96 loss: 14.718075\n",
      "3950: ********* epoch 7 ********* test accuracy:0.9504 test loss: 17.090933\n",
      "3960: accuracy:0.92 loss: 22.327135\n",
      "3970: accuracy:0.97 loss: 12.051081\n",
      "3980: accuracy:0.95 loss: 16.201853\n",
      "3990: accuracy:0.94 loss: 29.760475\n",
      "4000: accuracy:0.98 loss: 12.59345\n",
      "4000: ********* epoch 7 ********* test accuracy:0.9504 test loss: 16.61179\n",
      "4010: accuracy:0.96 loss: 13.9137535\n",
      "4020: accuracy:0.95 loss: 19.329546\n",
      "4030: accuracy:0.97 loss: 11.343126\n",
      "4040: accuracy:0.94 loss: 18.506786\n",
      "4050: accuracy:0.95 loss: 19.482227\n",
      "4050: ********* epoch 7 ********* test accuracy:0.9491 test loss: 16.925453\n",
      "4060: accuracy:1.0 loss: 6.159032\n",
      "4070: accuracy:0.92 loss: 27.568169\n",
      "4080: accuracy:0.96 loss: 13.592925\n",
      "4090: accuracy:0.97 loss: 12.871761\n",
      "4100: accuracy:0.94 loss: 17.129166\n",
      "4100: ********* epoch 7 ********* test accuracy:0.949 test loss: 17.026596\n",
      "4110: accuracy:0.97 loss: 13.976568\n",
      "4120: accuracy:0.93 loss: 16.756948\n",
      "4130: accuracy:0.93 loss: 24.558525\n",
      "4140: accuracy:0.95 loss: 21.1944\n",
      "4150: accuracy:0.96 loss: 17.93765\n",
      "4150: ********* epoch 7 ********* test accuracy:0.9506 test loss: 16.419878\n",
      "4160: accuracy:0.95 loss: 12.474704\n",
      "4170: accuracy:0.97 loss: 8.000416\n",
      "4180: accuracy:0.94 loss: 15.336624\n",
      "4190: accuracy:0.98 loss: 6.173545\n",
      "4200: accuracy:0.97 loss: 11.970669\n",
      "4200: ********* epoch 8 ********* test accuracy:0.9505 test loss: 16.30004\n",
      "4210: accuracy:0.96 loss: 13.120307\n",
      "4220: accuracy:0.98 loss: 8.332429\n",
      "4230: accuracy:0.96 loss: 10.935102\n",
      "4240: accuracy:0.92 loss: 21.651865\n",
      "4250: accuracy:0.95 loss: 18.25671\n",
      "4250: ********* epoch 8 ********* test accuracy:0.9494 test loss: 17.179676\n",
      "4260: accuracy:0.97 loss: 17.640253\n",
      "4270: accuracy:0.95 loss: 12.824121\n",
      "4280: accuracy:0.97 loss: 15.454262\n",
      "4290: accuracy:0.93 loss: 21.491627\n",
      "4300: accuracy:0.94 loss: 19.176044\n",
      "4300: ********* epoch 8 ********* test accuracy:0.9475 test loss: 17.037783\n",
      "4310: accuracy:1.0 loss: 5.3707747\n",
      "4320: accuracy:0.96 loss: 14.154453\n",
      "4330: accuracy:0.93 loss: 20.369423\n",
      "4340: accuracy:0.95 loss: 13.561758\n",
      "4350: accuracy:0.95 loss: 16.885452\n",
      "4350: ********* epoch 8 ********* test accuracy:0.9497 test loss: 16.531101\n",
      "4360: accuracy:0.95 loss: 16.125713\n",
      "4370: accuracy:0.96 loss: 14.188374\n",
      "4380: accuracy:0.94 loss: 14.903774\n",
      "4390: accuracy:0.98 loss: 9.416528\n",
      "4400: accuracy:0.98 loss: 8.788755\n",
      "4400: ********* epoch 8 ********* test accuracy:0.9498 test loss: 16.239796\n",
      "4410: accuracy:0.93 loss: 18.10976\n",
      "4420: accuracy:0.95 loss: 17.832743\n",
      "4430: accuracy:0.96 loss: 16.800726\n",
      "4440: accuracy:0.99 loss: 3.4806514\n",
      "4450: accuracy:0.93 loss: 22.280167\n",
      "4450: ********* epoch 8 ********* test accuracy:0.9507 test loss: 15.875652\n",
      "4460: accuracy:0.92 loss: 20.20708\n",
      "4470: accuracy:0.98 loss: 6.0422087\n",
      "4480: accuracy:0.95 loss: 19.201262\n",
      "4490: accuracy:0.97 loss: 11.126695\n",
      "4500: accuracy:0.96 loss: 15.634908\n",
      "4500: ********* epoch 8 ********* test accuracy:0.9514 test loss: 16.290955\n",
      "4510: accuracy:0.97 loss: 17.81951\n",
      "4520: accuracy:0.95 loss: 10.384262\n",
      "4530: accuracy:0.93 loss: 19.027927\n",
      "4540: accuracy:0.93 loss: 17.4909\n",
      "4550: accuracy:0.93 loss: 20.880112\n",
      "4550: ********* epoch 8 ********* test accuracy:0.9533 test loss: 15.705805\n",
      "4560: accuracy:0.96 loss: 17.173473\n",
      "4570: accuracy:0.97 loss: 8.880358\n",
      "4580: accuracy:0.96 loss: 13.662573\n",
      "4590: accuracy:0.96 loss: 12.898975\n",
      "4600: accuracy:0.97 loss: 7.7312346\n",
      "4600: ********* epoch 8 ********* test accuracy:0.9532 test loss: 15.356263\n",
      "4610: accuracy:0.96 loss: 20.75612\n",
      "4620: accuracy:0.96 loss: 22.36857\n",
      "4630: accuracy:0.92 loss: 25.61485\n",
      "4640: accuracy:0.99 loss: 8.025149\n",
      "4650: accuracy:0.94 loss: 17.625008\n",
      "4650: ********* epoch 8 ********* test accuracy:0.9491 test loss: 16.467611\n",
      "4660: accuracy:0.93 loss: 20.801708\n",
      "4670: accuracy:0.94 loss: 16.599981\n",
      "4680: accuracy:0.96 loss: 8.948296\n",
      "4690: accuracy:0.91 loss: 26.991701\n",
      "4700: accuracy:0.97 loss: 9.228674\n",
      "4700: ********* epoch 8 ********* test accuracy:0.9516 test loss: 16.195091\n",
      "4710: accuracy:0.96 loss: 15.043346\n",
      "4720: accuracy:0.97 loss: 7.9997864\n",
      "4730: accuracy:0.96 loss: 8.712706\n",
      "4740: accuracy:0.99 loss: 5.665924\n",
      "4750: accuracy:0.98 loss: 9.138246\n",
      "4750: ********* epoch 8 ********* test accuracy:0.9527 test loss: 15.247307\n",
      "4760: accuracy:0.97 loss: 13.650386\n",
      "4770: accuracy:0.97 loss: 10.369202\n",
      "4780: accuracy:0.97 loss: 12.622671\n",
      "4790: accuracy:0.94 loss: 22.758635\n",
      "4800: accuracy:0.91 loss: 22.387018\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9546 test loss: 15.0137615\n",
      "4810: accuracy:0.97 loss: 11.303107\n",
      "4820: accuracy:0.96 loss: 10.225314\n",
      "4830: accuracy:0.94 loss: 23.952309\n",
      "4840: accuracy:0.99 loss: 5.583374\n",
      "4850: accuracy:0.96 loss: 12.853758\n",
      "4850: ********* epoch 9 ********* test accuracy:0.956 test loss: 14.942514\n",
      "4860: accuracy:0.95 loss: 16.683992\n",
      "4870: accuracy:0.96 loss: 14.987936\n",
      "4880: accuracy:0.98 loss: 7.9904227\n",
      "4890: accuracy:0.98 loss: 6.1821074\n",
      "4900: accuracy:0.97 loss: 9.052947\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9536 test loss: 15.31766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4910: accuracy:0.97 loss: 10.035166\n",
      "4920: accuracy:0.96 loss: 9.916686\n",
      "4930: accuracy:0.95 loss: 20.719355\n",
      "4940: accuracy:0.97 loss: 12.291887\n",
      "4950: accuracy:0.95 loss: 24.843431\n",
      "4950: ********* epoch 9 ********* test accuracy:0.953 test loss: 15.805494\n",
      "4960: accuracy:0.96 loss: 14.738222\n",
      "4970: accuracy:0.99 loss: 7.506539\n",
      "4980: accuracy:0.93 loss: 17.517597\n",
      "4990: accuracy:0.96 loss: 10.536514\n",
      "5000: accuracy:0.98 loss: 15.703745\n",
      "5000: ********* epoch 9 ********* test accuracy:0.9555 test loss: 14.928447\n",
      "5010: accuracy:0.95 loss: 18.338327\n",
      "5020: accuracy:0.9 loss: 23.817715\n",
      "5030: accuracy:0.96 loss: 15.006748\n",
      "5040: accuracy:0.98 loss: 14.889682\n",
      "5050: accuracy:0.99 loss: 6.1129994\n",
      "5050: ********* epoch 9 ********* test accuracy:0.9544 test loss: 14.801533\n",
      "5060: accuracy:0.97 loss: 9.888882\n",
      "5070: accuracy:0.98 loss: 7.8484864\n",
      "5080: accuracy:0.99 loss: 6.687183\n",
      "5090: accuracy:0.97 loss: 10.87062\n",
      "5100: accuracy:0.94 loss: 18.260042\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9546 test loss: 14.654082\n",
      "5110: accuracy:0.95 loss: 12.294151\n",
      "5120: accuracy:0.97 loss: 20.793707\n",
      "5130: accuracy:0.97 loss: 10.969837\n",
      "5140: accuracy:0.97 loss: 13.669595\n",
      "5150: accuracy:0.94 loss: 22.861446\n",
      "5150: ********* epoch 9 ********* test accuracy:0.9533 test loss: 15.582642\n",
      "5160: accuracy:0.96 loss: 9.76055\n",
      "5170: accuracy:0.92 loss: 20.368534\n",
      "5180: accuracy:0.91 loss: 20.04704\n",
      "5190: accuracy:0.99 loss: 7.5038037\n",
      "5200: accuracy:0.97 loss: 13.006196\n",
      "5200: ********* epoch 9 ********* test accuracy:0.9552 test loss: 15.433411\n",
      "5210: accuracy:0.95 loss: 19.340395\n",
      "5220: accuracy:0.93 loss: 18.530006\n",
      "5230: accuracy:0.99 loss: 7.3503127\n",
      "5240: accuracy:0.98 loss: 10.443872\n",
      "5250: accuracy:0.95 loss: 18.708345\n",
      "5250: ********* epoch 9 ********* test accuracy:0.9558 test loss: 14.1371355\n",
      "5260: accuracy:0.94 loss: 13.163633\n",
      "5270: accuracy:0.98 loss: 8.417839\n",
      "5280: accuracy:0.95 loss: 14.956623\n",
      "5290: accuracy:0.94 loss: 20.44048\n",
      "5300: accuracy:0.94 loss: 18.405376\n",
      "5300: ********* epoch 9 ********* test accuracy:0.9567 test loss: 14.188577\n",
      "5310: accuracy:0.99 loss: 8.168522\n",
      "5320: accuracy:0.97 loss: 12.902475\n",
      "5330: accuracy:0.96 loss: 11.9738035\n",
      "5340: accuracy:0.95 loss: 13.442879\n",
      "5350: accuracy:0.95 loss: 17.694536\n",
      "5350: ********* epoch 9 ********* test accuracy:0.9565 test loss: 14.145876\n",
      "5360: accuracy:0.97 loss: 9.986804\n",
      "5370: accuracy:0.97 loss: 9.559521\n",
      "5380: accuracy:0.96 loss: 11.70549\n",
      "5390: accuracy:0.98 loss: 7.4379063\n",
      "5400: accuracy:0.97 loss: 10.445917\n",
      "5400: ********* epoch 10 ********* test accuracy:0.9569 test loss: 14.1292095\n",
      "5410: accuracy:0.98 loss: 11.048882\n",
      "5420: accuracy:0.98 loss: 10.3641815\n",
      "5430: accuracy:0.97 loss: 14.219471\n",
      "5440: accuracy:0.93 loss: 16.555765\n",
      "5450: accuracy:0.96 loss: 14.600426\n",
      "5450: ********* epoch 10 ********* test accuracy:0.9571 test loss: 14.468472\n",
      "5460: accuracy:0.99 loss: 8.610339\n",
      "5470: accuracy:0.99 loss: 5.8981495\n",
      "5480: accuracy:0.99 loss: 8.239217\n",
      "5490: accuracy:0.94 loss: 18.117615\n",
      "5500: accuracy:0.97 loss: 12.819021\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9562 test loss: 13.992338\n",
      "5510: accuracy:0.92 loss: 17.81257\n",
      "5520: accuracy:0.96 loss: 16.854652\n",
      "5530: accuracy:0.97 loss: 8.98376\n",
      "5540: accuracy:0.99 loss: 9.082563\n",
      "5550: accuracy:0.96 loss: 9.843784\n",
      "5550: ********* epoch 10 ********* test accuracy:0.9577 test loss: 14.1412945\n",
      "5560: accuracy:0.99 loss: 5.2867084\n",
      "5570: accuracy:0.94 loss: 17.825216\n",
      "5580: accuracy:0.97 loss: 9.222259\n",
      "5590: accuracy:0.94 loss: 21.057537\n",
      "5600: accuracy:0.95 loss: 7.854103\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9585 test loss: 13.960432\n",
      "5610: accuracy:0.97 loss: 10.577285\n",
      "5620: accuracy:0.97 loss: 10.172282\n",
      "5630: accuracy:0.96 loss: 16.702183\n",
      "5640: accuracy:0.97 loss: 13.714687\n",
      "5650: accuracy:0.94 loss: 15.53158\n",
      "5650: ********* epoch 10 ********* test accuracy:0.9566 test loss: 13.910449\n",
      "5660: accuracy:0.96 loss: 10.319189\n",
      "5670: accuracy:0.99 loss: 6.7058926\n",
      "5680: accuracy:0.97 loss: 7.321122\n",
      "5690: accuracy:0.95 loss: 11.781952\n",
      "5700: accuracy:0.98 loss: 12.105258\n",
      "5700: ********* epoch 10 ********* test accuracy:0.9571 test loss: 13.776199\n",
      "5710: accuracy:0.93 loss: 17.364521\n",
      "5720: accuracy:0.97 loss: 9.978188\n",
      "5730: accuracy:0.93 loss: 22.61117\n",
      "5740: accuracy:0.97 loss: 12.642428\n",
      "5750: accuracy:0.94 loss: 13.215254\n",
      "5750: ********* epoch 10 ********* test accuracy:0.9591 test loss: 13.257672\n",
      "5760: accuracy:0.95 loss: 20.507324\n",
      "5770: accuracy:0.92 loss: 19.843613\n",
      "5780: accuracy:0.99 loss: 6.349352\n",
      "5790: accuracy:0.98 loss: 8.033718\n",
      "5800: accuracy:0.96 loss: 11.388527\n",
      "5800: ********* epoch 10 ********* test accuracy:0.9588 test loss: 13.551327\n",
      "5810: accuracy:0.97 loss: 10.499741\n",
      "5820: accuracy:0.98 loss: 7.9555035\n",
      "5830: accuracy:0.92 loss: 31.369053\n",
      "5840: accuracy:0.96 loss: 17.622107\n",
      "5850: accuracy:0.95 loss: 11.8207\n",
      "5850: ********* epoch 10 ********* test accuracy:0.9595 test loss: 13.282169\n",
      "5860: accuracy:0.97 loss: 14.84808\n",
      "5870: accuracy:0.98 loss: 8.489605\n",
      "5880: accuracy:0.96 loss: 9.399868\n",
      "5890: accuracy:0.96 loss: 15.638273\n",
      "5900: accuracy:0.97 loss: 18.005064\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9584 test loss: 13.469893\n",
      "5910: accuracy:0.99 loss: 6.354966\n",
      "5920: accuracy:0.98 loss: 6.430846\n",
      "5930: accuracy:0.97 loss: 9.104449\n",
      "5940: accuracy:0.98 loss: 6.9293017\n",
      "5950: accuracy:0.99 loss: 5.0178347\n",
      "5950: ********* epoch 10 ********* test accuracy:0.9588 test loss: 13.257605\n",
      "5960: accuracy:0.97 loss: 6.7821054\n",
      "5970: accuracy:0.98 loss: 12.870452\n",
      "5980: accuracy:0.94 loss: 13.253344\n",
      "5990: accuracy:0.99 loss: 9.411137\n",
      "6000: accuracy:0.96 loss: 10.633449\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9574 test loss: 13.2495365\n",
      "6010: accuracy:0.99 loss: 4.199513\n",
      "6020: accuracy:0.93 loss: 15.575915\n",
      "6030: accuracy:0.97 loss: 12.259134\n",
      "6040: accuracy:0.97 loss: 11.668464\n",
      "6050: accuracy:0.96 loss: 10.587875\n",
      "6050: ********* epoch 11 ********* test accuracy:0.96 test loss: 12.9647455\n",
      "6060: accuracy:0.97 loss: 14.940659\n",
      "6070: accuracy:0.97 loss: 8.053799\n",
      "6080: accuracy:0.96 loss: 8.149822\n",
      "6090: accuracy:0.96 loss: 14.479067\n",
      "6100: accuracy:0.98 loss: 10.970904\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9603 test loss: 13.362159\n",
      "6110: accuracy:0.95 loss: 15.285559\n",
      "6120: accuracy:0.94 loss: 19.579252\n",
      "6130: accuracy:0.96 loss: 12.3347\n",
      "6140: accuracy:0.93 loss: 18.913074\n",
      "6150: accuracy:0.91 loss: 33.072937\n",
      "6150: ********* epoch 11 ********* test accuracy:0.9591 test loss: 13.3478155\n",
      "6160: accuracy:0.96 loss: 13.874149\n",
      "6170: accuracy:0.97 loss: 9.434483\n",
      "6180: accuracy:0.95 loss: 18.74636\n",
      "6190: accuracy:0.95 loss: 12.978044\n",
      "6200: accuracy:0.98 loss: 4.5592623\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9591 test loss: 13.406936\n",
      "6210: accuracy:0.94 loss: 15.755677\n",
      "6220: accuracy:0.91 loss: 16.977081\n",
      "6230: accuracy:0.99 loss: 6.7568207\n",
      "6240: accuracy:0.99 loss: 5.1964197\n",
      "6250: accuracy:0.98 loss: 9.331705\n",
      "6250: ********* epoch 11 ********* test accuracy:0.9603 test loss: 12.875244\n",
      "6260: accuracy:1.0 loss: 4.978477\n",
      "6270: accuracy:1.0 loss: 4.544942\n",
      "6280: accuracy:0.96 loss: 10.847442\n",
      "6290: accuracy:0.94 loss: 19.764751\n",
      "6300: accuracy:0.97 loss: 13.716995\n",
      "6300: ********* epoch 11 ********* test accuracy:0.9594 test loss: 12.878654\n",
      "6310: accuracy:0.95 loss: 16.41048\n",
      "6320: accuracy:0.97 loss: 9.975304\n",
      "6330: accuracy:0.96 loss: 8.7288475\n",
      "6340: accuracy:0.97 loss: 9.502925\n",
      "6350: accuracy:0.91 loss: 19.148327\n",
      "6350: ********* epoch 11 ********* test accuracy:0.96 test loss: 13.337082\n",
      "6360: accuracy:0.98 loss: 5.697833\n",
      "6370: accuracy:0.95 loss: 19.195663\n",
      "6380: accuracy:0.96 loss: 8.663831\n",
      "6390: accuracy:0.99 loss: 8.775953\n",
      "6400: accuracy:0.96 loss: 7.215051\n",
      "6400: ********* epoch 11 ********* test accuracy:0.9629 test loss: 12.421186\n",
      "6410: accuracy:0.96 loss: 12.644072\n",
      "6420: accuracy:0.95 loss: 10.3031435\n",
      "6430: accuracy:0.98 loss: 8.436251\n",
      "6440: accuracy:0.99 loss: 4.2665877\n",
      "6450: accuracy:0.98 loss: 7.9675727\n",
      "6450: ********* epoch 11 ********* test accuracy:0.961 test loss: 12.656866\n",
      "6460: accuracy:0.98 loss: 11.528447\n",
      "6470: accuracy:0.98 loss: 7.1337824\n",
      "6480: accuracy:0.98 loss: 7.3708525\n",
      "6490: accuracy:0.98 loss: 8.082426\n",
      "6500: accuracy:0.98 loss: 7.940283\n",
      "6500: ********* epoch 11 ********* test accuracy:0.9591 test loss: 13.464505\n",
      "6510: accuracy:0.97 loss: 12.905072\n",
      "6520: accuracy:0.98 loss: 8.031328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6530: accuracy:0.97 loss: 13.899887\n",
      "6540: accuracy:0.96 loss: 8.947302\n",
      "6550: accuracy:0.98 loss: 8.9988365\n",
      "6550: ********* epoch 11 ********* test accuracy:0.9614 test loss: 12.903944\n",
      "6560: accuracy:0.96 loss: 12.402272\n",
      "6570: accuracy:0.96 loss: 7.21712\n",
      "6580: accuracy:0.94 loss: 17.075228\n",
      "6590: accuracy:0.94 loss: 19.822632\n",
      "6600: accuracy:0.95 loss: 11.718634\n",
      "6600: ********* epoch 12 ********* test accuracy:0.9612 test loss: 12.414102\n",
      "6610: accuracy:0.97 loss: 8.113736\n",
      "6620: accuracy:0.98 loss: 7.1806946\n",
      "6630: accuracy:0.97 loss: 8.872014\n",
      "6640: accuracy:0.98 loss: 5.5715666\n",
      "6650: accuracy:0.92 loss: 19.690075\n",
      "6650: ********* epoch 12 ********* test accuracy:0.9616 test loss: 12.708803\n",
      "6660: accuracy:0.97 loss: 10.045698\n",
      "6670: accuracy:0.98 loss: 7.245981\n",
      "6680: accuracy:0.99 loss: 5.574781\n",
      "6690: accuracy:0.97 loss: 11.037828\n",
      "6700: accuracy:0.95 loss: 17.884106\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9602 test loss: 12.564885\n",
      "6710: accuracy:0.96 loss: 12.674487\n",
      "6720: accuracy:0.98 loss: 9.9402275\n",
      "6730: accuracy:0.98 loss: 12.774791\n",
      "6740: accuracy:0.95 loss: 13.153165\n",
      "6750: accuracy:0.96 loss: 12.976505\n",
      "6750: ********* epoch 12 ********* test accuracy:0.9631 test loss: 12.398057\n",
      "6760: accuracy:0.94 loss: 18.134892\n",
      "6770: accuracy:0.97 loss: 9.547735\n",
      "6780: accuracy:0.95 loss: 12.435457\n",
      "6790: accuracy:0.99 loss: 5.5275464\n",
      "6800: accuracy:0.96 loss: 13.851627\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9628 test loss: 12.294163\n",
      "6810: accuracy:0.97 loss: 10.1728325\n",
      "6820: accuracy:1.0 loss: 4.995283\n",
      "6830: accuracy:0.99 loss: 7.004197\n",
      "6840: accuracy:0.98 loss: 4.177514\n",
      "6850: accuracy:0.99 loss: 5.7297783\n",
      "6850: ********* epoch 12 ********* test accuracy:0.9605 test loss: 12.466437\n",
      "6860: accuracy:0.98 loss: 9.889194\n",
      "6870: accuracy:0.97 loss: 8.902754\n",
      "6880: accuracy:0.97 loss: 7.1216764\n",
      "6890: accuracy:0.96 loss: 13.123344\n",
      "6900: accuracy:0.96 loss: 14.534879\n",
      "6900: ********* epoch 12 ********* test accuracy:0.96 test loss: 13.09606\n",
      "6910: accuracy:0.99 loss: 5.8837757\n",
      "6920: accuracy:0.98 loss: 7.495544\n",
      "6930: accuracy:0.94 loss: 20.69728\n",
      "6940: accuracy:0.99 loss: 5.639183\n",
      "6950: accuracy:0.94 loss: 16.987793\n",
      "6950: ********* epoch 12 ********* test accuracy:0.962 test loss: 12.1736765\n",
      "6960: accuracy:0.97 loss: 8.966926\n",
      "6970: accuracy:0.98 loss: 5.318547\n",
      "6980: accuracy:0.97 loss: 9.333426\n",
      "6990: accuracy:0.97 loss: 12.050942\n",
      "7000: accuracy:0.99 loss: 5.7259502\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9634 test loss: 11.845487\n",
      "7010: accuracy:0.97 loss: 13.730114\n",
      "7020: accuracy:0.96 loss: 19.102837\n",
      "7030: accuracy:0.95 loss: 9.698465\n",
      "7040: accuracy:0.99 loss: 7.2959056\n",
      "7050: accuracy:0.97 loss: 12.392887\n",
      "7050: ********* epoch 12 ********* test accuracy:0.9634 test loss: 11.703003\n",
      "7060: accuracy:0.96 loss: 9.790797\n",
      "7070: accuracy:0.93 loss: 18.974554\n",
      "7080: accuracy:0.96 loss: 11.524393\n",
      "7090: accuracy:0.94 loss: 16.359663\n",
      "7100: accuracy:0.95 loss: 13.05863\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9645 test loss: 11.752392\n",
      "7110: accuracy:0.97 loss: 10.491847\n",
      "7120: accuracy:0.98 loss: 8.218533\n",
      "7130: accuracy:0.97 loss: 14.565922\n",
      "7140: accuracy:0.95 loss: 10.584091\n",
      "7150: accuracy:0.96 loss: 11.69333\n",
      "7150: ********* epoch 12 ********* test accuracy:0.961 test loss: 12.690555\n",
      "7160: accuracy:0.99 loss: 5.503502\n",
      "7170: accuracy:0.97 loss: 11.52792\n",
      "7180: accuracy:0.97 loss: 7.665268\n",
      "7190: accuracy:1.0 loss: 6.593206\n",
      "7200: accuracy:0.99 loss: 6.5425167\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9636 test loss: 11.698289\n",
      "7210: accuracy:0.96 loss: 10.473734\n",
      "7220: accuracy:0.96 loss: 11.800052\n",
      "7230: accuracy:0.97 loss: 10.624046\n",
      "7240: accuracy:0.95 loss: 11.443977\n",
      "7250: accuracy:0.96 loss: 11.0574875\n",
      "7250: ********* epoch 13 ********* test accuracy:0.9633 test loss: 11.990264\n",
      "7260: accuracy:1.0 loss: 2.3546424\n",
      "7270: accuracy:0.97 loss: 7.873406\n",
      "7280: accuracy:0.95 loss: 20.571789\n",
      "7290: accuracy:0.94 loss: 13.668184\n",
      "7300: accuracy:0.97 loss: 14.72818\n",
      "7300: ********* epoch 13 ********* test accuracy:0.9641 test loss: 11.87633\n",
      "7310: accuracy:0.96 loss: 8.632857\n",
      "7320: accuracy:0.95 loss: 11.095232\n",
      "7330: accuracy:0.98 loss: 7.0252075\n",
      "7340: accuracy:0.96 loss: 11.163559\n",
      "7350: accuracy:0.98 loss: 10.252351\n",
      "7350: ********* epoch 13 ********* test accuracy:0.9648 test loss: 11.529464\n",
      "7360: accuracy:0.97 loss: 11.443989\n",
      "7370: accuracy:0.99 loss: 7.939746\n",
      "7380: accuracy:0.99 loss: 5.0227914\n",
      "7390: accuracy:0.96 loss: 12.861654\n",
      "7400: accuracy:0.98 loss: 10.984423\n",
      "7400: ********* epoch 13 ********* test accuracy:0.9635 test loss: 11.762742\n",
      "7410: accuracy:0.96 loss: 19.987839\n",
      "7420: accuracy:0.97 loss: 12.488493\n",
      "7430: accuracy:0.98 loss: 11.202689\n",
      "7440: accuracy:0.94 loss: 22.653748\n",
      "7450: accuracy:0.98 loss: 5.8903866\n",
      "7450: ********* epoch 13 ********* test accuracy:0.9644 test loss: 11.244399\n",
      "7460: accuracy:0.98 loss: 4.4887447\n",
      "7470: accuracy:0.98 loss: 6.457688\n",
      "7480: accuracy:0.96 loss: 11.749584\n",
      "7490: accuracy:1.0 loss: 4.1258087\n",
      "7500: accuracy:0.95 loss: 12.282377\n",
      "7500: ********* epoch 13 ********* test accuracy:0.9637 test loss: 11.340483\n",
      "7510: accuracy:0.96 loss: 15.096221\n",
      "7520: accuracy:0.99 loss: 5.2432823\n",
      "7530: accuracy:0.98 loss: 9.453318\n",
      "7540: accuracy:0.98 loss: 8.406851\n",
      "7550: accuracy:0.99 loss: 6.472763\n",
      "7550: ********* epoch 13 ********* test accuracy:0.9642 test loss: 11.270031\n",
      "7560: accuracy:0.97 loss: 14.129553\n",
      "7570: accuracy:0.99 loss: 9.640491\n",
      "7580: accuracy:0.97 loss: 13.092976\n",
      "7590: accuracy:0.94 loss: 14.182827\n",
      "7600: accuracy:0.98 loss: 8.803983\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9651 test loss: 11.409762\n",
      "7610: accuracy:0.99 loss: 4.923004\n",
      "7620: accuracy:0.99 loss: 4.470937\n",
      "7630: accuracy:0.93 loss: 18.059435\n",
      "7640: accuracy:0.97 loss: 7.4750843\n",
      "7650: accuracy:0.99 loss: 6.4827576\n",
      "7650: ********* epoch 13 ********* test accuracy:0.9647 test loss: 11.358254\n",
      "7660: accuracy:0.98 loss: 8.988796\n",
      "7670: accuracy:0.96 loss: 13.966576\n",
      "7680: accuracy:0.99 loss: 4.233363\n",
      "7690: accuracy:1.0 loss: 1.7522753\n",
      "7700: accuracy:0.96 loss: 11.360411\n",
      "7700: ********* epoch 13 ********* test accuracy:0.9644 test loss: 11.617947\n",
      "7710: accuracy:0.97 loss: 11.535544\n",
      "7720: accuracy:0.99 loss: 7.247703\n",
      "7730: accuracy:1.0 loss: 5.5055256\n",
      "7740: accuracy:0.97 loss: 10.916809\n",
      "7750: accuracy:0.97 loss: 11.246744\n",
      "7750: ********* epoch 13 ********* test accuracy:0.9652 test loss: 11.074903\n",
      "7760: accuracy:0.98 loss: 8.501932\n",
      "7770: accuracy:0.96 loss: 17.636017\n",
      "7780: accuracy:0.97 loss: 8.94561\n",
      "7790: accuracy:0.99 loss: 4.32633\n",
      "7800: accuracy:0.94 loss: 14.449593\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9643 test loss: 11.330005\n",
      "7810: accuracy:0.96 loss: 8.445445\n",
      "7820: accuracy:1.0 loss: 3.638562\n",
      "7830: accuracy:0.96 loss: 14.011181\n",
      "7840: accuracy:1.0 loss: 4.009439\n",
      "7850: accuracy:0.98 loss: 9.938269\n",
      "7850: ********* epoch 14 ********* test accuracy:0.9657 test loss: 11.500892\n",
      "7860: accuracy:0.97 loss: 7.49201\n",
      "7870: accuracy:0.98 loss: 12.412351\n",
      "7880: accuracy:0.96 loss: 9.361869\n",
      "7890: accuracy:0.99 loss: 5.689146\n",
      "7900: accuracy:0.95 loss: 22.145836\n",
      "7900: ********* epoch 14 ********* test accuracy:0.9644 test loss: 11.272541\n",
      "7910: accuracy:0.95 loss: 9.322832\n",
      "7920: accuracy:0.99 loss: 7.323537\n",
      "7930: accuracy:0.97 loss: 13.431579\n",
      "7940: accuracy:0.99 loss: 5.4365172\n",
      "7950: accuracy:0.99 loss: 6.8285084\n",
      "7950: ********* epoch 14 ********* test accuracy:0.9647 test loss: 11.31055\n",
      "7960: accuracy:0.97 loss: 10.734071\n",
      "7970: accuracy:0.98 loss: 9.2189455\n",
      "7980: accuracy:0.98 loss: 5.872719\n",
      "7990: accuracy:0.99 loss: 8.038059\n",
      "8000: accuracy:0.98 loss: 6.075533\n",
      "8000: ********* epoch 14 ********* test accuracy:0.9659 test loss: 11.254146\n",
      "8010: accuracy:0.97 loss: 9.183944\n",
      "8020: accuracy:0.99 loss: 3.6613953\n",
      "8030: accuracy:0.98 loss: 6.2643948\n",
      "8040: accuracy:0.95 loss: 20.07069\n",
      "8050: accuracy:1.0 loss: 2.5451117\n",
      "8050: ********* epoch 14 ********* test accuracy:0.9654 test loss: 11.049454\n",
      "8060: accuracy:0.98 loss: 5.9329348\n",
      "8070: accuracy:0.97 loss: 11.349216\n",
      "8080: accuracy:0.97 loss: 13.818254\n",
      "8090: accuracy:0.95 loss: 17.456009\n",
      "8100: accuracy:0.97 loss: 8.98935\n",
      "8100: ********* epoch 14 ********* test accuracy:0.9634 test loss: 11.393239\n",
      "8110: accuracy:0.99 loss: 6.906228\n",
      "8120: accuracy:0.98 loss: 11.48187\n",
      "8130: accuracy:0.97 loss: 8.199508\n",
      "8140: accuracy:0.97 loss: 16.356407\n",
      "8150: accuracy:0.98 loss: 6.4048543\n",
      "8150: ********* epoch 14 ********* test accuracy:0.9651 test loss: 10.820042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8160: accuracy:0.98 loss: 20.365234\n",
      "8170: accuracy:0.95 loss: 16.62328\n",
      "8180: accuracy:0.95 loss: 10.542373\n",
      "8190: accuracy:0.97 loss: 7.860432\n",
      "8200: accuracy:0.97 loss: 10.369928\n",
      "8200: ********* epoch 14 ********* test accuracy:0.9663 test loss: 10.804397\n",
      "8210: accuracy:0.99 loss: 6.265766\n",
      "8220: accuracy:0.98 loss: 5.163719\n",
      "8230: accuracy:1.0 loss: 3.1666865\n",
      "8240: accuracy:0.93 loss: 26.288536\n",
      "8250: accuracy:1.0 loss: 4.677339\n",
      "8250: ********* epoch 14 ********* test accuracy:0.9649 test loss: 10.958618\n",
      "8260: accuracy:0.97 loss: 8.746966\n",
      "8270: accuracy:0.98 loss: 5.897494\n",
      "8280: accuracy:1.0 loss: 3.6752274\n",
      "8290: accuracy:0.99 loss: 5.7940893\n",
      "8300: accuracy:0.97 loss: 8.988296\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9673 test loss: 10.567816\n",
      "8310: accuracy:0.97 loss: 8.347408\n",
      "8320: accuracy:0.96 loss: 10.6558695\n",
      "8330: accuracy:1.0 loss: 2.1028214\n",
      "8340: accuracy:0.94 loss: 15.193929\n",
      "8350: accuracy:0.99 loss: 6.4450593\n",
      "8350: ********* epoch 14 ********* test accuracy:0.9658 test loss: 10.738074\n",
      "8360: accuracy:0.96 loss: 16.103985\n",
      "8370: accuracy:0.96 loss: 19.681055\n",
      "8380: accuracy:0.98 loss: 7.422139\n",
      "8390: accuracy:0.97 loss: 14.722469\n",
      "8400: accuracy:0.98 loss: 6.157719\n",
      "8400: ********* epoch 15 ********* test accuracy:0.9663 test loss: 11.070282\n",
      "8410: accuracy:0.94 loss: 15.0457535\n",
      "8420: accuracy:0.95 loss: 12.479105\n",
      "8430: accuracy:0.98 loss: 12.658142\n",
      "8440: accuracy:0.95 loss: 22.775434\n",
      "8450: accuracy:0.97 loss: 10.051801\n",
      "8450: ********* epoch 15 ********* test accuracy:0.9644 test loss: 11.077362\n",
      "8460: accuracy:0.98 loss: 5.421549\n",
      "8470: accuracy:0.98 loss: 9.4722185\n",
      "8480: accuracy:1.0 loss: 1.3071805\n",
      "8490: accuracy:0.99 loss: 4.3937855\n",
      "8500: accuracy:0.99 loss: 4.740958\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9675 test loss: 10.278111\n",
      "8510: accuracy:0.96 loss: 15.180776\n",
      "8520: accuracy:0.99 loss: 4.2121725\n",
      "8530: accuracy:0.99 loss: 3.4415226\n",
      "8540: accuracy:0.93 loss: 17.716358\n",
      "8550: accuracy:0.96 loss: 10.984102\n",
      "8550: ********* epoch 15 ********* test accuracy:0.9663 test loss: 10.541769\n",
      "8560: accuracy:0.94 loss: 24.318731\n",
      "8570: accuracy:0.97 loss: 7.769622\n",
      "8580: accuracy:0.95 loss: 10.172496\n",
      "8590: accuracy:0.98 loss: 3.415545\n",
      "8600: accuracy:0.97 loss: 9.165398\n",
      "8600: ********* epoch 15 ********* test accuracy:0.9664 test loss: 10.590583\n",
      "8610: accuracy:0.97 loss: 13.135282\n",
      "8620: accuracy:0.99 loss: 5.6918507\n",
      "8630: accuracy:0.99 loss: 6.834787\n",
      "8640: accuracy:0.98 loss: 7.4523964\n",
      "8650: accuracy:0.98 loss: 8.525112\n",
      "8650: ********* epoch 15 ********* test accuracy:0.9671 test loss: 10.433873\n",
      "8660: accuracy:0.97 loss: 10.821066\n",
      "8670: accuracy:0.96 loss: 12.776395\n",
      "8680: accuracy:0.96 loss: 15.550409\n",
      "8690: accuracy:0.97 loss: 7.6944675\n",
      "8700: accuracy:0.99 loss: 5.534019\n",
      "8700: ********* epoch 15 ********* test accuracy:0.9658 test loss: 10.8857565\n",
      "8710: accuracy:0.98 loss: 6.9815483\n",
      "8720: accuracy:0.96 loss: 10.472315\n",
      "8730: accuracy:0.98 loss: 8.839046\n",
      "8740: accuracy:0.96 loss: 16.800793\n",
      "8750: accuracy:0.95 loss: 18.612303\n",
      "8750: ********* epoch 15 ********* test accuracy:0.9666 test loss: 10.91771\n",
      "8760: accuracy:0.98 loss: 7.833651\n",
      "8770: accuracy:0.96 loss: 8.3536625\n",
      "8780: accuracy:0.97 loss: 6.689887\n",
      "8790: accuracy:0.98 loss: 7.6918597\n",
      "8800: accuracy:0.99 loss: 5.66966\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9667 test loss: 10.415261\n",
      "8810: accuracy:0.99 loss: 6.2522016\n",
      "8820: accuracy:0.99 loss: 3.795486\n",
      "8830: accuracy:0.97 loss: 10.148561\n",
      "8840: accuracy:0.98 loss: 5.696125\n",
      "8850: accuracy:0.99 loss: 4.532047\n",
      "8850: ********* epoch 15 ********* test accuracy:0.9674 test loss: 10.310653\n",
      "8860: accuracy:0.99 loss: 2.7623584\n",
      "8870: accuracy:0.97 loss: 9.152715\n",
      "8880: accuracy:0.97 loss: 6.3406405\n",
      "8890: accuracy:0.97 loss: 8.998168\n",
      "8900: accuracy:0.98 loss: 7.5324717\n",
      "8900: ********* epoch 15 ********* test accuracy:0.967 test loss: 11.134144\n",
      "8910: accuracy:0.97 loss: 11.877079\n",
      "8920: accuracy:0.98 loss: 7.9883437\n",
      "8930: accuracy:0.98 loss: 6.944058\n",
      "8940: accuracy:0.98 loss: 13.734108\n",
      "8950: accuracy:0.99 loss: 5.1257234\n",
      "8950: ********* epoch 15 ********* test accuracy:0.967 test loss: 10.351462\n",
      "8960: accuracy:0.98 loss: 4.6174407\n",
      "8970: accuracy:0.97 loss: 9.172763\n",
      "8980: accuracy:0.96 loss: 14.804656\n",
      "8990: accuracy:0.99 loss: 3.7855747\n",
      "9000: accuracy:0.98 loss: 5.8641443\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9675 test loss: 10.6201\n",
      "9010: accuracy:0.98 loss: 7.450572\n",
      "9020: accuracy:0.98 loss: 4.419812\n",
      "9030: accuracy:0.97 loss: 6.298383\n",
      "9040: accuracy:0.98 loss: 6.8322697\n",
      "9050: accuracy:0.97 loss: 8.742941\n",
      "9050: ********* epoch 16 ********* test accuracy:0.9686 test loss: 10.235701\n",
      "9060: accuracy:0.97 loss: 6.984124\n",
      "9070: accuracy:0.95 loss: 11.82415\n",
      "9080: accuracy:0.98 loss: 6.010297\n",
      "9090: accuracy:0.99 loss: 4.489506\n",
      "9100: accuracy:1.0 loss: 3.4986267\n",
      "9100: ********* epoch 16 ********* test accuracy:0.966 test loss: 10.532964\n",
      "9110: accuracy:0.98 loss: 7.142752\n",
      "9120: accuracy:0.97 loss: 6.171739\n",
      "9130: accuracy:0.97 loss: 13.2322235\n",
      "9140: accuracy:0.99 loss: 3.2330358\n",
      "9150: accuracy:0.99 loss: 5.3873873\n",
      "9150: ********* epoch 16 ********* test accuracy:0.9669 test loss: 10.572652\n",
      "9160: accuracy:0.98 loss: 6.8087215\n",
      "9170: accuracy:0.99 loss: 5.46731\n",
      "9180: accuracy:0.99 loss: 4.9585457\n",
      "9190: accuracy:1.0 loss: 3.944357\n",
      "9200: accuracy:0.98 loss: 5.4373074\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9677 test loss: 10.25197\n",
      "9210: accuracy:0.99 loss: 5.107735\n",
      "9220: accuracy:1.0 loss: 2.016673\n",
      "9230: accuracy:0.98 loss: 8.271223\n",
      "9240: accuracy:0.99 loss: 7.8734307\n",
      "9250: accuracy:0.99 loss: 9.864321\n",
      "9250: ********* epoch 16 ********* test accuracy:0.9678 test loss: 10.099124\n",
      "9260: accuracy:0.96 loss: 11.294992\n",
      "9270: accuracy:0.99 loss: 5.383562\n",
      "9280: accuracy:1.0 loss: 3.1910787\n",
      "9290: accuracy:0.98 loss: 5.475197\n",
      "9300: accuracy:0.99 loss: 4.7482066\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9686 test loss: 10.125817\n",
      "9310: accuracy:0.98 loss: 6.9203997\n",
      "9320: accuracy:0.93 loss: 19.992859\n",
      "9330: accuracy:0.98 loss: 7.0756135\n",
      "9340: accuracy:0.95 loss: 27.372087\n",
      "9350: accuracy:0.97 loss: 8.389711\n",
      "9350: ********* epoch 16 ********* test accuracy:0.9679 test loss: 10.046229\n",
      "9360: accuracy:0.97 loss: 7.294118\n",
      "9370: accuracy:0.97 loss: 9.044197\n",
      "9380: accuracy:0.98 loss: 8.548929\n",
      "9390: accuracy:0.98 loss: 6.730064\n",
      "9400: accuracy:0.97 loss: 8.318346\n",
      "9400: ********* epoch 16 ********* test accuracy:0.969 test loss: 9.819028\n",
      "9410: accuracy:1.0 loss: 2.589101\n",
      "9420: accuracy:0.97 loss: 13.901309\n",
      "9430: accuracy:0.98 loss: 11.094329\n",
      "9440: accuracy:0.97 loss: 7.577011\n",
      "9450: accuracy:0.98 loss: 8.849077\n",
      "9450: ********* epoch 16 ********* test accuracy:0.9685 test loss: 9.805411\n",
      "9460: accuracy:0.98 loss: 7.9946313\n",
      "9470: accuracy:0.98 loss: 5.1230526\n",
      "9480: accuracy:0.97 loss: 8.217726\n",
      "9490: accuracy:0.99 loss: 4.90215\n",
      "9500: accuracy:0.98 loss: 6.8029084\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9676 test loss: 10.05142\n",
      "9510: accuracy:0.98 loss: 7.0794783\n",
      "9520: accuracy:1.0 loss: 2.6472907\n",
      "9530: accuracy:0.98 loss: 9.329584\n",
      "9540: accuracy:0.97 loss: 15.861957\n",
      "9550: accuracy:0.99 loss: 6.205043\n",
      "9550: ********* epoch 16 ********* test accuracy:0.9681 test loss: 9.927677\n",
      "9560: accuracy:0.99 loss: 3.1582165\n",
      "9570: accuracy:0.99 loss: 3.2907395\n",
      "9580: accuracy:0.97 loss: 6.5648894\n",
      "9590: accuracy:0.97 loss: 13.418709\n",
      "9600: accuracy:0.98 loss: 7.2418976\n",
      "9600: ********* epoch 17 ********* test accuracy:0.9686 test loss: 10.128311\n",
      "9610: accuracy:0.99 loss: 3.737822\n",
      "9620: accuracy:0.99 loss: 3.796366\n",
      "9630: accuracy:0.98 loss: 5.8867826\n",
      "9640: accuracy:0.98 loss: 8.910151\n",
      "9650: accuracy:0.99 loss: 5.0611286\n",
      "9650: ********* epoch 17 ********* test accuracy:0.9692 test loss: 9.849092\n",
      "9660: accuracy:1.0 loss: 3.6403284\n",
      "9670: accuracy:0.98 loss: 5.0308347\n",
      "9680: accuracy:0.95 loss: 11.82605\n",
      "9690: accuracy:0.94 loss: 10.879147\n",
      "9700: accuracy:0.95 loss: 9.809486\n",
      "9700: ********* epoch 17 ********* test accuracy:0.969 test loss: 9.70905\n",
      "9710: accuracy:0.99 loss: 4.292083\n",
      "9720: accuracy:0.99 loss: 7.200624\n",
      "9730: accuracy:0.98 loss: 7.7908273\n",
      "9740: accuracy:0.99 loss: 4.900146\n",
      "9750: accuracy:0.98 loss: 6.6258116\n",
      "9750: ********* epoch 17 ********* test accuracy:0.9673 test loss: 10.065693\n",
      "9760: accuracy:0.98 loss: 7.7887735\n",
      "9770: accuracy:0.99 loss: 3.8115416\n",
      "9780: accuracy:0.99 loss: 8.015873\n",
      "9790: accuracy:0.98 loss: 6.7406197\n",
      "9800: accuracy:1.0 loss: 1.764757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9800: ********* epoch 17 ********* test accuracy:0.9686 test loss: 9.810943\n",
      "9810: accuracy:0.98 loss: 5.7278395\n",
      "9820: accuracy:0.99 loss: 7.9013042\n",
      "9830: accuracy:0.99 loss: 4.3569655\n",
      "9840: accuracy:0.99 loss: 3.3928196\n",
      "9850: accuracy:0.98 loss: 8.872281\n",
      "9850: ********* epoch 17 ********* test accuracy:0.968 test loss: 10.078445\n",
      "9860: accuracy:0.99 loss: 4.5093794\n",
      "9870: accuracy:0.98 loss: 3.3539186\n",
      "9880: accuracy:1.0 loss: 1.8833988\n",
      "9890: accuracy:0.97 loss: 8.967636\n",
      "9900: accuracy:0.98 loss: 7.2819486\n",
      "9900: ********* epoch 17 ********* test accuracy:0.9689 test loss: 9.775102\n",
      "9910: accuracy:0.94 loss: 19.295069\n",
      "9920: accuracy:0.96 loss: 8.999803\n",
      "9930: accuracy:0.97 loss: 9.632845\n",
      "9940: accuracy:0.98 loss: 10.82271\n",
      "9950: accuracy:0.95 loss: 15.226115\n",
      "9950: ********* epoch 17 ********* test accuracy:0.969 test loss: 9.695992\n",
      "9960: accuracy:0.98 loss: 4.7576327\n",
      "9970: accuracy:0.98 loss: 5.388066\n",
      "9980: accuracy:0.98 loss: 6.1032686\n",
      "9990: accuracy:0.97 loss: 6.5358934\n",
      "10000: accuracy:0.98 loss: 5.25508\n",
      "10000: ********* epoch 17 ********* test accuracy:0.9696 test loss: 9.781621\n",
      "max test accuracy: 0.9696\n"
     ]
    }
   ],
   "source": [
    "# input X => 28x28 grayscale dimensions ('None' indexes images in mini-batch)\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "\n",
    "# correct answers\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights\n",
    "W1 = tf.Variable(tf.truncated_normal([28*28, 200], stddev=0.1))\n",
    "W2 = tf.Variable(tf.truncated_normal([200, 100], stddev=0.1))\n",
    "W3 = tf.Variable(tf.truncated_normal([100, 10], stddev=0.1))\n",
    "\n",
    "# biases\n",
    "b1 = tf.Variable(tf.zeros([200]))\n",
    "b2 = tf.Variable(tf.zeros([100]))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# flatten images into single line\n",
    "XX = tf.reshape(X, [-1, 28*28])\n",
    "\n",
    "# model\n",
    "Y1 = tf.nn.sigmoid(tf.matmul(XX, W1) + b1)\n",
    "Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + b2)\n",
    "Y = tf.nn.softmax(tf.matmul(Y2, W3) + b3)\n",
    "\n",
    "# cross entropy\n",
    "cross_entropy = -tf.reduce_mean(Y_ * tf.log(Y)) * 1000.0\n",
    "\n",
    "# accuracy of the trained model [0, 1] ([worst, best])\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# training (learning rate is 0.003)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.003).minimize(cross_entropy)\n",
    "\n",
    "# initialization\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# call in loop to train the model 100 images at a time\n",
    "max_test_accuracy = 0.0\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    # train on batches of 100 images w/ 100 labels\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    # compute training values for visualisation\n",
    "    if update_train_data:\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: batch_X, Y_: batch_Y})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c))\n",
    "    # compute test values for visualization\n",
    "    if update_test_data:\n",
    "        global max_test_accuracy\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: mnist.test.images, Y_: mnist.test.labels})\n",
    "        if (a > max_test_accuracy):\n",
    "            max_test_accuracy = a\n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \n",
    "              \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        \n",
    "    # back-propogation training step\n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y})\n",
    "\n",
    "# text visualization of process\n",
    "for i in range(10000+1):\n",
    "    training_step(i, i % 50 == 0, i % 10 == 0)\n",
    "    \n",
    "# display the final max accuracy\n",
    "print(\"max test accuracy: \" + str(max_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Convergence with RELU\n",
    "The following cell contains even more optimizations to the code above that even greater improves the accuracy of the network. This specific example uses RELU (rectified linear unit) as an activation function instead of sigmoid, which provides a faster initial convergence and avoids problems with additional layers. Why? Sigmoid squashes all values between 0 and 1, resulting in neuron outputs and their gradients vanishing entirely. With 10k iterations, it maxes out with an accuracy of ~98%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.1 loss: 232.89574\n",
      "0: ********* epoch 1 ********* test accuracy:0.0974 test loss: 237.99121\n",
      "10: accuracy:0.61 loss: 140.30359\n",
      "20: accuracy:0.74 loss: 80.05866\n",
      "30: accuracy:0.79 loss: 62.93773\n",
      "40: accuracy:0.76 loss: 72.89701\n",
      "50: accuracy:0.84 loss: 48.733402\n",
      "50: ********* epoch 1 ********* test accuracy:0.8782 test loss: 43.16954\n",
      "60: accuracy:0.8 loss: 51.52118\n",
      "70: accuracy:0.9 loss: 31.022991\n",
      "80: accuracy:0.92 loss: 39.781654\n",
      "90: accuracy:0.89 loss: 40.246407\n",
      "100: accuracy:0.88 loss: 36.902454\n",
      "100: ********* epoch 1 ********* test accuracy:0.9015 test loss: 32.591778\n",
      "110: accuracy:0.9 loss: 30.38063\n",
      "120: accuracy:0.93 loss: 23.891836\n",
      "130: accuracy:0.93 loss: 27.31713\n",
      "140: accuracy:0.93 loss: 26.227203\n",
      "150: accuracy:0.91 loss: 33.503147\n",
      "150: ********* epoch 1 ********* test accuracy:0.851 test loss: 45.40488\n",
      "160: accuracy:0.89 loss: 31.632944\n",
      "170: accuracy:0.96 loss: 19.740503\n",
      "180: accuracy:0.91 loss: 26.831738\n",
      "190: accuracy:0.95 loss: 18.229511\n",
      "200: accuracy:0.93 loss: 26.574696\n",
      "200: ********* epoch 1 ********* test accuracy:0.92 test loss: 26.67456\n",
      "210: accuracy:0.95 loss: 25.507278\n",
      "220: accuracy:0.96 loss: 23.7326\n",
      "230: accuracy:0.87 loss: 36.59532\n",
      "240: accuracy:0.89 loss: 40.978924\n",
      "250: accuracy:0.96 loss: 13.558937\n",
      "250: ********* epoch 1 ********* test accuracy:0.9297 test loss: 22.896223\n",
      "260: accuracy:0.9 loss: 23.10281\n",
      "270: accuracy:0.9 loss: 25.412577\n",
      "280: accuracy:0.93 loss: 18.371727\n",
      "290: accuracy:0.91 loss: 23.060162\n",
      "300: accuracy:0.85 loss: 35.86708\n",
      "300: ********* epoch 1 ********* test accuracy:0.9435 test loss: 19.206007\n",
      "310: accuracy:0.9 loss: 40.16709\n",
      "320: accuracy:0.96 loss: 15.07915\n",
      "330: accuracy:0.91 loss: 37.973404\n",
      "340: accuracy:0.98 loss: 10.8601\n",
      "350: accuracy:0.94 loss: 25.564339\n",
      "350: ********* epoch 1 ********* test accuracy:0.9443 test loss: 19.089174\n",
      "360: accuracy:0.94 loss: 23.176617\n",
      "370: accuracy:0.92 loss: 26.688995\n",
      "380: accuracy:0.9 loss: 30.771423\n",
      "390: accuracy:0.96 loss: 10.760633\n",
      "400: accuracy:0.96 loss: 16.977365\n",
      "400: ********* epoch 1 ********* test accuracy:0.953 test loss: 16.088945\n",
      "410: accuracy:0.95 loss: 23.090233\n",
      "420: accuracy:0.93 loss: 26.531054\n",
      "430: accuracy:0.96 loss: 11.541453\n",
      "440: accuracy:0.95 loss: 13.799112\n",
      "450: accuracy:0.94 loss: 15.752407\n",
      "450: ********* epoch 1 ********* test accuracy:0.9518 test loss: 16.625622\n",
      "460: accuracy:0.93 loss: 32.101593\n",
      "470: accuracy:0.97 loss: 12.457996\n",
      "480: accuracy:0.94 loss: 24.04662\n",
      "490: accuracy:0.98 loss: 6.1610084\n",
      "500: accuracy:0.94 loss: 21.972406\n",
      "500: ********* epoch 1 ********* test accuracy:0.9553 test loss: 15.31478\n",
      "510: accuracy:0.93 loss: 16.666609\n",
      "520: accuracy:0.96 loss: 11.960319\n",
      "530: accuracy:0.96 loss: 12.357229\n",
      "540: accuracy:0.95 loss: 17.257988\n",
      "550: accuracy:0.94 loss: 14.836146\n",
      "550: ********* epoch 1 ********* test accuracy:0.9559 test loss: 14.5564165\n",
      "560: accuracy:0.95 loss: 13.257528\n",
      "570: accuracy:0.97 loss: 9.312697\n",
      "580: accuracy:0.96 loss: 15.426342\n",
      "590: accuracy:0.92 loss: 21.715755\n",
      "600: accuracy:0.96 loss: 9.587676\n",
      "600: ********* epoch 2 ********* test accuracy:0.956 test loss: 14.308265\n",
      "610: accuracy:0.95 loss: 12.354133\n",
      "620: accuracy:0.99 loss: 8.612619\n",
      "630: accuracy:0.94 loss: 15.142122\n",
      "640: accuracy:0.98 loss: 12.695459\n",
      "650: accuracy:1.0 loss: 3.5378256\n",
      "650: ********* epoch 2 ********* test accuracy:0.9572 test loss: 13.569782\n",
      "660: accuracy:0.95 loss: 24.8242\n",
      "670: accuracy:0.97 loss: 15.479231\n",
      "680: accuracy:0.97 loss: 10.460472\n",
      "690: accuracy:0.94 loss: 14.443989\n",
      "700: accuracy:0.98 loss: 9.036672\n",
      "700: ********* epoch 2 ********* test accuracy:0.9601 test loss: 12.750444\n",
      "710: accuracy:0.93 loss: 24.32266\n",
      "720: accuracy:0.98 loss: 7.8228655\n",
      "730: accuracy:0.95 loss: 16.237394\n",
      "740: accuracy:0.95 loss: 13.978084\n",
      "750: accuracy:0.96 loss: 10.176916\n",
      "750: ********* epoch 2 ********* test accuracy:0.9641 test loss: 11.853413\n",
      "760: accuracy:0.95 loss: 17.362875\n",
      "770: accuracy:0.94 loss: 18.814367\n",
      "780: accuracy:0.96 loss: 10.834376\n",
      "790: accuracy:0.96 loss: 13.168703\n",
      "800: accuracy:0.94 loss: 13.801381\n",
      "800: ********* epoch 2 ********* test accuracy:0.9607 test loss: 12.4304085\n",
      "810: accuracy:0.98 loss: 9.06665\n",
      "820: accuracy:0.96 loss: 8.529925\n",
      "830: accuracy:0.96 loss: 9.891661\n",
      "840: accuracy:0.98 loss: 6.4905033\n",
      "850: accuracy:0.97 loss: 17.631493\n",
      "850: ********* epoch 2 ********* test accuracy:0.9639 test loss: 11.8506155\n",
      "860: accuracy:0.98 loss: 7.8617306\n",
      "870: accuracy:0.98 loss: 6.360183\n",
      "880: accuracy:0.95 loss: 20.15665\n",
      "890: accuracy:0.96 loss: 9.256866\n",
      "900: accuracy:0.96 loss: 15.993646\n",
      "900: ********* epoch 2 ********* test accuracy:0.9657 test loss: 11.294399\n",
      "910: accuracy:0.96 loss: 13.11117\n",
      "920: accuracy:0.97 loss: 5.603218\n",
      "930: accuracy:0.91 loss: 23.042805\n",
      "940: accuracy:0.96 loss: 10.941815\n",
      "950: accuracy:0.97 loss: 10.880652\n",
      "950: ********* epoch 2 ********* test accuracy:0.9619 test loss: 12.249709\n",
      "960: accuracy:1.0 loss: 2.646774\n",
      "970: accuracy:0.96 loss: 12.909679\n",
      "980: accuracy:0.97 loss: 8.766031\n",
      "990: accuracy:0.95 loss: 20.911304\n",
      "1000: accuracy:0.96 loss: 10.419182\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9645 test loss: 11.667964\n",
      "1010: accuracy:0.99 loss: 3.6801147\n",
      "1020: accuracy:0.99 loss: 4.1173463\n",
      "1030: accuracy:0.92 loss: 20.039661\n",
      "1040: accuracy:0.96 loss: 11.802997\n",
      "1050: accuracy:0.95 loss: 10.420711\n",
      "1050: ********* epoch 2 ********* test accuracy:0.9654 test loss: 11.094091\n",
      "1060: accuracy:0.97 loss: 15.07602\n",
      "1070: accuracy:0.97 loss: 5.940943\n",
      "1080: accuracy:0.99 loss: 3.6176286\n",
      "1090: accuracy:0.99 loss: 8.6836605\n",
      "1100: accuracy:0.96 loss: 11.263362\n",
      "1100: ********* epoch 2 ********* test accuracy:0.9669 test loss: 11.02461\n",
      "1110: accuracy:0.96 loss: 8.844798\n",
      "1120: accuracy:0.98 loss: 5.9454556\n",
      "1130: accuracy:0.99 loss: 6.1145105\n",
      "1140: accuracy:0.97 loss: 8.240299\n",
      "1150: accuracy:0.98 loss: 11.050142\n",
      "1150: ********* epoch 2 ********* test accuracy:0.9637 test loss: 11.117717\n",
      "1160: accuracy:0.98 loss: 6.6658792\n",
      "1170: accuracy:0.96 loss: 14.019403\n",
      "1180: accuracy:0.98 loss: 8.192174\n",
      "1190: accuracy:0.94 loss: 22.675945\n",
      "1200: accuracy:0.97 loss: 18.456797\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9669 test loss: 10.67258\n",
      "1210: accuracy:0.96 loss: 10.221945\n",
      "1220: accuracy:0.98 loss: 8.354261\n",
      "1230: accuracy:0.97 loss: 9.568635\n",
      "1240: accuracy:1.0 loss: 1.5276862\n",
      "1250: accuracy:0.95 loss: 12.598036\n",
      "1250: ********* epoch 3 ********* test accuracy:0.9577 test loss: 13.333829\n",
      "1260: accuracy:0.98 loss: 15.5545435\n",
      "1270: accuracy:0.95 loss: 11.1318655\n",
      "1280: accuracy:0.97 loss: 6.349181\n",
      "1290: accuracy:1.0 loss: 3.4468489\n",
      "1300: accuracy:0.97 loss: 8.753145\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9658 test loss: 10.485922\n",
      "1310: accuracy:0.97 loss: 8.701389\n",
      "1320: accuracy:0.99 loss: 3.2265234\n",
      "1330: accuracy:0.97 loss: 5.298664\n",
      "1340: accuracy:0.98 loss: 6.6410217\n",
      "1350: accuracy:0.99 loss: 5.315259\n",
      "1350: ********* epoch 3 ********* test accuracy:0.9691 test loss: 9.917402\n",
      "1360: accuracy:0.99 loss: 3.1225085\n",
      "1370: accuracy:0.97 loss: 9.95007\n",
      "1380: accuracy:0.95 loss: 9.295152\n",
      "1390: accuracy:0.99 loss: 6.0296116\n",
      "1400: accuracy:1.0 loss: 4.5716233\n",
      "1400: ********* epoch 3 ********* test accuracy:0.9671 test loss: 10.219646\n",
      "1410: accuracy:0.99 loss: 5.306978\n",
      "1420: accuracy:0.99 loss: 6.0360637\n",
      "1430: accuracy:0.97 loss: 7.8299336\n",
      "1440: accuracy:0.97 loss: 18.638088\n",
      "1450: accuracy:1.0 loss: 1.9459945\n",
      "1450: ********* epoch 3 ********* test accuracy:0.9683 test loss: 9.848435\n",
      "1460: accuracy:0.97 loss: 13.685215\n",
      "1470: accuracy:1.0 loss: 3.331501\n",
      "1480: accuracy:0.96 loss: 9.604388\n",
      "1490: accuracy:0.96 loss: 8.913652\n",
      "1500: accuracy:0.99 loss: 4.4813633\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9724 test loss: 9.127026\n",
      "1510: accuracy:1.0 loss: 4.3331556\n",
      "1520: accuracy:0.99 loss: 2.1930943\n",
      "1530: accuracy:0.95 loss: 12.899738\n",
      "1540: accuracy:0.93 loss: 18.931858\n",
      "1550: accuracy:0.98 loss: 5.9672036\n",
      "1550: ********* epoch 3 ********* test accuracy:0.9711 test loss: 9.049459\n",
      "1560: accuracy:0.97 loss: 6.7822447\n",
      "1570: accuracy:0.97 loss: 11.955101\n",
      "1580: accuracy:1.0 loss: 2.9390068\n",
      "1590: accuracy:0.96 loss: 10.327175\n",
      "1600: accuracy:0.99 loss: 3.1612358\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9693 test loss: 9.566327\n",
      "1610: accuracy:0.99 loss: 3.4041169\n",
      "1620: accuracy:0.94 loss: 13.190798\n",
      "1630: accuracy:0.99 loss: 5.585248\n",
      "1640: accuracy:0.98 loss: 7.122536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1650: accuracy:0.99 loss: 4.3056107\n",
      "1650: ********* epoch 3 ********* test accuracy:0.9739 test loss: 8.500366\n",
      "1660: accuracy:0.97 loss: 13.111\n",
      "1670: accuracy:0.98 loss: 5.5574074\n",
      "1680: accuracy:0.99 loss: 3.9045806\n",
      "1690: accuracy:1.0 loss: 3.577617\n",
      "1700: accuracy:0.99 loss: 9.3973055\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9723 test loss: 8.984367\n",
      "1710: accuracy:0.98 loss: 5.6321077\n",
      "1720: accuracy:0.97 loss: 8.517115\n",
      "1730: accuracy:0.96 loss: 20.516169\n",
      "1740: accuracy:0.98 loss: 7.3997397\n",
      "1750: accuracy:0.99 loss: 4.5821238\n",
      "1750: ********* epoch 3 ********* test accuracy:0.9652 test loss: 11.439956\n",
      "1760: accuracy:0.99 loss: 5.025984\n",
      "1770: accuracy:0.97 loss: 6.9095783\n",
      "1780: accuracy:0.98 loss: 6.616021\n",
      "1790: accuracy:0.95 loss: 11.257166\n",
      "1800: accuracy:0.97 loss: 6.062848\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9708 test loss: 9.688369\n",
      "1810: accuracy:0.98 loss: 4.061741\n",
      "1820: accuracy:0.98 loss: 5.8079624\n",
      "1830: accuracy:0.98 loss: 7.037604\n",
      "1840: accuracy:0.99 loss: 4.045975\n",
      "1850: accuracy:0.98 loss: 5.1243906\n",
      "1850: ********* epoch 4 ********* test accuracy:0.974 test loss: 8.155417\n",
      "1860: accuracy:0.98 loss: 9.46183\n",
      "1870: accuracy:0.98 loss: 4.4198513\n",
      "1880: accuracy:0.99 loss: 3.4152074\n",
      "1890: accuracy:0.98 loss: 6.162886\n",
      "1900: accuracy:0.98 loss: 9.260067\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9718 test loss: 9.269326\n",
      "1910: accuracy:0.99 loss: 2.8334694\n",
      "1920: accuracy:0.98 loss: 4.2166944\n",
      "1930: accuracy:0.97 loss: 6.754882\n",
      "1940: accuracy:0.98 loss: 3.663817\n",
      "1950: accuracy:0.98 loss: 6.0758877\n",
      "1950: ********* epoch 4 ********* test accuracy:0.9729 test loss: 8.338683\n",
      "1960: accuracy:0.99 loss: 4.754942\n",
      "1970: accuracy:0.93 loss: 22.386679\n",
      "1980: accuracy:0.98 loss: 6.022393\n",
      "1990: accuracy:0.97 loss: 8.702905\n",
      "2000: accuracy:1.0 loss: 1.8842151\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9731 test loss: 8.708348\n",
      "2010: accuracy:0.98 loss: 3.5027854\n",
      "2020: accuracy:1.0 loss: 2.4752798\n",
      "2030: accuracy:0.98 loss: 8.283298\n",
      "2040: accuracy:0.97 loss: 8.000347\n",
      "2050: accuracy:0.99 loss: 7.745067\n",
      "2050: ********* epoch 4 ********* test accuracy:0.9734 test loss: 8.857771\n",
      "2060: accuracy:1.0 loss: 2.8766298\n",
      "2070: accuracy:0.99 loss: 2.5010302\n",
      "2080: accuracy:0.97 loss: 6.8418818\n",
      "2090: accuracy:0.97 loss: 4.9984674\n",
      "2100: accuracy:0.98 loss: 5.7203927\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9736 test loss: 8.162474\n",
      "2110: accuracy:0.98 loss: 4.4693465\n",
      "2120: accuracy:0.98 loss: 4.555753\n",
      "2130: accuracy:0.99 loss: 2.1008017\n",
      "2140: accuracy:0.99 loss: 2.6639106\n",
      "2150: accuracy:0.99 loss: 2.6821315\n",
      "2150: ********* epoch 4 ********* test accuracy:0.9743 test loss: 8.767639\n",
      "2160: accuracy:1.0 loss: 2.8261826\n",
      "2170: accuracy:0.98 loss: 11.11141\n",
      "2180: accuracy:0.97 loss: 4.5041447\n",
      "2190: accuracy:0.96 loss: 12.828891\n",
      "2200: accuracy:0.99 loss: 5.955509\n",
      "2200: ********* epoch 4 ********* test accuracy:0.974 test loss: 8.181746\n",
      "2210: accuracy:0.96 loss: 11.553801\n",
      "2220: accuracy:0.97 loss: 5.2092977\n",
      "2230: accuracy:0.95 loss: 12.540305\n",
      "2240: accuracy:0.98 loss: 3.0267007\n",
      "2250: accuracy:0.99 loss: 2.511654\n",
      "2250: ********* epoch 4 ********* test accuracy:0.9735 test loss: 8.042233\n",
      "2260: accuracy:0.97 loss: 8.578401\n",
      "2270: accuracy:0.96 loss: 7.046738\n",
      "2280: accuracy:0.99 loss: 2.6055865\n",
      "2290: accuracy:0.98 loss: 5.169258\n",
      "2300: accuracy:0.97 loss: 7.3930655\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9718 test loss: 8.979342\n",
      "2310: accuracy:0.99 loss: 2.8773005\n",
      "2320: accuracy:0.99 loss: 2.9819453\n",
      "2330: accuracy:0.98 loss: 3.8160448\n",
      "2340: accuracy:0.95 loss: 15.511541\n",
      "2350: accuracy:0.99 loss: 2.733274\n",
      "2350: ********* epoch 4 ********* test accuracy:0.9733 test loss: 8.400411\n",
      "2360: accuracy:1.0 loss: 1.4608606\n",
      "2370: accuracy:0.99 loss: 2.6065485\n",
      "2380: accuracy:0.97 loss: 14.957296\n",
      "2390: accuracy:0.99 loss: 4.012535\n",
      "2400: accuracy:1.0 loss: 1.7765279\n",
      "2400: ********* epoch 5 ********* test accuracy:0.973 test loss: 8.713472\n",
      "2410: accuracy:0.98 loss: 5.6053042\n",
      "2420: accuracy:0.98 loss: 7.4168186\n",
      "2430: accuracy:0.99 loss: 1.9946822\n",
      "2440: accuracy:0.98 loss: 3.2401628\n",
      "2450: accuracy:0.98 loss: 4.9108667\n",
      "2450: ********* epoch 5 ********* test accuracy:0.9751 test loss: 8.109175\n",
      "2460: accuracy:0.99 loss: 4.890292\n",
      "2470: accuracy:0.97 loss: 7.6240025\n",
      "2480: accuracy:0.97 loss: 8.914005\n",
      "2490: accuracy:1.0 loss: 3.4777884\n",
      "2500: accuracy:0.99 loss: 11.844476\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9723 test loss: 9.242892\n",
      "2510: accuracy:0.99 loss: 8.634076\n",
      "2520: accuracy:1.0 loss: 1.9036894\n",
      "2530: accuracy:0.97 loss: 7.970209\n",
      "2540: accuracy:0.99 loss: 2.5214026\n",
      "2550: accuracy:0.98 loss: 6.9522986\n",
      "2550: ********* epoch 5 ********* test accuracy:0.9745 test loss: 7.902853\n",
      "2560: accuracy:0.99 loss: 3.7552123\n",
      "2570: accuracy:0.98 loss: 4.3413653\n",
      "2580: accuracy:1.0 loss: 1.1685519\n",
      "2590: accuracy:1.0 loss: 1.3741605\n",
      "2600: accuracy:0.99 loss: 2.2992213\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9738 test loss: 7.8347454\n",
      "2610: accuracy:1.0 loss: 2.3093877\n",
      "2620: accuracy:0.97 loss: 4.7760935\n",
      "2630: accuracy:1.0 loss: 2.5748801\n",
      "2640: accuracy:1.0 loss: 0.70419854\n",
      "2650: accuracy:0.99 loss: 2.9633873\n",
      "2650: ********* epoch 5 ********* test accuracy:0.9763 test loss: 7.766323\n",
      "2660: accuracy:1.0 loss: 0.5002538\n",
      "2670: accuracy:0.97 loss: 9.889192\n",
      "2680: accuracy:0.99 loss: 2.5663671\n",
      "2690: accuracy:1.0 loss: 1.5261977\n",
      "2700: accuracy:0.99 loss: 1.6164562\n",
      "2700: ********* epoch 5 ********* test accuracy:0.975 test loss: 8.286579\n",
      "2710: accuracy:0.99 loss: 1.6524374\n",
      "2720: accuracy:1.0 loss: 2.3927422\n",
      "2730: accuracy:0.98 loss: 10.873885\n",
      "2740: accuracy:0.99 loss: 3.4224043\n",
      "2750: accuracy:0.99 loss: 4.156484\n",
      "2750: ********* epoch 5 ********* test accuracy:0.9743 test loss: 8.27055\n",
      "2760: accuracy:1.0 loss: 1.135782\n",
      "2770: accuracy:1.0 loss: 2.952342\n",
      "2780: accuracy:0.98 loss: 5.9988537\n",
      "2790: accuracy:0.98 loss: 6.7626295\n",
      "2800: accuracy:0.97 loss: 5.407754\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9796 test loss: 6.676775\n",
      "2810: accuracy:0.97 loss: 7.014963\n",
      "2820: accuracy:0.99 loss: 6.4358225\n",
      "2830: accuracy:1.0 loss: 1.2833358\n",
      "2840: accuracy:0.98 loss: 4.601075\n",
      "2850: accuracy:0.99 loss: 5.3396497\n",
      "2850: ********* epoch 5 ********* test accuracy:0.9769 test loss: 7.6412845\n",
      "2860: accuracy:0.99 loss: 3.3060575\n",
      "2870: accuracy:1.0 loss: 2.5204744\n",
      "2880: accuracy:0.97 loss: 9.033405\n",
      "2890: accuracy:0.96 loss: 11.74806\n",
      "2900: accuracy:0.97 loss: 6.388826\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9751 test loss: 7.676041\n",
      "2910: accuracy:0.99 loss: 7.610848\n",
      "2920: accuracy:1.0 loss: 2.6427708\n",
      "2930: accuracy:0.98 loss: 6.11014\n",
      "2940: accuracy:0.98 loss: 4.673625\n",
      "2950: accuracy:0.99 loss: 1.9856421\n",
      "2950: ********* epoch 5 ********* test accuracy:0.9761 test loss: 7.3376737\n",
      "2960: accuracy:0.99 loss: 2.1465304\n",
      "2970: accuracy:0.99 loss: 5.689552\n",
      "2980: accuracy:0.99 loss: 5.0190334\n",
      "2990: accuracy:0.97 loss: 10.097174\n",
      "3000: accuracy:0.99 loss: 6.7886543\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9763 test loss: 7.4228015\n",
      "3010: accuracy:1.0 loss: 1.876029\n",
      "3020: accuracy:1.0 loss: 2.2740498\n",
      "3030: accuracy:1.0 loss: 1.074528\n",
      "3040: accuracy:0.99 loss: 1.9483031\n",
      "3050: accuracy:1.0 loss: 1.1727617\n",
      "3050: ********* epoch 6 ********* test accuracy:0.9781 test loss: 7.022872\n",
      "3060: accuracy:0.96 loss: 8.399204\n",
      "3070: accuracy:0.99 loss: 2.5928202\n",
      "3080: accuracy:0.99 loss: 8.476524\n",
      "3090: accuracy:0.98 loss: 3.8841465\n",
      "3100: accuracy:1.0 loss: 3.6779094\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9777 test loss: 7.3147593\n",
      "3110: accuracy:0.98 loss: 3.9801846\n",
      "3120: accuracy:0.98 loss: 3.3272703\n",
      "3130: accuracy:1.0 loss: 1.410264\n",
      "3140: accuracy:1.0 loss: 0.95519984\n",
      "3150: accuracy:0.98 loss: 4.5466886\n",
      "3150: ********* epoch 6 ********* test accuracy:0.9773 test loss: 7.5830936\n",
      "3160: accuracy:1.0 loss: 2.082563\n",
      "3170: accuracy:1.0 loss: 0.45275563\n",
      "3180: accuracy:0.99 loss: 3.7832885\n",
      "3190: accuracy:0.99 loss: 1.8263081\n",
      "3200: accuracy:0.99 loss: 3.166977\n",
      "3200: ********* epoch 6 ********* test accuracy:0.977 test loss: 7.777828\n",
      "3210: accuracy:0.98 loss: 2.4272418\n",
      "3220: accuracy:0.99 loss: 2.4396882\n",
      "3230: accuracy:1.0 loss: 2.071272\n",
      "3240: accuracy:0.99 loss: 4.0254793\n",
      "3250: accuracy:0.99 loss: 2.8444662\n",
      "3250: ********* epoch 6 ********* test accuracy:0.9776 test loss: 6.9690247\n",
      "3260: accuracy:1.0 loss: 2.0991793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3270: accuracy:0.99 loss: 3.7717857\n",
      "3280: accuracy:0.98 loss: 7.539109\n",
      "3290: accuracy:1.0 loss: 2.0746663\n",
      "3300: accuracy:1.0 loss: 1.4355849\n",
      "3300: ********* epoch 6 ********* test accuracy:0.9748 test loss: 7.8949676\n",
      "3310: accuracy:1.0 loss: 1.7544398\n",
      "3320: accuracy:0.99 loss: 4.051076\n",
      "3330: accuracy:0.98 loss: 3.4971766\n",
      "3340: accuracy:1.0 loss: 0.7702863\n",
      "3350: accuracy:0.99 loss: 3.6711526\n",
      "3350: ********* epoch 6 ********* test accuracy:0.9783 test loss: 7.3844914\n",
      "3360: accuracy:0.98 loss: 4.981498\n",
      "3370: accuracy:0.99 loss: 3.1280534\n",
      "3380: accuracy:0.96 loss: 7.7708282\n",
      "3390: accuracy:1.0 loss: 1.978823\n",
      "3400: accuracy:0.99 loss: 3.0092092\n",
      "3400: ********* epoch 6 ********* test accuracy:0.9786 test loss: 7.168137\n",
      "3410: accuracy:1.0 loss: 1.4656708\n",
      "3420: accuracy:0.93 loss: 18.156311\n",
      "3430: accuracy:1.0 loss: 1.3288258\n",
      "3440: accuracy:0.97 loss: 10.9586735\n",
      "3450: accuracy:0.96 loss: 9.062937\n",
      "3450: ********* epoch 6 ********* test accuracy:0.9754 test loss: 8.435006\n",
      "3460: accuracy:0.99 loss: 6.7081723\n",
      "3470: accuracy:1.0 loss: 1.0757587\n",
      "3480: accuracy:0.98 loss: 4.076334\n",
      "3490: accuracy:1.0 loss: 1.597167\n",
      "3500: accuracy:0.97 loss: 13.485861\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9772 test loss: 7.5193834\n",
      "3510: accuracy:0.99 loss: 2.5108113\n",
      "3520: accuracy:0.97 loss: 7.651954\n",
      "3530: accuracy:1.0 loss: 1.604118\n",
      "3540: accuracy:1.0 loss: 0.99495625\n",
      "3550: accuracy:1.0 loss: 1.8943985\n",
      "3550: ********* epoch 6 ********* test accuracy:0.9727 test loss: 8.460174\n",
      "3560: accuracy:0.99 loss: 2.2492075\n",
      "3570: accuracy:0.99 loss: 1.424423\n",
      "3580: accuracy:0.99 loss: 4.778272\n",
      "3590: accuracy:1.0 loss: 0.8998935\n",
      "3600: accuracy:1.0 loss: 1.0838221\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9782 test loss: 7.5272155\n",
      "3610: accuracy:1.0 loss: 0.4683084\n",
      "3620: accuracy:1.0 loss: 0.5160008\n",
      "3630: accuracy:1.0 loss: 0.23837097\n",
      "3640: accuracy:1.0 loss: 1.3384724\n",
      "3650: accuracy:0.99 loss: 3.0123296\n",
      "3650: ********* epoch 7 ********* test accuracy:0.9787 test loss: 7.460513\n",
      "3660: accuracy:1.0 loss: 0.63988954\n",
      "3670: accuracy:0.99 loss: 1.5926299\n",
      "3680: accuracy:0.99 loss: 6.3335505\n",
      "3690: accuracy:0.98 loss: 7.442792\n",
      "3700: accuracy:0.98 loss: 3.516905\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9734 test loss: 8.501449\n",
      "3710: accuracy:0.99 loss: 2.470563\n",
      "3720: accuracy:1.0 loss: 0.78016824\n",
      "3730: accuracy:1.0 loss: 0.31784713\n",
      "3740: accuracy:1.0 loss: 1.3783734\n",
      "3750: accuracy:1.0 loss: 0.8491736\n",
      "3750: ********* epoch 7 ********* test accuracy:0.9779 test loss: 7.517299\n",
      "3760: accuracy:1.0 loss: 1.3497311\n",
      "3770: accuracy:0.99 loss: 1.8765341\n",
      "3780: accuracy:0.98 loss: 4.5059385\n",
      "3790: accuracy:0.98 loss: 4.955265\n",
      "3800: accuracy:0.98 loss: 6.6187544\n",
      "3800: ********* epoch 7 ********* test accuracy:0.977 test loss: 7.4242697\n",
      "3810: accuracy:1.0 loss: 0.62674004\n",
      "3820: accuracy:1.0 loss: 0.68784\n",
      "3830: accuracy:0.99 loss: 3.840438\n",
      "3840: accuracy:0.99 loss: 3.0489912\n",
      "3850: accuracy:0.97 loss: 7.0720277\n",
      "3850: ********* epoch 7 ********* test accuracy:0.976 test loss: 8.0056305\n",
      "3860: accuracy:0.99 loss: 2.914724\n",
      "3870: accuracy:1.0 loss: 1.1034842\n",
      "3880: accuracy:0.99 loss: 2.3341997\n",
      "3890: accuracy:0.99 loss: 5.151651\n",
      "3900: accuracy:0.98 loss: 6.5787215\n",
      "3900: ********* epoch 7 ********* test accuracy:0.975 test loss: 8.23658\n",
      "3910: accuracy:0.97 loss: 9.89013\n",
      "3920: accuracy:0.97 loss: 4.560835\n",
      "3930: accuracy:0.97 loss: 6.03353\n",
      "3940: accuracy:1.0 loss: 1.6362369\n",
      "3950: accuracy:0.98 loss: 3.1914158\n",
      "3950: ********* epoch 7 ********* test accuracy:0.9756 test loss: 7.8906493\n",
      "3960: accuracy:1.0 loss: 0.97869337\n",
      "3970: accuracy:0.98 loss: 4.5472183\n",
      "3980: accuracy:1.0 loss: 1.4143512\n",
      "3990: accuracy:0.98 loss: 4.3348637\n",
      "4000: accuracy:0.99 loss: 1.7981775\n",
      "4000: ********* epoch 7 ********* test accuracy:0.9759 test loss: 7.924079\n",
      "4010: accuracy:1.0 loss: 1.4271898\n",
      "4020: accuracy:0.98 loss: 4.449094\n",
      "4030: accuracy:0.98 loss: 4.1049604\n",
      "4040: accuracy:1.0 loss: 1.3455544\n",
      "4050: accuracy:0.99 loss: 3.3559046\n",
      "4050: ********* epoch 7 ********* test accuracy:0.9752 test loss: 8.17593\n",
      "4060: accuracy:1.0 loss: 1.0839016\n",
      "4070: accuracy:0.99 loss: 1.1785183\n",
      "4080: accuracy:1.0 loss: 0.7205738\n",
      "4090: accuracy:1.0 loss: 0.95667505\n",
      "4100: accuracy:1.0 loss: 0.8378205\n",
      "4100: ********* epoch 7 ********* test accuracy:0.9774 test loss: 7.4288397\n",
      "4110: accuracy:0.99 loss: 1.9732562\n",
      "4120: accuracy:1.0 loss: 1.4263843\n",
      "4130: accuracy:0.97 loss: 7.705775\n",
      "4140: accuracy:0.95 loss: 8.383811\n",
      "4150: accuracy:1.0 loss: 2.762088\n",
      "4150: ********* epoch 7 ********* test accuracy:0.978 test loss: 7.5725417\n",
      "4160: accuracy:0.98 loss: 6.2174196\n",
      "4170: accuracy:1.0 loss: 1.6482906\n",
      "4180: accuracy:1.0 loss: 1.932562\n",
      "4190: accuracy:0.98 loss: 4.4878635\n",
      "4200: accuracy:0.98 loss: 3.756121\n",
      "4200: ********* epoch 8 ********* test accuracy:0.98 test loss: 7.162523\n",
      "4210: accuracy:0.99 loss: 4.7780104\n",
      "4220: accuracy:1.0 loss: 1.1139462\n",
      "4230: accuracy:1.0 loss: 0.31565994\n",
      "4240: accuracy:1.0 loss: 0.889702\n",
      "4250: accuracy:1.0 loss: 0.6687006\n",
      "4250: ********* epoch 8 ********* test accuracy:0.9796 test loss: 6.916608\n",
      "4260: accuracy:1.0 loss: 0.35118908\n",
      "4270: accuracy:1.0 loss: 1.3755163\n",
      "4280: accuracy:1.0 loss: 1.4008471\n",
      "4290: accuracy:1.0 loss: 0.46184158\n",
      "4300: accuracy:0.97 loss: 6.9259152\n",
      "4300: ********* epoch 8 ********* test accuracy:0.9776 test loss: 7.6606555\n",
      "4310: accuracy:1.0 loss: 0.16390833\n",
      "4320: accuracy:0.97 loss: 5.079739\n",
      "4330: accuracy:0.97 loss: 3.7014766\n",
      "4340: accuracy:1.0 loss: 1.401139\n",
      "4350: accuracy:1.0 loss: 0.687227\n",
      "4350: ********* epoch 8 ********* test accuracy:0.9792 test loss: 7.1352854\n",
      "4360: accuracy:0.99 loss: 2.3783927\n",
      "4370: accuracy:1.0 loss: 2.3625224\n",
      "4380: accuracy:0.98 loss: 4.3012776\n",
      "4390: accuracy:0.99 loss: 15.4480295\n",
      "4400: accuracy:1.0 loss: 0.9107175\n",
      "4400: ********* epoch 8 ********* test accuracy:0.9793 test loss: 7.1047144\n",
      "4410: accuracy:1.0 loss: 2.2466643\n",
      "4420: accuracy:0.98 loss: 3.587755\n",
      "4430: accuracy:1.0 loss: 1.440977\n",
      "4440: accuracy:0.99 loss: 1.3468611\n",
      "4450: accuracy:0.99 loss: 3.2660267\n",
      "4450: ********* epoch 8 ********* test accuracy:0.979 test loss: 7.2996583\n",
      "4460: accuracy:0.99 loss: 2.1854854\n",
      "4470: accuracy:0.99 loss: 2.9387708\n",
      "4480: accuracy:1.0 loss: 2.3717458\n",
      "4490: accuracy:1.0 loss: 1.7206035\n",
      "4500: accuracy:0.98 loss: 5.1389956\n",
      "4500: ********* epoch 8 ********* test accuracy:0.9781 test loss: 7.452946\n",
      "4510: accuracy:1.0 loss: 0.93769014\n",
      "4520: accuracy:0.99 loss: 1.7700335\n",
      "4530: accuracy:0.98 loss: 2.7928905\n",
      "4540: accuracy:0.99 loss: 3.9878604\n",
      "4550: accuracy:0.99 loss: 2.146733\n",
      "4550: ********* epoch 8 ********* test accuracy:0.9804 test loss: 6.9822097\n",
      "4560: accuracy:1.0 loss: 1.3081402\n",
      "4570: accuracy:0.98 loss: 5.2035694\n",
      "4580: accuracy:0.98 loss: 9.360899\n",
      "4590: accuracy:0.99 loss: 2.8202517\n",
      "4600: accuracy:1.0 loss: 0.7121963\n",
      "4600: ********* epoch 8 ********* test accuracy:0.9795 test loss: 7.207334\n",
      "4610: accuracy:0.99 loss: 4.825069\n",
      "4620: accuracy:0.99 loss: 5.061276\n",
      "4630: accuracy:0.99 loss: 2.4104412\n",
      "4640: accuracy:1.0 loss: 0.8340014\n",
      "4650: accuracy:0.99 loss: 3.7378004\n",
      "4650: ********* epoch 8 ********* test accuracy:0.9775 test loss: 7.587394\n",
      "4660: accuracy:1.0 loss: 0.68430805\n",
      "4670: accuracy:1.0 loss: 1.5205461\n",
      "4680: accuracy:0.98 loss: 3.417498\n",
      "4690: accuracy:1.0 loss: 0.8138153\n",
      "4700: accuracy:1.0 loss: 0.93425554\n",
      "4700: ********* epoch 8 ********* test accuracy:0.978 test loss: 7.6840158\n",
      "4710: accuracy:1.0 loss: 0.90292704\n",
      "4720: accuracy:0.98 loss: 12.194906\n",
      "4730: accuracy:1.0 loss: 0.4729218\n",
      "4740: accuracy:0.99 loss: 2.0066752\n",
      "4750: accuracy:0.99 loss: 2.7220266\n",
      "4750: ********* epoch 8 ********* test accuracy:0.9784 test loss: 7.2273326\n",
      "4760: accuracy:1.0 loss: 0.70869035\n",
      "4770: accuracy:1.0 loss: 1.6391072\n",
      "4780: accuracy:0.99 loss: 6.1583276\n",
      "4790: accuracy:0.99 loss: 2.6697593\n",
      "4800: accuracy:0.99 loss: 1.7861427\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9788 test loss: 7.3036156\n",
      "4810: accuracy:1.0 loss: 0.4348687\n",
      "4820: accuracy:1.0 loss: 1.472119\n",
      "4830: accuracy:0.99 loss: 3.1252015\n",
      "4840: accuracy:1.0 loss: 0.26294044\n",
      "4850: accuracy:0.99 loss: 1.764097\n",
      "4850: ********* epoch 9 ********* test accuracy:0.9814 test loss: 6.9368715\n",
      "4860: accuracy:1.0 loss: 1.2681956\n",
      "4870: accuracy:1.0 loss: 0.70734334\n",
      "4880: accuracy:1.0 loss: 0.3742632\n",
      "4890: accuracy:0.99 loss: 1.1941524\n",
      "4900: accuracy:1.0 loss: 0.2573008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4900: ********* epoch 9 ********* test accuracy:0.9799 test loss: 7.222501\n",
      "4910: accuracy:1.0 loss: 1.0087247\n",
      "4920: accuracy:0.99 loss: 4.011354\n",
      "4930: accuracy:1.0 loss: 1.7806596\n",
      "4940: accuracy:1.0 loss: 0.9371427\n",
      "4950: accuracy:1.0 loss: 0.6652443\n",
      "4950: ********* epoch 9 ********* test accuracy:0.9787 test loss: 7.0022845\n",
      "4960: accuracy:1.0 loss: 0.40542984\n",
      "4970: accuracy:0.97 loss: 4.56106\n",
      "4980: accuracy:1.0 loss: 1.0314465\n",
      "4990: accuracy:1.0 loss: 0.34937954\n",
      "5000: accuracy:1.0 loss: 0.9270545\n",
      "5000: ********* epoch 9 ********* test accuracy:0.9782 test loss: 7.94613\n",
      "5010: accuracy:1.0 loss: 0.6102452\n",
      "5020: accuracy:1.0 loss: 0.55428666\n",
      "5030: accuracy:0.99 loss: 1.1459125\n",
      "5040: accuracy:1.0 loss: 0.65061355\n",
      "5050: accuracy:1.0 loss: 0.9458269\n",
      "5050: ********* epoch 9 ********* test accuracy:0.978 test loss: 7.3949413\n",
      "5060: accuracy:1.0 loss: 0.5625415\n",
      "5070: accuracy:1.0 loss: 0.28795522\n",
      "5080: accuracy:1.0 loss: 0.9756131\n",
      "5090: accuracy:1.0 loss: 0.83272314\n",
      "5100: accuracy:1.0 loss: 0.42138827\n",
      "5100: ********* epoch 9 ********* test accuracy:0.98 test loss: 7.1601515\n",
      "5110: accuracy:0.99 loss: 3.8571038\n",
      "5120: accuracy:1.0 loss: 0.76450443\n",
      "5130: accuracy:1.0 loss: 0.4994253\n",
      "5140: accuracy:1.0 loss: 0.5770239\n",
      "5150: accuracy:0.99 loss: 4.012299\n",
      "5150: ********* epoch 9 ********* test accuracy:0.9779 test loss: 7.533251\n",
      "5160: accuracy:0.97 loss: 8.03501\n",
      "5170: accuracy:0.99 loss: 2.2235713\n",
      "5180: accuracy:1.0 loss: 0.54934263\n",
      "5190: accuracy:1.0 loss: 0.32678866\n",
      "5200: accuracy:0.98 loss: 3.3617582\n",
      "5200: ********* epoch 9 ********* test accuracy:0.9763 test loss: 8.263046\n",
      "5210: accuracy:1.0 loss: 0.49259844\n",
      "5220: accuracy:1.0 loss: 1.2229782\n",
      "5230: accuracy:1.0 loss: 1.1770442\n",
      "5240: accuracy:1.0 loss: 2.0756402\n",
      "5250: accuracy:1.0 loss: 0.4116642\n",
      "5250: ********* epoch 9 ********* test accuracy:0.9786 test loss: 7.2380567\n",
      "5260: accuracy:0.99 loss: 2.06131\n",
      "5270: accuracy:1.0 loss: 0.61617625\n",
      "5280: accuracy:0.99 loss: 1.5360434\n",
      "5290: accuracy:0.98 loss: 3.6881585\n",
      "5300: accuracy:0.99 loss: 2.4063065\n",
      "5300: ********* epoch 9 ********* test accuracy:0.9785 test loss: 7.6815705\n",
      "5310: accuracy:0.99 loss: 2.4106648\n",
      "5320: accuracy:0.99 loss: 1.7401298\n",
      "5330: accuracy:0.98 loss: 3.1264298\n",
      "5340: accuracy:1.0 loss: 0.57327205\n",
      "5350: accuracy:1.0 loss: 0.7315271\n",
      "5350: ********* epoch 9 ********* test accuracy:0.9797 test loss: 7.208018\n",
      "5360: accuracy:0.99 loss: 1.9567313\n",
      "5370: accuracy:0.98 loss: 4.723499\n",
      "5380: accuracy:0.98 loss: 3.2792573\n",
      "5390: accuracy:0.98 loss: 3.1381931\n",
      "5400: accuracy:1.0 loss: 0.44111788\n",
      "5400: ********* epoch 10 ********* test accuracy:0.9791 test loss: 7.300018\n",
      "5410: accuracy:0.99 loss: 1.428623\n",
      "5420: accuracy:1.0 loss: 1.623923\n",
      "5430: accuracy:1.0 loss: 0.4665244\n",
      "5440: accuracy:1.0 loss: 0.8941767\n",
      "5450: accuracy:1.0 loss: 0.43342787\n",
      "5450: ********* epoch 10 ********* test accuracy:0.9801 test loss: 7.364345\n",
      "5460: accuracy:1.0 loss: 0.41771552\n",
      "5470: accuracy:1.0 loss: 0.26431045\n",
      "5480: accuracy:1.0 loss: 0.35114834\n",
      "5490: accuracy:1.0 loss: 0.31129703\n",
      "5500: accuracy:1.0 loss: 0.2624861\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9806 test loss: 6.9377403\n",
      "5510: accuracy:0.99 loss: 3.0939074\n",
      "5520: accuracy:1.0 loss: 0.5664882\n",
      "5530: accuracy:1.0 loss: 0.36664864\n",
      "5540: accuracy:1.0 loss: 0.3510798\n",
      "5550: accuracy:1.0 loss: 0.40221918\n",
      "5550: ********* epoch 10 ********* test accuracy:0.9805 test loss: 7.123507\n",
      "5560: accuracy:1.0 loss: 0.34770226\n",
      "5570: accuracy:1.0 loss: 1.2407951\n",
      "5580: accuracy:1.0 loss: 1.087182\n",
      "5590: accuracy:0.99 loss: 1.1100864\n",
      "5600: accuracy:1.0 loss: 1.3269725\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9782 test loss: 7.6505594\n",
      "5610: accuracy:1.0 loss: 0.15707079\n",
      "5620: accuracy:1.0 loss: 0.39301127\n",
      "5630: accuracy:1.0 loss: 0.3860363\n",
      "5640: accuracy:1.0 loss: 1.4182999\n",
      "5650: accuracy:0.99 loss: 3.0893583\n",
      "5650: ********* epoch 10 ********* test accuracy:0.9791 test loss: 7.2816396\n",
      "5660: accuracy:1.0 loss: 0.42718273\n",
      "5670: accuracy:1.0 loss: 0.996458\n",
      "5680: accuracy:1.0 loss: 1.033332\n",
      "5690: accuracy:1.0 loss: 0.2747088\n",
      "5700: accuracy:1.0 loss: 0.8028706\n",
      "5700: ********* epoch 10 ********* test accuracy:0.9802 test loss: 7.4874682\n",
      "5710: accuracy:0.98 loss: 5.818842\n",
      "5720: accuracy:1.0 loss: 0.72336316\n",
      "5730: accuracy:1.0 loss: 0.22480333\n",
      "5740: accuracy:1.0 loss: 0.60060996\n",
      "5750: accuracy:1.0 loss: 0.5036378\n",
      "5750: ********* epoch 10 ********* test accuracy:0.9805 test loss: 7.080342\n",
      "5760: accuracy:1.0 loss: 1.480237\n",
      "5770: accuracy:1.0 loss: 0.8431237\n",
      "5780: accuracy:0.99 loss: 3.0745134\n",
      "5790: accuracy:1.0 loss: 0.642118\n",
      "5800: accuracy:1.0 loss: 1.027183\n",
      "5800: ********* epoch 10 ********* test accuracy:0.9793 test loss: 7.771837\n",
      "5810: accuracy:1.0 loss: 0.69724846\n",
      "5820: accuracy:1.0 loss: 0.6040225\n",
      "5830: accuracy:1.0 loss: 0.7118529\n",
      "5840: accuracy:0.99 loss: 1.6483792\n",
      "5850: accuracy:1.0 loss: 1.1312366\n",
      "5850: ********* epoch 10 ********* test accuracy:0.9798 test loss: 7.4205904\n",
      "5860: accuracy:1.0 loss: 0.7225299\n",
      "5870: accuracy:1.0 loss: 0.09352498\n",
      "5880: accuracy:1.0 loss: 0.80508715\n",
      "5890: accuracy:1.0 loss: 0.7800716\n",
      "5900: accuracy:1.0 loss: 0.50520074\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9805 test loss: 7.099158\n",
      "5910: accuracy:0.98 loss: 4.5613\n",
      "5920: accuracy:0.98 loss: 3.5528555\n",
      "5930: accuracy:0.99 loss: 2.6440516\n",
      "5940: accuracy:1.0 loss: 0.41922873\n",
      "5950: accuracy:1.0 loss: 0.5465837\n",
      "5950: ********* epoch 10 ********* test accuracy:0.9802 test loss: 7.186173\n",
      "5960: accuracy:0.99 loss: 1.8182822\n",
      "5970: accuracy:1.0 loss: 0.98170966\n",
      "5980: accuracy:1.0 loss: 1.826793\n",
      "5990: accuracy:1.0 loss: 0.5466454\n",
      "6000: accuracy:1.0 loss: 0.04304256\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9815 test loss: 6.727539\n",
      "6010: accuracy:1.0 loss: 0.49763972\n",
      "6020: accuracy:1.0 loss: 0.36167625\n",
      "6030: accuracy:1.0 loss: 1.0702188\n",
      "6040: accuracy:0.99 loss: 2.0727696\n",
      "6050: accuracy:1.0 loss: 1.1730849\n",
      "6050: ********* epoch 11 ********* test accuracy:0.9792 test loss: 6.8951125\n",
      "6060: accuracy:1.0 loss: 0.18328932\n",
      "6070: accuracy:1.0 loss: 0.77639955\n",
      "6080: accuracy:1.0 loss: 0.9873677\n",
      "6090: accuracy:1.0 loss: 0.5597258\n",
      "6100: accuracy:1.0 loss: 0.40159187\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9792 test loss: 7.1287794\n",
      "6110: accuracy:1.0 loss: 0.16613068\n",
      "6120: accuracy:1.0 loss: 0.49471703\n",
      "6130: accuracy:1.0 loss: 0.45372343\n",
      "6140: accuracy:1.0 loss: 0.31500742\n",
      "6150: accuracy:0.99 loss: 1.9436677\n",
      "6150: ********* epoch 11 ********* test accuracy:0.9771 test loss: 7.7596765\n",
      "6160: accuracy:0.99 loss: 1.1011741\n",
      "6170: accuracy:1.0 loss: 0.9208097\n",
      "6180: accuracy:1.0 loss: 1.1631072\n",
      "6190: accuracy:1.0 loss: 1.3270692\n",
      "6200: accuracy:1.0 loss: 0.41854316\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9795 test loss: 6.9872794\n",
      "6210: accuracy:1.0 loss: 1.2218448\n",
      "6220: accuracy:1.0 loss: 1.2930448\n",
      "6230: accuracy:1.0 loss: 0.3961448\n",
      "6240: accuracy:1.0 loss: 0.14821352\n",
      "6250: accuracy:1.0 loss: 0.5123595\n",
      "6250: ********* epoch 11 ********* test accuracy:0.9814 test loss: 6.736818\n",
      "6260: accuracy:1.0 loss: 0.64938545\n",
      "6270: accuracy:1.0 loss: 0.34041637\n",
      "6280: accuracy:0.99 loss: 2.456128\n",
      "6290: accuracy:1.0 loss: 1.0281305\n",
      "6300: accuracy:1.0 loss: 0.5704288\n",
      "6300: ********* epoch 11 ********* test accuracy:0.9798 test loss: 7.3323865\n",
      "6310: accuracy:0.99 loss: 2.298444\n",
      "6320: accuracy:1.0 loss: 1.2271556\n",
      "6330: accuracy:1.0 loss: 0.13682066\n",
      "6340: accuracy:1.0 loss: 0.3156985\n",
      "6350: accuracy:1.0 loss: 1.1850255\n",
      "6350: ********* epoch 11 ********* test accuracy:0.9805 test loss: 7.2556996\n",
      "6360: accuracy:1.0 loss: 0.6315325\n",
      "6370: accuracy:0.98 loss: 4.079566\n",
      "6380: accuracy:1.0 loss: 0.9772747\n",
      "6390: accuracy:1.0 loss: 0.29431102\n",
      "6400: accuracy:1.0 loss: 0.33997056\n",
      "6400: ********* epoch 11 ********* test accuracy:0.9799 test loss: 7.7654133\n",
      "6410: accuracy:1.0 loss: 1.0254445\n",
      "6420: accuracy:1.0 loss: 1.0308864\n",
      "6430: accuracy:1.0 loss: 0.22487056\n",
      "6440: accuracy:0.99 loss: 3.538869\n",
      "6450: accuracy:0.99 loss: 1.3657063\n",
      "6450: ********* epoch 11 ********* test accuracy:0.9784 test loss: 8.265642\n",
      "6460: accuracy:1.0 loss: 0.40803277\n",
      "6470: accuracy:1.0 loss: 0.39350176\n",
      "6480: accuracy:1.0 loss: 0.35751706\n",
      "6490: accuracy:1.0 loss: 0.32885608\n",
      "6500: accuracy:0.99 loss: 1.8485428\n",
      "6500: ********* epoch 11 ********* test accuracy:0.9801 test loss: 7.411061\n",
      "6510: accuracy:1.0 loss: 0.40024412\n",
      "6520: accuracy:0.99 loss: 1.822586\n",
      "6530: accuracy:1.0 loss: 0.6069007\n",
      "6540: accuracy:1.0 loss: 0.13815723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6550: accuracy:0.99 loss: 1.7561748\n",
      "6550: ********* epoch 11 ********* test accuracy:0.9792 test loss: 7.638776\n",
      "6560: accuracy:0.98 loss: 1.8855253\n",
      "6570: accuracy:1.0 loss: 1.3249247\n",
      "6580: accuracy:1.0 loss: 1.5312158\n",
      "6590: accuracy:0.99 loss: 2.6069381\n",
      "6600: accuracy:1.0 loss: 0.5462884\n",
      "6600: ********* epoch 12 ********* test accuracy:0.9798 test loss: 7.555912\n",
      "6610: accuracy:1.0 loss: 0.69993675\n",
      "6620: accuracy:1.0 loss: 0.2992351\n",
      "6630: accuracy:1.0 loss: 0.41605347\n",
      "6640: accuracy:1.0 loss: 0.26635066\n",
      "6650: accuracy:1.0 loss: 0.35542345\n",
      "6650: ********* epoch 12 ********* test accuracy:0.9803 test loss: 7.2910233\n",
      "6660: accuracy:1.0 loss: 0.37662813\n",
      "6670: accuracy:1.0 loss: 0.29124984\n",
      "6680: accuracy:1.0 loss: 0.21637048\n",
      "6690: accuracy:1.0 loss: 0.32222414\n",
      "6700: accuracy:1.0 loss: 0.4507828\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9814 test loss: 6.9958878\n",
      "6710: accuracy:1.0 loss: 0.24164088\n",
      "6720: accuracy:1.0 loss: 0.1048592\n",
      "6730: accuracy:1.0 loss: 0.13680565\n",
      "6740: accuracy:0.99 loss: 1.4528211\n",
      "6750: accuracy:0.99 loss: 1.7587674\n",
      "6750: ********* epoch 12 ********* test accuracy:0.9808 test loss: 7.275326\n",
      "6760: accuracy:1.0 loss: 0.54942596\n",
      "6770: accuracy:1.0 loss: 0.18200052\n",
      "6780: accuracy:1.0 loss: 0.33996242\n",
      "6790: accuracy:1.0 loss: 0.12659127\n",
      "6800: accuracy:1.0 loss: 0.37912798\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9806 test loss: 7.2861958\n",
      "6810: accuracy:1.0 loss: 0.6725283\n",
      "6820: accuracy:1.0 loss: 0.3656512\n",
      "6830: accuracy:1.0 loss: 0.52099216\n",
      "6840: accuracy:1.0 loss: 0.546324\n",
      "6850: accuracy:1.0 loss: 0.14020279\n",
      "6850: ********* epoch 12 ********* test accuracy:0.9811 test loss: 7.1501474\n",
      "6860: accuracy:1.0 loss: 0.2658221\n",
      "6870: accuracy:1.0 loss: 0.1263954\n",
      "6880: accuracy:1.0 loss: 0.20302609\n",
      "6890: accuracy:1.0 loss: 0.20980749\n",
      "6900: accuracy:1.0 loss: 0.8192638\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9811 test loss: 7.077239\n",
      "6910: accuracy:1.0 loss: 0.3749303\n",
      "6920: accuracy:1.0 loss: 0.08414685\n",
      "6930: accuracy:1.0 loss: 0.088774264\n",
      "6940: accuracy:1.0 loss: 0.22952282\n",
      "6950: accuracy:1.0 loss: 0.28222334\n",
      "6950: ********* epoch 12 ********* test accuracy:0.9809 test loss: 7.082052\n",
      "6960: accuracy:1.0 loss: 0.28832328\n",
      "6970: accuracy:1.0 loss: 1.194055\n",
      "6980: accuracy:1.0 loss: 0.1299097\n",
      "6990: accuracy:0.98 loss: 2.229907\n",
      "7000: accuracy:1.0 loss: 0.18706758\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9806 test loss: 7.3974795\n",
      "7010: accuracy:1.0 loss: 0.3673834\n",
      "7020: accuracy:1.0 loss: 1.0587004\n",
      "7030: accuracy:1.0 loss: 0.26651037\n",
      "7040: accuracy:1.0 loss: 0.49528888\n",
      "7050: accuracy:1.0 loss: 0.3007516\n",
      "7050: ********* epoch 12 ********* test accuracy:0.9799 test loss: 7.669093\n",
      "7060: accuracy:1.0 loss: 0.85343677\n",
      "7070: accuracy:0.98 loss: 6.527053\n",
      "7080: accuracy:0.99 loss: 2.6077845\n",
      "7090: accuracy:1.0 loss: 0.23158489\n",
      "7100: accuracy:1.0 loss: 0.091988385\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9802 test loss: 7.442402\n",
      "7110: accuracy:1.0 loss: 0.95343626\n",
      "7120: accuracy:0.99 loss: 2.3174856\n",
      "7130: accuracy:1.0 loss: 0.05255107\n",
      "7140: accuracy:1.0 loss: 0.5689197\n",
      "7150: accuracy:1.0 loss: 0.24388728\n",
      "7150: ********* epoch 12 ********* test accuracy:0.9812 test loss: 7.072876\n",
      "7160: accuracy:1.0 loss: 0.85430634\n",
      "7170: accuracy:0.99 loss: 1.7968609\n",
      "7180: accuracy:1.0 loss: 0.48583403\n",
      "7190: accuracy:1.0 loss: 0.2760486\n",
      "7200: accuracy:1.0 loss: 0.30906194\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9817 test loss: 7.321911\n",
      "7210: accuracy:1.0 loss: 0.37143573\n",
      "7220: accuracy:1.0 loss: 0.39436162\n",
      "7230: accuracy:1.0 loss: 0.38108438\n",
      "7240: accuracy:1.0 loss: 0.24004409\n",
      "7250: accuracy:1.0 loss: 0.5279293\n",
      "7250: ********* epoch 13 ********* test accuracy:0.9806 test loss: 7.0688796\n",
      "7260: accuracy:1.0 loss: 0.78311473\n",
      "7270: accuracy:1.0 loss: 0.2297816\n",
      "7280: accuracy:0.99 loss: 2.2728891\n",
      "7290: accuracy:1.0 loss: 0.5219499\n",
      "7300: accuracy:1.0 loss: 0.57065284\n",
      "7300: ********* epoch 13 ********* test accuracy:0.9819 test loss: 7.0005555\n",
      "7310: accuracy:1.0 loss: 0.37419277\n",
      "7320: accuracy:1.0 loss: 0.23029378\n",
      "7330: accuracy:1.0 loss: 0.23975793\n",
      "7340: accuracy:1.0 loss: 0.29771018\n",
      "7350: accuracy:1.0 loss: 0.09657825\n",
      "7350: ********* epoch 13 ********* test accuracy:0.9822 test loss: 7.1311803\n",
      "7360: accuracy:1.0 loss: 0.07477833\n",
      "7370: accuracy:1.0 loss: 0.3779904\n",
      "7380: accuracy:1.0 loss: 0.0802316\n",
      "7390: accuracy:0.99 loss: 1.4977264\n",
      "7400: accuracy:1.0 loss: 0.37023705\n",
      "7400: ********* epoch 13 ********* test accuracy:0.9817 test loss: 7.064463\n",
      "7410: accuracy:1.0 loss: 0.2219094\n",
      "7420: accuracy:1.0 loss: 0.12128543\n",
      "7430: accuracy:1.0 loss: 0.09656611\n",
      "7440: accuracy:1.0 loss: 0.1360588\n",
      "7450: accuracy:1.0 loss: 0.29973775\n",
      "7450: ********* epoch 13 ********* test accuracy:0.9813 test loss: 7.483721\n",
      "7460: accuracy:1.0 loss: 0.14171895\n",
      "7470: accuracy:1.0 loss: 0.6172361\n",
      "7480: accuracy:1.0 loss: 0.22662252\n",
      "7490: accuracy:1.0 loss: 0.37526926\n",
      "7500: accuracy:1.0 loss: 0.6688832\n",
      "7500: ********* epoch 13 ********* test accuracy:0.9805 test loss: 7.4872837\n",
      "7510: accuracy:1.0 loss: 0.25764817\n",
      "7520: accuracy:1.0 loss: 0.31904978\n",
      "7530: accuracy:1.0 loss: 0.33191735\n",
      "7540: accuracy:1.0 loss: 0.16173244\n",
      "7550: accuracy:1.0 loss: 0.09631124\n",
      "7550: ********* epoch 13 ********* test accuracy:0.9798 test loss: 7.4048495\n",
      "7560: accuracy:1.0 loss: 0.09924483\n",
      "7570: accuracy:1.0 loss: 0.08718271\n",
      "7580: accuracy:1.0 loss: 0.581102\n",
      "7590: accuracy:1.0 loss: 0.45088467\n",
      "7600: accuracy:1.0 loss: 0.36936367\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9814 test loss: 7.3422337\n",
      "7610: accuracy:1.0 loss: 0.2777316\n",
      "7620: accuracy:1.0 loss: 1.0477853\n",
      "7630: accuracy:1.0 loss: 0.23834991\n",
      "7640: accuracy:1.0 loss: 0.22971302\n",
      "7650: accuracy:1.0 loss: 0.32046375\n",
      "7650: ********* epoch 13 ********* test accuracy:0.9802 test loss: 7.4081044\n",
      "7660: accuracy:1.0 loss: 0.20594695\n",
      "7670: accuracy:1.0 loss: 0.06638421\n",
      "7680: accuracy:1.0 loss: 0.62875664\n",
      "7690: accuracy:1.0 loss: 0.84178543\n",
      "7700: accuracy:1.0 loss: 0.11728075\n",
      "7700: ********* epoch 13 ********* test accuracy:0.9813 test loss: 7.2166314\n",
      "7710: accuracy:1.0 loss: 0.19939639\n",
      "7720: accuracy:1.0 loss: 0.18087219\n",
      "7730: accuracy:1.0 loss: 0.47852874\n",
      "7740: accuracy:1.0 loss: 0.09541726\n",
      "7750: accuracy:1.0 loss: 0.5368993\n",
      "7750: ********* epoch 13 ********* test accuracy:0.981 test loss: 7.2880287\n",
      "7760: accuracy:1.0 loss: 0.0480661\n",
      "7770: accuracy:1.0 loss: 0.19818859\n",
      "7780: accuracy:1.0 loss: 0.0822397\n",
      "7790: accuracy:1.0 loss: 0.4643399\n",
      "7800: accuracy:1.0 loss: 0.25654605\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9814 test loss: 7.3470764\n",
      "7810: accuracy:1.0 loss: 0.61297935\n",
      "7820: accuracy:1.0 loss: 0.35072178\n",
      "7830: accuracy:1.0 loss: 0.13874939\n",
      "7840: accuracy:1.0 loss: 0.2590647\n",
      "7850: accuracy:1.0 loss: 0.09445648\n",
      "7850: ********* epoch 14 ********* test accuracy:0.9817 test loss: 7.4050155\n",
      "7860: accuracy:1.0 loss: 0.20201832\n",
      "7870: accuracy:1.0 loss: 0.4215123\n",
      "7880: accuracy:1.0 loss: 0.117211595\n",
      "7890: accuracy:1.0 loss: 0.1490405\n",
      "7900: accuracy:1.0 loss: 0.046832953\n",
      "7900: ********* epoch 14 ********* test accuracy:0.982 test loss: 7.2983327\n",
      "7910: accuracy:1.0 loss: 0.13804118\n",
      "7920: accuracy:1.0 loss: 0.13992687\n",
      "7930: accuracy:1.0 loss: 0.1160554\n",
      "7940: accuracy:1.0 loss: 0.16038997\n",
      "7950: accuracy:1.0 loss: 0.095984094\n",
      "7950: ********* epoch 14 ********* test accuracy:0.9814 test loss: 7.1626005\n",
      "7960: accuracy:1.0 loss: 0.45520586\n",
      "7970: accuracy:1.0 loss: 0.11999068\n",
      "7980: accuracy:1.0 loss: 0.1120491\n",
      "7990: accuracy:1.0 loss: 0.5104556\n",
      "8000: accuracy:1.0 loss: 0.48090065\n",
      "8000: ********* epoch 14 ********* test accuracy:0.9812 test loss: 7.203528\n",
      "8010: accuracy:1.0 loss: 0.30176234\n",
      "8020: accuracy:1.0 loss: 0.048971985\n",
      "8030: accuracy:1.0 loss: 0.37720755\n",
      "8040: accuracy:1.0 loss: 0.31444076\n",
      "8050: accuracy:1.0 loss: 0.08470458\n",
      "8050: ********* epoch 14 ********* test accuracy:0.9818 test loss: 7.1987114\n",
      "8060: accuracy:1.0 loss: 0.29041818\n",
      "8070: accuracy:1.0 loss: 0.0165083\n",
      "8080: accuracy:1.0 loss: 0.053238105\n",
      "8090: accuracy:1.0 loss: 0.17412989\n",
      "8100: accuracy:1.0 loss: 0.2681591\n",
      "8100: ********* epoch 14 ********* test accuracy:0.9796 test loss: 7.47279\n",
      "8110: accuracy:1.0 loss: 0.26210672\n",
      "8120: accuracy:1.0 loss: 0.23280075\n",
      "8130: accuracy:1.0 loss: 0.14817834\n",
      "8140: accuracy:1.0 loss: 0.17786022\n",
      "8150: accuracy:1.0 loss: 0.07576939\n",
      "8150: ********* epoch 14 ********* test accuracy:0.9817 test loss: 7.448181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8160: accuracy:1.0 loss: 0.2335329\n",
      "8170: accuracy:1.0 loss: 0.2242175\n",
      "8180: accuracy:1.0 loss: 0.051623218\n",
      "8190: accuracy:1.0 loss: 0.24669501\n",
      "8200: accuracy:1.0 loss: 0.5024909\n",
      "8200: ********* epoch 14 ********* test accuracy:0.9812 test loss: 7.4993243\n",
      "8210: accuracy:1.0 loss: 0.55200475\n",
      "8220: accuracy:1.0 loss: 0.12083358\n",
      "8230: accuracy:1.0 loss: 0.32416594\n",
      "8240: accuracy:1.0 loss: 0.20079184\n",
      "8250: accuracy:1.0 loss: 0.6611445\n",
      "8250: ********* epoch 14 ********* test accuracy:0.9816 test loss: 7.4236364\n",
      "8260: accuracy:1.0 loss: 0.39955887\n",
      "8270: accuracy:1.0 loss: 0.12841079\n",
      "8280: accuracy:1.0 loss: 0.06594524\n",
      "8290: accuracy:1.0 loss: 0.2716502\n",
      "8300: accuracy:1.0 loss: 0.2853057\n",
      "8300: ********* epoch 14 ********* test accuracy:0.982 test loss: 7.522883\n",
      "8310: accuracy:1.0 loss: 0.14697069\n",
      "8320: accuracy:1.0 loss: 0.3943184\n",
      "8330: accuracy:1.0 loss: 0.24952792\n",
      "8340: accuracy:1.0 loss: 0.30589736\n",
      "8350: accuracy:1.0 loss: 0.13551131\n",
      "8350: ********* epoch 14 ********* test accuracy:0.9824 test loss: 7.2942705\n",
      "8360: accuracy:1.0 loss: 0.13494334\n",
      "8370: accuracy:1.0 loss: 0.16352996\n",
      "8380: accuracy:1.0 loss: 0.29877153\n",
      "8390: accuracy:1.0 loss: 0.094049096\n",
      "8400: accuracy:1.0 loss: 0.40386993\n",
      "8400: ********* epoch 15 ********* test accuracy:0.9824 test loss: 7.116256\n",
      "8410: accuracy:1.0 loss: 0.18923822\n",
      "8420: accuracy:1.0 loss: 0.11106574\n",
      "8430: accuracy:1.0 loss: 0.19843625\n",
      "8440: accuracy:1.0 loss: 0.059061892\n",
      "8450: accuracy:1.0 loss: 0.18007454\n",
      "8450: ********* epoch 15 ********* test accuracy:0.9825 test loss: 7.138789\n",
      "8460: accuracy:1.0 loss: 0.15156499\n",
      "8470: accuracy:1.0 loss: 0.10430351\n",
      "8480: accuracy:1.0 loss: 0.06369257\n",
      "8490: accuracy:1.0 loss: 0.14432955\n",
      "8500: accuracy:1.0 loss: 0.08969189\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9825 test loss: 7.10714\n",
      "8510: accuracy:1.0 loss: 0.16260451\n",
      "8520: accuracy:1.0 loss: 0.037097547\n",
      "8530: accuracy:1.0 loss: 0.27662653\n",
      "8540: accuracy:1.0 loss: 0.15316893\n",
      "8550: accuracy:1.0 loss: 0.14536783\n",
      "8550: ********* epoch 15 ********* test accuracy:0.9826 test loss: 7.2210593\n",
      "8560: accuracy:1.0 loss: 0.8090353\n",
      "8570: accuracy:1.0 loss: 0.117523305\n",
      "8580: accuracy:0.99 loss: 4.5069246\n",
      "8590: accuracy:1.0 loss: 0.19894716\n",
      "8600: accuracy:1.0 loss: 0.21178618\n",
      "8600: ********* epoch 15 ********* test accuracy:0.9821 test loss: 7.1774793\n",
      "8610: accuracy:1.0 loss: 0.14327186\n",
      "8620: accuracy:1.0 loss: 0.13317224\n",
      "8630: accuracy:1.0 loss: 0.16087836\n",
      "8640: accuracy:1.0 loss: 0.082741596\n",
      "8650: accuracy:1.0 loss: 0.2273339\n",
      "8650: ********* epoch 15 ********* test accuracy:0.9813 test loss: 7.2106953\n",
      "8660: accuracy:1.0 loss: 0.14156462\n",
      "8670: accuracy:1.0 loss: 0.061669197\n",
      "8680: accuracy:1.0 loss: 0.052217174\n",
      "8690: accuracy:1.0 loss: 0.22209626\n",
      "8700: accuracy:1.0 loss: 0.054835137\n",
      "8700: ********* epoch 15 ********* test accuracy:0.9818 test loss: 7.2228603\n",
      "8710: accuracy:1.0 loss: 0.1567621\n",
      "8720: accuracy:1.0 loss: 0.3332972\n",
      "8730: accuracy:1.0 loss: 0.031586792\n",
      "8740: accuracy:1.0 loss: 0.20292056\n",
      "8750: accuracy:1.0 loss: 0.1606012\n",
      "8750: ********* epoch 15 ********* test accuracy:0.9822 test loss: 7.337087\n",
      "8760: accuracy:1.0 loss: 0.071226686\n",
      "8770: accuracy:1.0 loss: 0.05354604\n",
      "8780: accuracy:1.0 loss: 0.18241107\n",
      "8790: accuracy:1.0 loss: 0.14456527\n",
      "8800: accuracy:1.0 loss: 0.18101516\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9817 test loss: 7.473692\n",
      "8810: accuracy:1.0 loss: 0.19359353\n",
      "8820: accuracy:1.0 loss: 0.059198324\n",
      "8830: accuracy:1.0 loss: 0.03759656\n",
      "8840: accuracy:1.0 loss: 0.039720837\n",
      "8850: accuracy:1.0 loss: 0.12826222\n",
      "8850: ********* epoch 15 ********* test accuracy:0.9813 test loss: 7.4684863\n",
      "8860: accuracy:1.0 loss: 0.18708503\n",
      "8870: accuracy:1.0 loss: 0.11112221\n",
      "8880: accuracy:1.0 loss: 0.23026836\n",
      "8890: accuracy:1.0 loss: 0.19103244\n",
      "8900: accuracy:1.0 loss: 0.18730865\n",
      "8900: ********* epoch 15 ********* test accuracy:0.9818 test loss: 7.4445214\n",
      "8910: accuracy:1.0 loss: 0.36527905\n",
      "8920: accuracy:1.0 loss: 0.11263871\n",
      "8930: accuracy:1.0 loss: 0.10295004\n",
      "8940: accuracy:1.0 loss: 0.046727195\n",
      "8950: accuracy:1.0 loss: 0.21422109\n",
      "8950: ********* epoch 15 ********* test accuracy:0.9824 test loss: 7.3976645\n",
      "8960: accuracy:1.0 loss: 0.028491387\n",
      "8970: accuracy:1.0 loss: 0.07191436\n",
      "8980: accuracy:1.0 loss: 0.25376654\n",
      "8990: accuracy:1.0 loss: 0.2512752\n",
      "9000: accuracy:1.0 loss: 0.27720818\n",
      "9000: ********* epoch 16 ********* test accuracy:0.982 test loss: 7.5973134\n",
      "9010: accuracy:1.0 loss: 0.061045308\n",
      "9020: accuracy:1.0 loss: 0.117462486\n",
      "9030: accuracy:1.0 loss: 0.21539302\n",
      "9040: accuracy:1.0 loss: 0.18911314\n",
      "9050: accuracy:1.0 loss: 0.10390699\n",
      "9050: ********* epoch 16 ********* test accuracy:0.9825 test loss: 7.405214\n",
      "9060: accuracy:1.0 loss: 0.22127664\n",
      "9070: accuracy:1.0 loss: 0.0404602\n",
      "9080: accuracy:1.0 loss: 0.19669372\n",
      "9090: accuracy:1.0 loss: 0.16016127\n",
      "9100: accuracy:1.0 loss: 0.017900962\n",
      "9100: ********* epoch 16 ********* test accuracy:0.9817 test loss: 7.4736266\n",
      "9110: accuracy:1.0 loss: 0.12134307\n",
      "9120: accuracy:1.0 loss: 0.10606213\n",
      "9130: accuracy:1.0 loss: 0.13974386\n",
      "9140: accuracy:1.0 loss: 0.18084809\n",
      "9150: accuracy:1.0 loss: 0.06949064\n",
      "9150: ********* epoch 16 ********* test accuracy:0.9818 test loss: 7.594243\n",
      "9160: accuracy:1.0 loss: 0.063817546\n",
      "9170: accuracy:1.0 loss: 0.17135304\n",
      "9180: accuracy:1.0 loss: 0.026828961\n",
      "9190: accuracy:1.0 loss: 0.25255543\n",
      "9200: accuracy:1.0 loss: 0.08667838\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9806 test loss: 7.714524\n",
      "9210: accuracy:1.0 loss: 0.101852104\n",
      "9220: accuracy:1.0 loss: 0.03843143\n",
      "9230: accuracy:1.0 loss: 0.035932146\n",
      "9240: accuracy:1.0 loss: 0.15625608\n",
      "9250: accuracy:1.0 loss: 0.15364298\n",
      "9250: ********* epoch 16 ********* test accuracy:0.9813 test loss: 7.589146\n",
      "9260: accuracy:1.0 loss: 0.086851336\n",
      "9270: accuracy:1.0 loss: 0.31610572\n",
      "9280: accuracy:1.0 loss: 0.17345639\n",
      "9290: accuracy:1.0 loss: 0.2041134\n",
      "9300: accuracy:1.0 loss: 0.15577437\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9815 test loss: 7.608986\n",
      "9310: accuracy:1.0 loss: 0.11099593\n",
      "9320: accuracy:1.0 loss: 0.15365107\n",
      "9330: accuracy:1.0 loss: 0.09497078\n",
      "9340: accuracy:1.0 loss: 0.16127738\n",
      "9350: accuracy:1.0 loss: 0.25822198\n",
      "9350: ********* epoch 16 ********* test accuracy:0.9822 test loss: 7.489343\n",
      "9360: accuracy:1.0 loss: 0.09536445\n",
      "9370: accuracy:1.0 loss: 0.10794958\n",
      "9380: accuracy:1.0 loss: 0.099349506\n",
      "9390: accuracy:1.0 loss: 0.27834174\n",
      "9400: accuracy:1.0 loss: 0.08533618\n",
      "9400: ********* epoch 16 ********* test accuracy:0.9816 test loss: 7.642921\n",
      "9410: accuracy:1.0 loss: 0.17314345\n",
      "9420: accuracy:1.0 loss: 0.08446513\n",
      "9430: accuracy:1.0 loss: 0.08065003\n",
      "9440: accuracy:1.0 loss: 0.0967101\n",
      "9450: accuracy:1.0 loss: 0.012856418\n",
      "9450: ********* epoch 16 ********* test accuracy:0.9819 test loss: 7.416072\n",
      "9460: accuracy:1.0 loss: 0.3307723\n",
      "9470: accuracy:1.0 loss: 0.0653292\n",
      "9480: accuracy:1.0 loss: 0.078799754\n",
      "9490: accuracy:1.0 loss: 0.34887218\n",
      "9500: accuracy:1.0 loss: 0.06434582\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9814 test loss: 7.460322\n",
      "9510: accuracy:1.0 loss: 0.2672931\n",
      "9520: accuracy:1.0 loss: 0.21606222\n",
      "9530: accuracy:1.0 loss: 0.13510701\n",
      "9540: accuracy:1.0 loss: 0.113407776\n",
      "9550: accuracy:1.0 loss: 0.10511729\n",
      "9550: ********* epoch 16 ********* test accuracy:0.9827 test loss: 7.4363136\n",
      "9560: accuracy:1.0 loss: 0.09310016\n",
      "9570: accuracy:1.0 loss: 0.09806864\n",
      "9580: accuracy:1.0 loss: 0.115500174\n",
      "9590: accuracy:1.0 loss: 0.27647012\n",
      "9600: accuracy:1.0 loss: 0.13644113\n",
      "9600: ********* epoch 17 ********* test accuracy:0.9818 test loss: 7.555734\n",
      "9610: accuracy:1.0 loss: 0.056693584\n",
      "9620: accuracy:1.0 loss: 0.38923612\n",
      "9630: accuracy:1.0 loss: 0.059466366\n",
      "9640: accuracy:1.0 loss: 0.047738187\n",
      "9650: accuracy:1.0 loss: 0.12216387\n",
      "9650: ********* epoch 17 ********* test accuracy:0.9818 test loss: 7.546166\n",
      "9660: accuracy:1.0 loss: 0.03672258\n",
      "9670: accuracy:1.0 loss: 0.07280193\n",
      "9680: accuracy:1.0 loss: 0.056526184\n",
      "9690: accuracy:1.0 loss: 0.21011814\n",
      "9700: accuracy:1.0 loss: 0.054745972\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9823 test loss: 7.620088\n",
      "9710: accuracy:1.0 loss: 0.10082724\n",
      "9720: accuracy:1.0 loss: 0.05942047\n",
      "9730: accuracy:1.0 loss: 0.031615555\n",
      "9740: accuracy:1.0 loss: 0.13044265\n",
      "9750: accuracy:1.0 loss: 0.1327643\n",
      "9750: ********* epoch 17 ********* test accuracy:0.9826 test loss: 7.470028\n",
      "9760: accuracy:1.0 loss: 0.024208479\n",
      "9770: accuracy:1.0 loss: 0.13644266\n",
      "9780: accuracy:1.0 loss: 0.15518516\n",
      "9790: accuracy:1.0 loss: 0.093516424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9800: accuracy:1.0 loss: 0.15702717\n",
      "9800: ********* epoch 17 ********* test accuracy:0.9823 test loss: 7.565481\n",
      "9810: accuracy:1.0 loss: 0.026495736\n",
      "9820: accuracy:1.0 loss: 0.14122407\n",
      "9830: accuracy:1.0 loss: 0.07511341\n",
      "9840: accuracy:1.0 loss: 0.041508295\n",
      "9850: accuracy:1.0 loss: 0.098792955\n",
      "9850: ********* epoch 17 ********* test accuracy:0.9819 test loss: 7.5903916\n",
      "9860: accuracy:1.0 loss: 0.06441355\n",
      "9870: accuracy:1.0 loss: 0.07973298\n",
      "9880: accuracy:1.0 loss: 0.061480887\n",
      "9890: accuracy:1.0 loss: 0.11048361\n",
      "9900: accuracy:1.0 loss: 0.17460144\n",
      "9900: ********* epoch 17 ********* test accuracy:0.9827 test loss: 7.4944186\n",
      "9910: accuracy:1.0 loss: 0.03484533\n",
      "9920: accuracy:1.0 loss: 0.16848788\n",
      "9930: accuracy:1.0 loss: 0.029789502\n",
      "9940: accuracy:1.0 loss: 0.059818517\n",
      "9950: accuracy:1.0 loss: 0.38913262\n",
      "9950: ********* epoch 17 ********* test accuracy:0.9818 test loss: 7.54243\n",
      "9960: accuracy:1.0 loss: 0.1388847\n",
      "9970: accuracy:1.0 loss: 0.087174885\n",
      "9980: accuracy:1.0 loss: 0.07320559\n",
      "9990: accuracy:1.0 loss: 0.13621047\n",
      "10000: accuracy:1.0 loss: 0.12275798\n",
      "10000: ********* epoch 17 ********* test accuracy:0.9813 test loss: 7.7237697\n",
      "max test accuracy: 0.9827\n"
     ]
    }
   ],
   "source": [
    "# input X => 28x28 grayscale dimensions ('None' indexes images in mini-batch)\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "\n",
    "# correct answers\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights\n",
    "W1 = tf.Variable(tf.truncated_normal([28*28, 200], stddev=0.1))\n",
    "W2 = tf.Variable(tf.truncated_normal([200, 100], stddev=0.1))\n",
    "W3 = tf.Variable(tf.truncated_normal([100, 10], stddev=0.1))\n",
    "\n",
    "# biases\n",
    "b1 = tf.Variable(tf.zeros([200]))\n",
    "b2 = tf.Variable(tf.zeros([100]))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# flatten images into single line\n",
    "XX = tf.reshape(X, [-1, 28*28])\n",
    "\n",
    "# model\n",
    "Y1 = tf.nn.relu(tf.matmul(XX, W1) + b1)\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)\n",
    "Y = tf.nn.softmax(tf.matmul(Y2, W3) + b3)\n",
    "\n",
    "# cross entropy\n",
    "cross_entropy = -tf.reduce_mean(Y_ * tf.log(Y)) * 1000.0\n",
    "\n",
    "# accuracy of the trained model [0, 1] ([worst, best])\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# training (learning rate is 0.003)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.003).minimize(cross_entropy)\n",
    "\n",
    "# initialization\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# call in loop to train the model 100 images at a time\n",
    "max_test_accuracy = 0.0\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    # train on batches of 100 images w/ 100 labels\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    # compute training values for visualisation\n",
    "    if update_train_data:\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: batch_X, Y_: batch_Y})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c))\n",
    "    # compute test values for visualization\n",
    "    if update_test_data:\n",
    "        global max_test_accuracy\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: mnist.test.images, Y_: mnist.test.labels})\n",
    "        if (a > max_test_accuracy):\n",
    "            max_test_accuracy = a\n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \n",
    "              \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        \n",
    "    # back-propogation training step\n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y})\n",
    "\n",
    "# text visualization of process\n",
    "for i in range(10000+1):\n",
    "    training_step(i, i % 50 == 0, i % 10 == 0)\n",
    "    \n",
    "# display the final max accuracy\n",
    "print(\"max test accuracy: \" + str(max_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Convergence with a Better Optimizer\n",
    "The following cell includes a better optimizer than the previously used GradientDescentOptimizer: AdamOptimizer. This prevents saddle points (gradient is 0 despite not being local minima, resulting in the optimizer getting stuck), which are relatively frequent with datasets/dimensional spaces of this size (10k weights and biases), from occuring. This is possible because the optimizer has inertia to push through the saddle points. With 10k iterations, it maxes out with an accuracy of ~98% (includes NaN!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.04 loss: 239.69489\n",
      "0: ********* epoch 1 ********* test accuracy:0.0752 test loss: 237.57788\n",
      "10: accuracy:0.84 loss: 69.24802\n",
      "20: accuracy:0.89 loss: 50.932175\n",
      "30: accuracy:0.89 loss: 30.148518\n",
      "40: accuracy:0.89 loss: 32.66878\n",
      "50: accuracy:0.9 loss: 35.950336\n",
      "50: ********* epoch 1 ********* test accuracy:0.8994 test loss: 34.712727\n",
      "60: accuracy:0.89 loss: 43.162537\n",
      "70: accuracy:0.87 loss: 42.589195\n",
      "80: accuracy:0.92 loss: 26.72678\n",
      "90: accuracy:0.85 loss: 37.728394\n",
      "100: accuracy:0.95 loss: 21.076817\n",
      "100: ********* epoch 1 ********* test accuracy:0.9192 test loss: 25.741932\n",
      "110: accuracy:0.96 loss: 16.973732\n",
      "120: accuracy:0.99 loss: 9.826069\n",
      "130: accuracy:0.93 loss: 20.660856\n",
      "140: accuracy:0.91 loss: 25.065395\n",
      "150: accuracy:0.92 loss: 21.79847\n",
      "150: ********* epoch 1 ********* test accuracy:0.9402 test loss: 20.764004\n",
      "160: accuracy:0.88 loss: 31.473742\n",
      "170: accuracy:0.96 loss: 26.040588\n",
      "180: accuracy:0.92 loss: 21.528635\n",
      "190: accuracy:0.97 loss: 13.784019\n",
      "200: accuracy:0.96 loss: 15.084931\n",
      "200: ********* epoch 1 ********* test accuracy:0.9476 test loss: 17.693254\n",
      "210: accuracy:0.93 loss: 18.718494\n",
      "220: accuracy:0.92 loss: 30.975025\n",
      "230: accuracy:0.94 loss: 17.196613\n",
      "240: accuracy:0.93 loss: 27.723923\n",
      "250: accuracy:0.98 loss: 6.2739854\n",
      "250: ********* epoch 1 ********* test accuracy:0.9501 test loss: 15.658918\n",
      "260: accuracy:0.93 loss: 17.230257\n",
      "270: accuracy:0.95 loss: 19.066181\n",
      "280: accuracy:0.93 loss: 25.698107\n",
      "290: accuracy:0.9 loss: 23.71539\n",
      "300: accuracy:0.97 loss: 10.026786\n",
      "300: ********* epoch 1 ********* test accuracy:0.9539 test loss: 15.093177\n",
      "310: accuracy:0.96 loss: 15.265844\n",
      "320: accuracy:0.97 loss: 20.851637\n",
      "330: accuracy:0.92 loss: 18.311226\n",
      "340: accuracy:0.94 loss: 18.344124\n",
      "350: accuracy:0.97 loss: 11.201939\n",
      "350: ********* epoch 1 ********* test accuracy:0.9539 test loss: 14.801575\n",
      "360: accuracy:0.96 loss: 21.415808\n",
      "370: accuracy:0.96 loss: 11.721307\n",
      "380: accuracy:0.98 loss: 9.345357\n",
      "390: accuracy:0.99 loss: 4.550173\n",
      "400: accuracy:0.99 loss: 6.726753\n",
      "400: ********* epoch 1 ********* test accuracy:0.96 test loss: 13.230561\n",
      "410: accuracy:0.99 loss: 5.608512\n",
      "420: accuracy:0.97 loss: 8.347267\n",
      "430: accuracy:0.94 loss: 22.97898\n",
      "440: accuracy:0.98 loss: 8.337305\n",
      "450: accuracy:0.96 loss: 9.21616\n",
      "450: ********* epoch 1 ********* test accuracy:0.96 test loss: 12.3243885\n",
      "460: accuracy:0.94 loss: 19.911816\n",
      "470: accuracy:0.97 loss: 8.214541\n",
      "480: accuracy:0.98 loss: 7.5643206\n",
      "490: accuracy:0.96 loss: 9.702664\n",
      "500: accuracy:0.96 loss: 10.50005\n",
      "500: ********* epoch 1 ********* test accuracy:0.9607 test loss: 12.374356\n",
      "510: accuracy:0.95 loss: 21.332205\n",
      "520: accuracy:0.95 loss: 14.600903\n",
      "530: accuracy:0.98 loss: 9.158827\n",
      "540: accuracy:0.98 loss: 3.7738392\n",
      "550: accuracy:0.98 loss: 5.2439156\n",
      "550: ********* epoch 1 ********* test accuracy:0.967 test loss: 10.591748\n",
      "560: accuracy:0.96 loss: 12.652348\n",
      "570: accuracy:0.96 loss: 11.686178\n",
      "580: accuracy:0.96 loss: 17.210135\n",
      "590: accuracy:0.97 loss: 9.460895\n",
      "600: accuracy:0.98 loss: 6.703659\n",
      "600: ********* epoch 2 ********* test accuracy:0.9635 test loss: 11.65517\n",
      "610: accuracy:0.92 loss: 15.80278\n",
      "620: accuracy:0.94 loss: 19.508713\n",
      "630: accuracy:0.96 loss: 8.989159\n",
      "640: accuracy:0.98 loss: 10.939812\n",
      "650: accuracy:0.95 loss: 10.613366\n",
      "650: ********* epoch 2 ********* test accuracy:0.9661 test loss: 10.992784\n",
      "660: accuracy:0.96 loss: 9.93811\n",
      "670: accuracy:0.98 loss: 9.775957\n",
      "680: accuracy:0.98 loss: 6.9837303\n",
      "690: accuracy:0.99 loss: 5.8950825\n",
      "700: accuracy:0.97 loss: 10.156267\n",
      "700: ********* epoch 2 ********* test accuracy:0.9671 test loss: 10.415295\n",
      "710: accuracy:0.96 loss: 13.535447\n",
      "720: accuracy:0.99 loss: 4.6411486\n",
      "730: accuracy:0.97 loss: 14.240564\n",
      "740: accuracy:0.98 loss: 7.349928\n",
      "750: accuracy:0.97 loss: 8.293202\n",
      "750: ********* epoch 2 ********* test accuracy:0.9725 test loss: 9.207415\n",
      "760: accuracy:0.98 loss: 9.712513\n",
      "770: accuracy:1.0 loss: 1.9298428\n",
      "780: accuracy:0.93 loss: 24.15194\n",
      "790: accuracy:0.97 loss: 10.645458\n",
      "800: accuracy:0.96 loss: 15.076241\n",
      "800: ********* epoch 2 ********* test accuracy:0.9698 test loss: 10.231013\n",
      "810: accuracy:0.98 loss: 10.850082\n",
      "820: accuracy:0.98 loss: 14.290746\n",
      "830: accuracy:0.99 loss: 3.3684287\n",
      "840: accuracy:0.97 loss: 7.821683\n",
      "850: accuracy:0.94 loss: 8.989398\n",
      "850: ********* epoch 2 ********* test accuracy:0.9614 test loss: 12.435234\n",
      "860: accuracy:0.97 loss: 11.713142\n",
      "870: accuracy:0.98 loss: 4.421369\n",
      "880: accuracy:0.97 loss: 8.547996\n",
      "890: accuracy:0.98 loss: 7.046126\n",
      "900: accuracy:0.97 loss: 10.281399\n",
      "900: ********* epoch 2 ********* test accuracy:0.9671 test loss: 11.193842\n",
      "910: accuracy:0.96 loss: 14.576387\n",
      "920: accuracy:0.96 loss: 10.065884\n",
      "930: accuracy:0.95 loss: 9.943433\n",
      "940: accuracy:0.98 loss: 3.3676648\n",
      "950: accuracy:0.95 loss: 12.185406\n",
      "950: ********* epoch 2 ********* test accuracy:0.9715 test loss: 8.8033905\n",
      "960: accuracy:0.98 loss: 7.8886824\n",
      "970: accuracy:1.0 loss: 3.785597\n",
      "980: accuracy:0.99 loss: 6.284624\n",
      "990: accuracy:1.0 loss: 1.0898838\n",
      "1000: accuracy:0.99 loss: 5.01069\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9678 test loss: 10.6088505\n",
      "1010: accuracy:0.97 loss: 8.625627\n",
      "1020: accuracy:1.0 loss: 1.88714\n",
      "1030: accuracy:0.97 loss: 11.641298\n",
      "1040: accuracy:0.97 loss: 7.204263\n",
      "1050: accuracy:0.99 loss: 2.532631\n",
      "1050: ********* epoch 2 ********* test accuracy:0.972 test loss: 8.899338\n",
      "1060: accuracy:0.97 loss: 6.8361244\n",
      "1070: accuracy:0.98 loss: 6.2310996\n",
      "1080: accuracy:0.98 loss: 11.11606\n",
      "1090: accuracy:0.97 loss: 10.454663\n",
      "1100: accuracy:0.98 loss: 6.6942654\n",
      "1100: ********* epoch 2 ********* test accuracy:0.9715 test loss: 9.453641\n",
      "1110: accuracy:0.97 loss: 11.326936\n",
      "1120: accuracy:0.95 loss: 15.614412\n",
      "1130: accuracy:0.98 loss: 4.7519484\n",
      "1140: accuracy:1.0 loss: 3.473197\n",
      "1150: accuracy:0.98 loss: 7.1298227\n",
      "1150: ********* epoch 2 ********* test accuracy:0.9743 test loss: 8.272756\n",
      "1160: accuracy:0.99 loss: 2.266106\n",
      "1170: accuracy:0.96 loss: 9.759277\n",
      "1180: accuracy:0.97 loss: 9.113221\n",
      "1190: accuracy:0.99 loss: 11.915633\n",
      "1200: accuracy:0.98 loss: 6.7664127\n",
      "1200: ********* epoch 3 ********* test accuracy:0.972 test loss: 8.66465\n",
      "1210: accuracy:0.95 loss: 14.059249\n",
      "1220: accuracy:0.97 loss: 11.270773\n",
      "1230: accuracy:1.0 loss: 4.099684\n",
      "1240: accuracy:0.96 loss: 16.106478\n",
      "1250: accuracy:0.97 loss: 11.218888\n",
      "1250: ********* epoch 3 ********* test accuracy:0.97 test loss: 9.372817\n",
      "1260: accuracy:1.0 loss: 1.2269956\n",
      "1270: accuracy:0.92 loss: 19.038734\n",
      "1280: accuracy:0.96 loss: 9.87572\n",
      "1290: accuracy:0.97 loss: 10.552343\n",
      "1300: accuracy:0.98 loss: 7.1370864\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9764 test loss: 8.342009\n",
      "1310: accuracy:0.99 loss: 2.97329\n",
      "1320: accuracy:0.97 loss: 19.69683\n",
      "1330: accuracy:1.0 loss: 1.7621621\n",
      "1340: accuracy:0.98 loss: 9.969143\n",
      "1350: accuracy:0.97 loss: 6.821483\n",
      "1350: ********* epoch 3 ********* test accuracy:0.9735 test loss: 8.454553\n",
      "1360: accuracy:0.98 loss: 4.0459824\n",
      "1370: accuracy:0.98 loss: 5.824579\n",
      "1380: accuracy:0.98 loss: 5.816188\n",
      "1390: accuracy:0.98 loss: 5.726907\n",
      "1400: accuracy:1.0 loss: 1.9210459\n",
      "1400: ********* epoch 3 ********* test accuracy:0.976 test loss: 7.473644\n",
      "1410: accuracy:1.0 loss: 1.6916754\n",
      "1420: accuracy:0.97 loss: 14.938027\n",
      "1430: accuracy:1.0 loss: 1.2435324\n",
      "1440: accuracy:0.97 loss: 5.7384534\n",
      "1450: accuracy:0.96 loss: 7.924971\n",
      "1450: ********* epoch 3 ********* test accuracy:0.9752 test loss: 7.2977786\n",
      "1460: accuracy:0.98 loss: 6.3535695\n",
      "1470: accuracy:0.98 loss: 4.1623\n",
      "1480: accuracy:1.0 loss: 1.484042\n",
      "1490: accuracy:0.99 loss: 3.7634997\n",
      "1500: accuracy:0.99 loss: 2.6644187\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9741 test loss: 8.056169\n",
      "1510: accuracy:0.97 loss: 9.905956\n",
      "1520: accuracy:0.97 loss: 5.8045254\n",
      "1530: accuracy:1.0 loss: 1.9115283\n",
      "1540: accuracy:0.98 loss: 6.4787045\n",
      "1550: accuracy:1.0 loss: 1.0422752\n",
      "1550: ********* epoch 3 ********* test accuracy:0.9765 test loss: 7.5211377\n",
      "1560: accuracy:0.99 loss: 5.2555604\n",
      "1570: accuracy:0.98 loss: 6.903524\n",
      "1580: accuracy:0.98 loss: 6.7320294\n",
      "1590: accuracy:0.98 loss: 4.173701\n",
      "1600: accuracy:0.99 loss: 4.7887983\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9775 test loss: 7.6976757\n",
      "1610: accuracy:0.97 loss: 11.759869\n",
      "1620: accuracy:0.98 loss: 14.722607\n",
      "1630: accuracy:0.98 loss: 5.7688503\n",
      "1640: accuracy:0.96 loss: 8.472183\n",
      "1650: accuracy:0.95 loss: 11.376298\n",
      "1650: ********* epoch 3 ********* test accuracy:0.9715 test loss: 9.612606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1660: accuracy:1.0 loss: 0.755193\n",
      "1670: accuracy:0.99 loss: 1.5041687\n",
      "1680: accuracy:0.96 loss: 12.219926\n",
      "1690: accuracy:0.99 loss: 3.351328\n",
      "1700: accuracy:1.0 loss: 1.6565142\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9769 test loss: 7.728529\n",
      "1710: accuracy:0.99 loss: 12.975424\n",
      "1720: accuracy:0.98 loss: 3.504114\n",
      "1730: accuracy:0.97 loss: 7.4002056\n",
      "1740: accuracy:1.0 loss: 2.6269445\n",
      "1750: accuracy:0.99 loss: 7.9499006\n",
      "1750: ********* epoch 3 ********* test accuracy:0.9754 test loss: 8.123494\n",
      "1760: accuracy:0.96 loss: 11.129644\n",
      "1770: accuracy:0.98 loss: 11.253323\n",
      "1780: accuracy:0.99 loss: 2.524527\n",
      "1790: accuracy:1.0 loss: 3.4136262\n",
      "1800: accuracy:0.99 loss: 2.616599\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9729 test loss: 9.15447\n",
      "1810: accuracy:0.98 loss: 4.812662\n",
      "1820: accuracy:0.98 loss: 4.781053\n",
      "1830: accuracy:0.99 loss: 1.9186162\n",
      "1840: accuracy:0.99 loss: 2.3450012\n",
      "1850: accuracy:0.97 loss: 5.899619\n",
      "1850: ********* epoch 4 ********* test accuracy:0.9731 test loss: 9.234786\n",
      "1860: accuracy:0.99 loss: 4.1281967\n",
      "1870: accuracy:1.0 loss: 1.5894337\n",
      "1880: accuracy:0.99 loss: 3.319964\n",
      "1890: accuracy:0.99 loss: 2.5769224\n",
      "1900: accuracy:0.99 loss: 8.430486\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9754 test loss: 8.3626585\n",
      "1910: accuracy:0.99 loss: 9.94684\n",
      "1920: accuracy:0.98 loss: 4.3564243\n",
      "1930: accuracy:0.99 loss: 6.5995483\n",
      "1940: accuracy:0.99 loss: 3.0154898\n",
      "1950: accuracy:0.99 loss: 4.007515\n",
      "1950: ********* epoch 4 ********* test accuracy:0.9716 test loss: 9.351323\n",
      "1960: accuracy:0.96 loss: 13.791307\n",
      "1970: accuracy:0.97 loss: 6.0058665\n",
      "1980: accuracy:0.99 loss: 2.5854154\n",
      "1990: accuracy:0.97 loss: 9.225843\n",
      "2000: accuracy:1.0 loss: 1.8159813\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9767 test loss: 7.986851\n",
      "2010: accuracy:1.0 loss: 1.5231868\n",
      "2020: accuracy:0.99 loss: 2.6158757\n",
      "2030: accuracy:0.99 loss: 2.1208591\n",
      "2040: accuracy:1.0 loss: 1.5476793\n",
      "2050: accuracy:1.0 loss: 0.96236503\n",
      "2050: ********* epoch 4 ********* test accuracy:0.9785 test loss: 7.430848\n",
      "2060: accuracy:0.99 loss: 6.2763457\n",
      "2070: accuracy:0.98 loss: 5.76282\n",
      "2080: accuracy:1.0 loss: 1.4028623\n",
      "2090: accuracy:0.99 loss: 2.1347678\n",
      "2100: accuracy:0.97 loss: 11.600019\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9786 test loss: 7.98814\n",
      "2110: accuracy:0.99 loss: 3.7966697\n",
      "2120: accuracy:0.99 loss: 3.3907356\n",
      "2130: accuracy:0.98 loss: 6.067378\n",
      "2140: accuracy:0.99 loss: 5.1189375\n",
      "2150: accuracy:0.95 loss: 7.7244687\n",
      "2150: ********* epoch 4 ********* test accuracy:0.9755 test loss: 8.173159\n",
      "2160: accuracy:0.99 loss: 4.5551834\n",
      "2170: accuracy:1.0 loss: 2.3793707\n",
      "2180: accuracy:0.99 loss: 8.275716\n",
      "2190: accuracy:1.0 loss: 0.93601125\n",
      "2200: accuracy:0.99 loss: 2.6485634\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9761 test loss: 9.125619\n",
      "2210: accuracy:0.96 loss: 7.5620804\n",
      "2220: accuracy:0.98 loss: 10.005399\n",
      "2230: accuracy:0.98 loss: 4.7914834\n",
      "2240: accuracy:0.98 loss: 4.2314005\n",
      "2250: accuracy:0.96 loss: 9.193982\n",
      "2250: ********* epoch 4 ********* test accuracy:0.9758 test loss: 7.9372926\n",
      "2260: accuracy:0.99 loss: 2.625319\n",
      "2270: accuracy:0.97 loss: 5.4969525\n",
      "2280: accuracy:0.98 loss: 5.053651\n",
      "2290: accuracy:0.99 loss: 8.613341\n",
      "2300: accuracy:0.98 loss: 4.9992933\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9777 test loss: 7.8708\n",
      "2310: accuracy:1.0 loss: 2.6190805\n",
      "2320: accuracy:1.0 loss: 1.7669356\n",
      "2330: accuracy:0.98 loss: 3.5542278\n",
      "2340: accuracy:1.0 loss: 0.9364452\n",
      "2350: accuracy:0.99 loss: 2.862632\n",
      "2350: ********* epoch 4 ********* test accuracy:0.9701 test loss: 10.058771\n",
      "2360: accuracy:0.98 loss: 6.5096397\n",
      "2370: accuracy:1.0 loss: 1.0643218\n",
      "2380: accuracy:0.97 loss: 6.8788576\n",
      "2390: accuracy:0.96 loss: 5.6081076\n",
      "2400: accuracy:0.96 loss: 7.2356663\n",
      "2400: ********* epoch 5 ********* test accuracy:0.9755 test loss: 8.378602\n",
      "2410: accuracy:0.98 loss: 4.8046403\n",
      "2420: accuracy:0.99 loss: 5.406864\n",
      "2430: accuracy:0.97 loss: 10.381221\n",
      "2440: accuracy:0.97 loss: 5.9331264\n",
      "2450: accuracy:1.0 loss: 1.0848752\n",
      "2450: ********* epoch 5 ********* test accuracy:0.971 test loss: 9.042661\n",
      "2460: accuracy:0.96 loss: 9.263543\n",
      "2470: accuracy:0.99 loss: 1.334914\n",
      "2480: accuracy:1.0 loss: 1.1135681\n",
      "2490: accuracy:0.96 loss: 17.690659\n",
      "2500: accuracy:1.0 loss: 2.3668287\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9755 test loss: 8.27743\n",
      "2510: accuracy:0.99 loss: 2.3481805\n",
      "2520: accuracy:0.99 loss: 2.1587183\n",
      "2530: accuracy:1.0 loss: 1.950558\n",
      "2540: accuracy:0.98 loss: 4.953977\n",
      "2550: accuracy:0.98 loss: 5.8362293\n",
      "2550: ********* epoch 5 ********* test accuracy:0.9784 test loss: 7.439437\n",
      "2560: accuracy:0.99 loss: 3.6108835\n",
      "2570: accuracy:0.98 loss: 4.627013\n",
      "2580: accuracy:0.98 loss: 8.660698\n",
      "2590: accuracy:0.98 loss: 7.6682525\n",
      "2600: accuracy:1.0 loss: 0.82575065\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9759 test loss: 7.9421067\n",
      "2610: accuracy:0.97 loss: 7.0981984\n",
      "2620: accuracy:0.99 loss: 4.832077\n",
      "2630: accuracy:1.0 loss: 1.2600027\n",
      "2640: accuracy:0.99 loss: 2.4520066\n",
      "2650: accuracy:1.0 loss: 1.7482107\n",
      "2650: ********* epoch 5 ********* test accuracy:0.9793 test loss: 7.2197657\n",
      "2660: accuracy:0.98 loss: 9.315635\n",
      "2670: accuracy:0.99 loss: 4.278867\n",
      "2680: accuracy:1.0 loss: 0.43053374\n",
      "2690: accuracy:0.99 loss: 2.0176191\n",
      "2700: accuracy:1.0 loss: 1.1815071\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9773 test loss: 7.761316\n",
      "2710: accuracy:0.98 loss: 2.7352388\n",
      "2720: accuracy:0.98 loss: 3.4149456\n",
      "2730: accuracy:1.0 loss: 0.49368402\n",
      "2740: accuracy:0.99 loss: 7.3604813\n",
      "2750: accuracy:1.0 loss: 0.803912\n",
      "2750: ********* epoch 5 ********* test accuracy:0.9817 test loss: 6.3624907\n",
      "2760: accuracy:1.0 loss: 1.5679169\n",
      "2770: accuracy:0.98 loss: 3.9470158\n",
      "2780: accuracy:1.0 loss: 0.69640684\n",
      "2790: accuracy:1.0 loss: 0.4732051\n",
      "2800: accuracy:1.0 loss: 0.6034211\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9762 test loss: 8.581401\n",
      "2810: accuracy:0.99 loss: 2.5776305\n",
      "2820: accuracy:0.99 loss: 3.3817272\n",
      "2830: accuracy:0.99 loss: 3.1384137\n",
      "2840: accuracy:1.0 loss: 0.74114084\n",
      "2850: accuracy:1.0 loss: 0.65040195\n",
      "2850: ********* epoch 5 ********* test accuracy:0.98 test loss: 7.5938783\n",
      "2860: accuracy:1.0 loss: 1.3621298\n",
      "2870: accuracy:1.0 loss: 0.7483164\n",
      "2880: accuracy:0.99 loss: 2.162706\n",
      "2890: accuracy:1.0 loss: 0.23626938\n",
      "2900: accuracy:0.97 loss: 7.9418783\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9766 test loss: 8.839481\n",
      "2910: accuracy:0.99 loss: 3.8755357\n",
      "2920: accuracy:1.0 loss: 0.75983953\n",
      "2930: accuracy:0.98 loss: 2.8645377\n",
      "2940: accuracy:0.99 loss: 1.0552257\n",
      "2950: accuracy:1.0 loss: 1.1033531\n",
      "2950: ********* epoch 5 ********* test accuracy:0.9746 test loss: 9.597393\n",
      "2960: accuracy:0.98 loss: 8.108056\n",
      "2970: accuracy:0.99 loss: 1.514811\n",
      "2980: accuracy:0.96 loss: 15.685732\n",
      "2990: accuracy:0.98 loss: 3.6890993\n",
      "3000: accuracy:0.99 loss: 1.9526525\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9754 test loss: 8.49502\n",
      "3010: accuracy:0.98 loss: 4.225224\n",
      "3020: accuracy:0.99 loss: 3.1709378\n",
      "3030: accuracy:0.97 loss: 4.8699713\n",
      "3040: accuracy:0.98 loss: 5.7428274\n",
      "3050: accuracy:0.98 loss: 5.740676\n",
      "3050: ********* epoch 6 ********* test accuracy:0.9769 test loss: 8.586168\n",
      "3060: accuracy:0.99 loss: 8.135081\n",
      "3070: accuracy:1.0 loss: 0.19576132\n",
      "3080: accuracy:0.98 loss: 4.1389956\n",
      "3090: accuracy:0.99 loss: 1.6705266\n",
      "3100: accuracy:0.98 loss: 5.0619793\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9759 test loss: 8.366709\n",
      "3110: accuracy:1.0 loss: 0.5755188\n",
      "3120: accuracy:1.0 loss: 0.7890047\n",
      "3130: accuracy:0.98 loss: 8.484804\n",
      "3140: accuracy:1.0 loss: 1.0901228\n",
      "3150: accuracy:0.99 loss: 5.0560126\n",
      "3150: ********* epoch 6 ********* test accuracy:0.9791 test loss: 7.0834837\n",
      "3160: accuracy:0.97 loss: 5.6380253\n",
      "3170: accuracy:1.0 loss: 0.513745\n",
      "3180: accuracy:1.0 loss: 1.5505362\n",
      "3190: accuracy:1.0 loss: 1.2701381\n",
      "3200: accuracy:1.0 loss: 1.3661231\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9781 test loss: 7.6527843\n",
      "3210: accuracy:0.99 loss: 7.437802\n",
      "3220: accuracy:0.99 loss: 8.301108\n",
      "3230: accuracy:0.99 loss: 3.2401407\n",
      "3240: accuracy:1.0 loss: 0.8206814\n",
      "3250: accuracy:0.98 loss: 4.465657\n",
      "3250: ********* epoch 6 ********* test accuracy:0.9795 test loss: 7.23744\n",
      "3260: accuracy:0.99 loss: 2.2929268\n",
      "3270: accuracy:0.97 loss: 4.4908686\n",
      "3280: accuracy:0.98 loss: 6.382333\n",
      "3290: accuracy:1.0 loss: 1.0669007\n",
      "3300: accuracy:0.99 loss: 9.3454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300: ********* epoch 6 ********* test accuracy:0.9811 test loss: 6.8894477\n",
      "3310: accuracy:1.0 loss: 0.73797554\n",
      "3320: accuracy:1.0 loss: 0.48449826\n",
      "3330: accuracy:1.0 loss: 0.56382537\n",
      "3340: accuracy:0.99 loss: 3.1748998\n",
      "3350: accuracy:0.99 loss: 2.114405\n",
      "3350: ********* epoch 6 ********* test accuracy:0.9813 test loss: 7.21327\n",
      "3360: accuracy:1.0 loss: 0.7264555\n",
      "3370: accuracy:0.99 loss: 1.283144\n",
      "3380: accuracy:0.09 loss: nan\n",
      "3390: accuracy:0.1 loss: nan\n",
      "3400: accuracy:0.06 loss: nan\n",
      "3400: ********* epoch 6 ********* test accuracy:0.098 test loss: nan\n",
      "3410: accuracy:0.11 loss: nan\n",
      "3420: accuracy:0.12 loss: nan\n",
      "3430: accuracy:0.13 loss: nan\n",
      "3440: accuracy:0.07 loss: nan\n",
      "3450: accuracy:0.11 loss: nan\n",
      "3450: ********* epoch 6 ********* test accuracy:0.098 test loss: nan\n",
      "3460: accuracy:0.13 loss: nan\n",
      "3470: accuracy:0.12 loss: nan\n",
      "3480: accuracy:0.08 loss: nan\n",
      "3490: accuracy:0.08 loss: nan\n",
      "3500: accuracy:0.12 loss: nan\n",
      "3500: ********* epoch 6 ********* test accuracy:0.098 test loss: nan\n",
      "3510: accuracy:0.18 loss: nan\n",
      "3520: accuracy:0.11 loss: nan\n",
      "3530: accuracy:0.12 loss: nan\n",
      "3540: accuracy:0.06 loss: nan\n",
      "3550: accuracy:0.1 loss: nan\n",
      "3550: ********* epoch 6 ********* test accuracy:0.098 test loss: nan\n",
      "3560: accuracy:0.08 loss: nan\n",
      "3570: accuracy:0.12 loss: nan\n",
      "3580: accuracy:0.08 loss: nan\n",
      "3590: accuracy:0.1 loss: nan\n",
      "3600: accuracy:0.04 loss: nan\n",
      "3600: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "3610: accuracy:0.09 loss: nan\n",
      "3620: accuracy:0.14 loss: nan\n",
      "3630: accuracy:0.12 loss: nan\n",
      "3640: accuracy:0.12 loss: nan\n",
      "3650: accuracy:0.05 loss: nan\n",
      "3650: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "3660: accuracy:0.11 loss: nan\n",
      "3670: accuracy:0.1 loss: nan\n",
      "3680: accuracy:0.09 loss: nan\n",
      "3690: accuracy:0.04 loss: nan\n",
      "3700: accuracy:0.09 loss: nan\n",
      "3700: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "3710: accuracy:0.08 loss: nan\n",
      "3720: accuracy:0.1 loss: nan\n",
      "3730: accuracy:0.13 loss: nan\n",
      "3740: accuracy:0.1 loss: nan\n",
      "3750: accuracy:0.11 loss: nan\n",
      "3750: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "3760: accuracy:0.06 loss: nan\n",
      "3770: accuracy:0.06 loss: nan\n",
      "3780: accuracy:0.1 loss: nan\n",
      "3790: accuracy:0.14 loss: nan\n",
      "3800: accuracy:0.1 loss: nan\n",
      "3800: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "3810: accuracy:0.14 loss: nan\n",
      "3820: accuracy:0.08 loss: nan\n",
      "3830: accuracy:0.13 loss: nan\n",
      "3840: accuracy:0.07 loss: nan\n",
      "3850: accuracy:0.08 loss: nan\n",
      "3850: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "3860: accuracy:0.14 loss: nan\n",
      "3870: accuracy:0.06 loss: nan\n",
      "3880: accuracy:0.13 loss: nan\n",
      "3890: accuracy:0.12 loss: nan\n",
      "3900: accuracy:0.08 loss: nan\n",
      "3900: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "3910: accuracy:0.15 loss: nan\n",
      "3920: accuracy:0.16 loss: nan\n",
      "3930: accuracy:0.12 loss: nan\n",
      "3940: accuracy:0.08 loss: nan\n",
      "3950: accuracy:0.1 loss: nan\n",
      "3950: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "3960: accuracy:0.08 loss: nan\n",
      "3970: accuracy:0.11 loss: nan\n",
      "3980: accuracy:0.11 loss: nan\n",
      "3990: accuracy:0.12 loss: nan\n",
      "4000: accuracy:0.11 loss: nan\n",
      "4000: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "4010: accuracy:0.08 loss: nan\n",
      "4020: accuracy:0.09 loss: nan\n",
      "4030: accuracy:0.17 loss: nan\n",
      "4040: accuracy:0.09 loss: nan\n",
      "4050: accuracy:0.09 loss: nan\n",
      "4050: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "4060: accuracy:0.08 loss: nan\n",
      "4070: accuracy:0.11 loss: nan\n",
      "4080: accuracy:0.1 loss: nan\n",
      "4090: accuracy:0.11 loss: nan\n",
      "4100: accuracy:0.11 loss: nan\n",
      "4100: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "4110: accuracy:0.13 loss: nan\n",
      "4120: accuracy:0.08 loss: nan\n",
      "4130: accuracy:0.07 loss: nan\n",
      "4140: accuracy:0.09 loss: nan\n",
      "4150: accuracy:0.13 loss: nan\n",
      "4150: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "4160: accuracy:0.1 loss: nan\n",
      "4170: accuracy:0.09 loss: nan\n",
      "4180: accuracy:0.1 loss: nan\n",
      "4190: accuracy:0.1 loss: nan\n",
      "4200: accuracy:0.11 loss: nan\n",
      "4200: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4210: accuracy:0.12 loss: nan\n",
      "4220: accuracy:0.07 loss: nan\n",
      "4230: accuracy:0.12 loss: nan\n",
      "4240: accuracy:0.11 loss: nan\n",
      "4250: accuracy:0.12 loss: nan\n",
      "4250: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4260: accuracy:0.16 loss: nan\n",
      "4270: accuracy:0.09 loss: nan\n",
      "4280: accuracy:0.12 loss: nan\n",
      "4290: accuracy:0.11 loss: nan\n",
      "4300: accuracy:0.09 loss: nan\n",
      "4300: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4310: accuracy:0.09 loss: nan\n",
      "4320: accuracy:0.11 loss: nan\n",
      "4330: accuracy:0.09 loss: nan\n",
      "4340: accuracy:0.07 loss: nan\n",
      "4350: accuracy:0.14 loss: nan\n",
      "4350: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4360: accuracy:0.07 loss: nan\n",
      "4370: accuracy:0.18 loss: nan\n",
      "4380: accuracy:0.08 loss: nan\n",
      "4390: accuracy:0.13 loss: nan\n",
      "4400: accuracy:0.11 loss: nan\n",
      "4400: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4410: accuracy:0.07 loss: nan\n",
      "4420: accuracy:0.09 loss: nan\n",
      "4430: accuracy:0.11 loss: nan\n",
      "4440: accuracy:0.09 loss: nan\n",
      "4450: accuracy:0.07 loss: nan\n",
      "4450: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4460: accuracy:0.12 loss: nan\n",
      "4470: accuracy:0.2 loss: nan\n",
      "4480: accuracy:0.08 loss: nan\n",
      "4490: accuracy:0.09 loss: nan\n",
      "4500: accuracy:0.12 loss: nan\n",
      "4500: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4510: accuracy:0.06 loss: nan\n",
      "4520: accuracy:0.14 loss: nan\n",
      "4530: accuracy:0.08 loss: nan\n",
      "4540: accuracy:0.09 loss: nan\n",
      "4550: accuracy:0.14 loss: nan\n",
      "4550: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4560: accuracy:0.06 loss: nan\n",
      "4570: accuracy:0.1 loss: nan\n",
      "4580: accuracy:0.14 loss: nan\n",
      "4590: accuracy:0.1 loss: nan\n",
      "4600: accuracy:0.14 loss: nan\n",
      "4600: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4610: accuracy:0.15 loss: nan\n",
      "4620: accuracy:0.12 loss: nan\n",
      "4630: accuracy:0.09 loss: nan\n",
      "4640: accuracy:0.07 loss: nan\n",
      "4650: accuracy:0.13 loss: nan\n",
      "4650: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4660: accuracy:0.1 loss: nan\n",
      "4670: accuracy:0.08 loss: nan\n",
      "4680: accuracy:0.09 loss: nan\n",
      "4690: accuracy:0.07 loss: nan\n",
      "4700: accuracy:0.09 loss: nan\n",
      "4700: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4710: accuracy:0.11 loss: nan\n",
      "4720: accuracy:0.12 loss: nan\n",
      "4730: accuracy:0.12 loss: nan\n",
      "4740: accuracy:0.11 loss: nan\n",
      "4750: accuracy:0.08 loss: nan\n",
      "4750: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4760: accuracy:0.11 loss: nan\n",
      "4770: accuracy:0.1 loss: nan\n",
      "4780: accuracy:0.05 loss: nan\n",
      "4790: accuracy:0.08 loss: nan\n",
      "4800: accuracy:0.09 loss: nan\n",
      "4800: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "4810: accuracy:0.06 loss: nan\n",
      "4820: accuracy:0.1 loss: nan\n",
      "4830: accuracy:0.1 loss: nan\n",
      "4840: accuracy:0.16 loss: nan\n",
      "4850: accuracy:0.08 loss: nan\n",
      "4850: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "4860: accuracy:0.08 loss: nan\n",
      "4870: accuracy:0.07 loss: nan\n",
      "4880: accuracy:0.12 loss: nan\n",
      "4890: accuracy:0.08 loss: nan\n",
      "4900: accuracy:0.1 loss: nan\n",
      "4900: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "4910: accuracy:0.08 loss: nan\n",
      "4920: accuracy:0.14 loss: nan\n",
      "4930: accuracy:0.07 loss: nan\n",
      "4940: accuracy:0.1 loss: nan\n",
      "4950: accuracy:0.18 loss: nan\n",
      "4950: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "4960: accuracy:0.1 loss: nan\n",
      "4970: accuracy:0.09 loss: nan\n",
      "4980: accuracy:0.08 loss: nan\n",
      "4990: accuracy:0.11 loss: nan\n",
      "5000: accuracy:0.06 loss: nan\n",
      "5000: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5010: accuracy:0.1 loss: nan\n",
      "5020: accuracy:0.05 loss: nan\n",
      "5030: accuracy:0.09 loss: nan\n",
      "5040: accuracy:0.09 loss: nan\n",
      "5050: accuracy:0.12 loss: nan\n",
      "5050: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5060: accuracy:0.07 loss: nan\n",
      "5070: accuracy:0.12 loss: nan\n",
      "5080: accuracy:0.13 loss: nan\n",
      "5090: accuracy:0.12 loss: nan\n",
      "5100: accuracy:0.09 loss: nan\n",
      "5100: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5110: accuracy:0.13 loss: nan\n",
      "5120: accuracy:0.1 loss: nan\n",
      "5130: accuracy:0.11 loss: nan\n",
      "5140: accuracy:0.11 loss: nan\n",
      "5150: accuracy:0.13 loss: nan\n",
      "5150: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160: accuracy:0.05 loss: nan\n",
      "5170: accuracy:0.1 loss: nan\n",
      "5180: accuracy:0.13 loss: nan\n",
      "5190: accuracy:0.07 loss: nan\n",
      "5200: accuracy:0.09 loss: nan\n",
      "5200: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5210: accuracy:0.13 loss: nan\n",
      "5220: accuracy:0.11 loss: nan\n",
      "5230: accuracy:0.08 loss: nan\n",
      "5240: accuracy:0.1 loss: nan\n",
      "5250: accuracy:0.12 loss: nan\n",
      "5250: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5260: accuracy:0.1 loss: nan\n",
      "5270: accuracy:0.11 loss: nan\n",
      "5280: accuracy:0.14 loss: nan\n",
      "5290: accuracy:0.07 loss: nan\n",
      "5300: accuracy:0.09 loss: nan\n",
      "5300: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5310: accuracy:0.12 loss: nan\n",
      "5320: accuracy:0.05 loss: nan\n",
      "5330: accuracy:0.08 loss: nan\n",
      "5340: accuracy:0.05 loss: nan\n",
      "5350: accuracy:0.06 loss: nan\n",
      "5350: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5360: accuracy:0.1 loss: nan\n",
      "5370: accuracy:0.1 loss: nan\n",
      "5380: accuracy:0.11 loss: nan\n",
      "5390: accuracy:0.1 loss: nan\n",
      "5400: accuracy:0.05 loss: nan\n",
      "5400: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5410: accuracy:0.17 loss: nan\n",
      "5420: accuracy:0.11 loss: nan\n",
      "5430: accuracy:0.1 loss: nan\n",
      "5440: accuracy:0.15 loss: nan\n",
      "5450: accuracy:0.15 loss: nan\n",
      "5450: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5460: accuracy:0.11 loss: nan\n",
      "5470: accuracy:0.09 loss: nan\n",
      "5480: accuracy:0.13 loss: nan\n",
      "5490: accuracy:0.13 loss: nan\n",
      "5500: accuracy:0.11 loss: nan\n",
      "5500: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5510: accuracy:0.11 loss: nan\n",
      "5520: accuracy:0.02 loss: nan\n",
      "5530: accuracy:0.13 loss: nan\n",
      "5540: accuracy:0.05 loss: nan\n",
      "5550: accuracy:0.11 loss: nan\n",
      "5550: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5560: accuracy:0.05 loss: nan\n",
      "5570: accuracy:0.06 loss: nan\n",
      "5580: accuracy:0.04 loss: nan\n",
      "5590: accuracy:0.14 loss: nan\n",
      "5600: accuracy:0.16 loss: nan\n",
      "5600: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5610: accuracy:0.13 loss: nan\n",
      "5620: accuracy:0.09 loss: nan\n",
      "5630: accuracy:0.12 loss: nan\n",
      "5640: accuracy:0.1 loss: nan\n",
      "5650: accuracy:0.08 loss: nan\n",
      "5650: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5660: accuracy:0.11 loss: nan\n",
      "5670: accuracy:0.13 loss: nan\n",
      "5680: accuracy:0.05 loss: nan\n",
      "5690: accuracy:0.06 loss: nan\n",
      "5700: accuracy:0.1 loss: nan\n",
      "5700: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5710: accuracy:0.07 loss: nan\n",
      "5720: accuracy:0.1 loss: nan\n",
      "5730: accuracy:0.14 loss: nan\n",
      "5740: accuracy:0.08 loss: nan\n",
      "5750: accuracy:0.06 loss: nan\n",
      "5750: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5760: accuracy:0.05 loss: nan\n",
      "5770: accuracy:0.03 loss: nan\n",
      "5780: accuracy:0.15 loss: nan\n",
      "5790: accuracy:0.1 loss: nan\n",
      "5800: accuracy:0.08 loss: nan\n",
      "5800: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5810: accuracy:0.07 loss: nan\n",
      "5820: accuracy:0.11 loss: nan\n",
      "5830: accuracy:0.14 loss: nan\n",
      "5840: accuracy:0.07 loss: nan\n",
      "5850: accuracy:0.12 loss: nan\n",
      "5850: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5860: accuracy:0.11 loss: nan\n",
      "5870: accuracy:0.1 loss: nan\n",
      "5880: accuracy:0.07 loss: nan\n",
      "5890: accuracy:0.05 loss: nan\n",
      "5900: accuracy:0.11 loss: nan\n",
      "5900: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5910: accuracy:0.08 loss: nan\n",
      "5920: accuracy:0.09 loss: nan\n",
      "5930: accuracy:0.06 loss: nan\n",
      "5940: accuracy:0.13 loss: nan\n",
      "5950: accuracy:0.06 loss: nan\n",
      "5950: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5960: accuracy:0.08 loss: nan\n",
      "5970: accuracy:0.13 loss: nan\n",
      "5980: accuracy:0.06 loss: nan\n",
      "5990: accuracy:0.07 loss: nan\n",
      "6000: accuracy:0.08 loss: nan\n",
      "6000: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6010: accuracy:0.11 loss: nan\n",
      "6020: accuracy:0.09 loss: nan\n",
      "6030: accuracy:0.09 loss: nan\n",
      "6040: accuracy:0.11 loss: nan\n",
      "6050: accuracy:0.15 loss: nan\n",
      "6050: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6060: accuracy:0.11 loss: nan\n",
      "6070: accuracy:0.07 loss: nan\n",
      "6080: accuracy:0.12 loss: nan\n",
      "6090: accuracy:0.07 loss: nan\n",
      "6100: accuracy:0.09 loss: nan\n",
      "6100: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6110: accuracy:0.12 loss: nan\n",
      "6120: accuracy:0.08 loss: nan\n",
      "6130: accuracy:0.09 loss: nan\n",
      "6140: accuracy:0.12 loss: nan\n",
      "6150: accuracy:0.13 loss: nan\n",
      "6150: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6160: accuracy:0.09 loss: nan\n",
      "6170: accuracy:0.11 loss: nan\n",
      "6180: accuracy:0.11 loss: nan\n",
      "6190: accuracy:0.08 loss: nan\n",
      "6200: accuracy:0.09 loss: nan\n",
      "6200: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6210: accuracy:0.09 loss: nan\n",
      "6220: accuracy:0.06 loss: nan\n",
      "6230: accuracy:0.08 loss: nan\n",
      "6240: accuracy:0.18 loss: nan\n",
      "6250: accuracy:0.1 loss: nan\n",
      "6250: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6260: accuracy:0.1 loss: nan\n",
      "6270: accuracy:0.05 loss: nan\n",
      "6280: accuracy:0.1 loss: nan\n",
      "6290: accuracy:0.05 loss: nan\n",
      "6300: accuracy:0.15 loss: nan\n",
      "6300: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6310: accuracy:0.11 loss: nan\n",
      "6320: accuracy:0.11 loss: nan\n",
      "6330: accuracy:0.08 loss: nan\n",
      "6340: accuracy:0.15 loss: nan\n",
      "6350: accuracy:0.09 loss: nan\n",
      "6350: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6360: accuracy:0.09 loss: nan\n",
      "6370: accuracy:0.13 loss: nan\n",
      "6380: accuracy:0.09 loss: nan\n",
      "6390: accuracy:0.07 loss: nan\n",
      "6400: accuracy:0.11 loss: nan\n",
      "6400: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6410: accuracy:0.07 loss: nan\n",
      "6420: accuracy:0.15 loss: nan\n",
      "6430: accuracy:0.1 loss: nan\n",
      "6440: accuracy:0.15 loss: nan\n",
      "6450: accuracy:0.1 loss: nan\n",
      "6450: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6460: accuracy:0.11 loss: nan\n",
      "6470: accuracy:0.1 loss: nan\n",
      "6480: accuracy:0.15 loss: nan\n",
      "6490: accuracy:0.09 loss: nan\n",
      "6500: accuracy:0.17 loss: nan\n",
      "6500: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6510: accuracy:0.08 loss: nan\n",
      "6520: accuracy:0.15 loss: nan\n",
      "6530: accuracy:0.11 loss: nan\n",
      "6540: accuracy:0.14 loss: nan\n",
      "6550: accuracy:0.11 loss: nan\n",
      "6550: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6560: accuracy:0.13 loss: nan\n",
      "6570: accuracy:0.08 loss: nan\n",
      "6580: accuracy:0.07 loss: nan\n",
      "6590: accuracy:0.12 loss: nan\n",
      "6600: accuracy:0.07 loss: nan\n",
      "6600: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6610: accuracy:0.09 loss: nan\n",
      "6620: accuracy:0.09 loss: nan\n",
      "6630: accuracy:0.06 loss: nan\n",
      "6640: accuracy:0.06 loss: nan\n",
      "6650: accuracy:0.08 loss: nan\n",
      "6650: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6660: accuracy:0.08 loss: nan\n",
      "6670: accuracy:0.07 loss: nan\n",
      "6680: accuracy:0.08 loss: nan\n",
      "6690: accuracy:0.08 loss: nan\n",
      "6700: accuracy:0.09 loss: nan\n",
      "6700: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6710: accuracy:0.11 loss: nan\n",
      "6720: accuracy:0.07 loss: nan\n",
      "6730: accuracy:0.11 loss: nan\n",
      "6740: accuracy:0.05 loss: nan\n",
      "6750: accuracy:0.09 loss: nan\n",
      "6750: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6760: accuracy:0.11 loss: nan\n",
      "6770: accuracy:0.15 loss: nan\n",
      "6780: accuracy:0.07 loss: nan\n",
      "6790: accuracy:0.08 loss: nan\n",
      "6800: accuracy:0.09 loss: nan\n",
      "6800: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6810: accuracy:0.1 loss: nan\n",
      "6820: accuracy:0.05 loss: nan\n",
      "6830: accuracy:0.11 loss: nan\n",
      "6840: accuracy:0.06 loss: nan\n",
      "6850: accuracy:0.07 loss: nan\n",
      "6850: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6860: accuracy:0.12 loss: nan\n",
      "6870: accuracy:0.17 loss: nan\n",
      "6880: accuracy:0.12 loss: nan\n",
      "6890: accuracy:0.06 loss: nan\n",
      "6900: accuracy:0.1 loss: nan\n",
      "6900: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6910: accuracy:0.1 loss: nan\n",
      "6920: accuracy:0.06 loss: nan\n",
      "6930: accuracy:0.06 loss: nan\n",
      "6940: accuracy:0.07 loss: nan\n",
      "6950: accuracy:0.04 loss: nan\n",
      "6950: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6960: accuracy:0.16 loss: nan\n",
      "6970: accuracy:0.14 loss: nan\n",
      "6980: accuracy:0.11 loss: nan\n",
      "6990: accuracy:0.04 loss: nan\n",
      "7000: accuracy:0.07 loss: nan\n",
      "7000: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "7010: accuracy:0.03 loss: nan\n",
      "7020: accuracy:0.09 loss: nan\n",
      "7030: accuracy:0.04 loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7040: accuracy:0.12 loss: nan\n",
      "7050: accuracy:0.09 loss: nan\n",
      "7050: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "7060: accuracy:0.09 loss: nan\n",
      "7070: accuracy:0.13 loss: nan\n",
      "7080: accuracy:0.11 loss: nan\n",
      "7090: accuracy:0.13 loss: nan\n",
      "7100: accuracy:0.08 loss: nan\n",
      "7100: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "7110: accuracy:0.07 loss: nan\n",
      "7120: accuracy:0.09 loss: nan\n",
      "7130: accuracy:0.09 loss: nan\n",
      "7140: accuracy:0.1 loss: nan\n",
      "7150: accuracy:0.09 loss: nan\n",
      "7150: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "7160: accuracy:0.1 loss: nan\n",
      "7170: accuracy:0.1 loss: nan\n",
      "7180: accuracy:0.13 loss: nan\n",
      "7190: accuracy:0.14 loss: nan\n",
      "7200: accuracy:0.13 loss: nan\n",
      "7200: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7210: accuracy:0.07 loss: nan\n",
      "7220: accuracy:0.18 loss: nan\n",
      "7230: accuracy:0.14 loss: nan\n",
      "7240: accuracy:0.06 loss: nan\n",
      "7250: accuracy:0.15 loss: nan\n",
      "7250: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7260: accuracy:0.12 loss: nan\n",
      "7270: accuracy:0.14 loss: nan\n",
      "7280: accuracy:0.09 loss: nan\n",
      "7290: accuracy:0.12 loss: nan\n",
      "7300: accuracy:0.04 loss: nan\n",
      "7300: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7310: accuracy:0.11 loss: nan\n",
      "7320: accuracy:0.14 loss: nan\n",
      "7330: accuracy:0.07 loss: nan\n",
      "7340: accuracy:0.12 loss: nan\n",
      "7350: accuracy:0.07 loss: nan\n",
      "7350: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7360: accuracy:0.04 loss: nan\n",
      "7370: accuracy:0.11 loss: nan\n",
      "7380: accuracy:0.1 loss: nan\n",
      "7390: accuracy:0.08 loss: nan\n",
      "7400: accuracy:0.09 loss: nan\n",
      "7400: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7410: accuracy:0.08 loss: nan\n",
      "7420: accuracy:0.11 loss: nan\n",
      "7430: accuracy:0.04 loss: nan\n",
      "7440: accuracy:0.12 loss: nan\n",
      "7450: accuracy:0.07 loss: nan\n",
      "7450: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7460: accuracy:0.11 loss: nan\n",
      "7470: accuracy:0.08 loss: nan\n",
      "7480: accuracy:0.09 loss: nan\n",
      "7490: accuracy:0.07 loss: nan\n",
      "7500: accuracy:0.13 loss: nan\n",
      "7500: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7510: accuracy:0.07 loss: nan\n",
      "7520: accuracy:0.1 loss: nan\n",
      "7530: accuracy:0.08 loss: nan\n",
      "7540: accuracy:0.07 loss: nan\n",
      "7550: accuracy:0.1 loss: nan\n",
      "7550: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7560: accuracy:0.17 loss: nan\n",
      "7570: accuracy:0.07 loss: nan\n",
      "7580: accuracy:0.07 loss: nan\n",
      "7590: accuracy:0.08 loss: nan\n",
      "7600: accuracy:0.09 loss: nan\n",
      "7600: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7610: accuracy:0.09 loss: nan\n",
      "7620: accuracy:0.08 loss: nan\n",
      "7630: accuracy:0.12 loss: nan\n",
      "7640: accuracy:0.11 loss: nan\n",
      "7650: accuracy:0.08 loss: nan\n",
      "7650: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7660: accuracy:0.06 loss: nan\n",
      "7670: accuracy:0.12 loss: nan\n",
      "7680: accuracy:0.1 loss: nan\n",
      "7690: accuracy:0.08 loss: nan\n",
      "7700: accuracy:0.06 loss: nan\n",
      "7700: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7710: accuracy:0.11 loss: nan\n",
      "7720: accuracy:0.06 loss: nan\n",
      "7730: accuracy:0.11 loss: nan\n",
      "7740: accuracy:0.11 loss: nan\n",
      "7750: accuracy:0.08 loss: nan\n",
      "7750: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7760: accuracy:0.05 loss: nan\n",
      "7770: accuracy:0.09 loss: nan\n",
      "7780: accuracy:0.14 loss: nan\n",
      "7790: accuracy:0.1 loss: nan\n",
      "7800: accuracy:0.1 loss: nan\n",
      "7800: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "7810: accuracy:0.08 loss: nan\n",
      "7820: accuracy:0.09 loss: nan\n",
      "7830: accuracy:0.12 loss: nan\n",
      "7840: accuracy:0.09 loss: nan\n",
      "7850: accuracy:0.06 loss: nan\n",
      "7850: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "7860: accuracy:0.13 loss: nan\n",
      "7870: accuracy:0.12 loss: nan\n",
      "7880: accuracy:0.12 loss: nan\n",
      "7890: accuracy:0.12 loss: nan\n",
      "7900: accuracy:0.16 loss: nan\n",
      "7900: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "7910: accuracy:0.1 loss: nan\n",
      "7920: accuracy:0.09 loss: nan\n",
      "7930: accuracy:0.06 loss: nan\n",
      "7940: accuracy:0.09 loss: nan\n",
      "7950: accuracy:0.11 loss: nan\n",
      "7950: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "7960: accuracy:0.11 loss: nan\n",
      "7970: accuracy:0.16 loss: nan\n",
      "7980: accuracy:0.08 loss: nan\n",
      "7990: accuracy:0.07 loss: nan\n",
      "8000: accuracy:0.13 loss: nan\n",
      "8000: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8010: accuracy:0.06 loss: nan\n",
      "8020: accuracy:0.11 loss: nan\n",
      "8030: accuracy:0.12 loss: nan\n",
      "8040: accuracy:0.14 loss: nan\n",
      "8050: accuracy:0.1 loss: nan\n",
      "8050: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8060: accuracy:0.09 loss: nan\n",
      "8070: accuracy:0.11 loss: nan\n",
      "8080: accuracy:0.11 loss: nan\n",
      "8090: accuracy:0.07 loss: nan\n",
      "8100: accuracy:0.11 loss: nan\n",
      "8100: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8110: accuracy:0.11 loss: nan\n",
      "8120: accuracy:0.12 loss: nan\n",
      "8130: accuracy:0.12 loss: nan\n",
      "8140: accuracy:0.08 loss: nan\n",
      "8150: accuracy:0.13 loss: nan\n",
      "8150: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8160: accuracy:0.08 loss: nan\n",
      "8170: accuracy:0.08 loss: nan\n",
      "8180: accuracy:0.08 loss: nan\n",
      "8190: accuracy:0.11 loss: nan\n",
      "8200: accuracy:0.08 loss: nan\n",
      "8200: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8210: accuracy:0.03 loss: nan\n",
      "8220: accuracy:0.11 loss: nan\n",
      "8230: accuracy:0.07 loss: nan\n",
      "8240: accuracy:0.09 loss: nan\n",
      "8250: accuracy:0.1 loss: nan\n",
      "8250: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8260: accuracy:0.12 loss: nan\n",
      "8270: accuracy:0.08 loss: nan\n",
      "8280: accuracy:0.1 loss: nan\n",
      "8290: accuracy:0.14 loss: nan\n",
      "8300: accuracy:0.1 loss: nan\n",
      "8300: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8310: accuracy:0.09 loss: nan\n",
      "8320: accuracy:0.16 loss: nan\n",
      "8330: accuracy:0.1 loss: nan\n",
      "8340: accuracy:0.15 loss: nan\n",
      "8350: accuracy:0.09 loss: nan\n",
      "8350: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8360: accuracy:0.16 loss: nan\n",
      "8370: accuracy:0.12 loss: nan\n",
      "8380: accuracy:0.11 loss: nan\n",
      "8390: accuracy:0.1 loss: nan\n",
      "8400: accuracy:0.13 loss: nan\n",
      "8400: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8410: accuracy:0.07 loss: nan\n",
      "8420: accuracy:0.13 loss: nan\n",
      "8430: accuracy:0.06 loss: nan\n",
      "8440: accuracy:0.07 loss: nan\n",
      "8450: accuracy:0.1 loss: nan\n",
      "8450: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8460: accuracy:0.15 loss: nan\n",
      "8470: accuracy:0.12 loss: nan\n",
      "8480: accuracy:0.05 loss: nan\n",
      "8490: accuracy:0.08 loss: nan\n",
      "8500: accuracy:0.06 loss: nan\n",
      "8500: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8510: accuracy:0.08 loss: nan\n",
      "8520: accuracy:0.06 loss: nan\n",
      "8530: accuracy:0.06 loss: nan\n",
      "8540: accuracy:0.11 loss: nan\n",
      "8550: accuracy:0.13 loss: nan\n",
      "8550: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8560: accuracy:0.1 loss: nan\n",
      "8570: accuracy:0.11 loss: nan\n",
      "8580: accuracy:0.09 loss: nan\n",
      "8590: accuracy:0.14 loss: nan\n",
      "8600: accuracy:0.09 loss: nan\n",
      "8600: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8610: accuracy:0.09 loss: nan\n",
      "8620: accuracy:0.12 loss: nan\n",
      "8630: accuracy:0.05 loss: nan\n",
      "8640: accuracy:0.1 loss: nan\n",
      "8650: accuracy:0.14 loss: nan\n",
      "8650: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8660: accuracy:0.1 loss: nan\n",
      "8670: accuracy:0.06 loss: nan\n",
      "8680: accuracy:0.12 loss: nan\n",
      "8690: accuracy:0.06 loss: nan\n",
      "8700: accuracy:0.07 loss: nan\n",
      "8700: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8710: accuracy:0.12 loss: nan\n",
      "8720: accuracy:0.11 loss: nan\n",
      "8730: accuracy:0.09 loss: nan\n",
      "8740: accuracy:0.04 loss: nan\n",
      "8750: accuracy:0.11 loss: nan\n",
      "8750: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8760: accuracy:0.1 loss: nan\n",
      "8770: accuracy:0.1 loss: nan\n",
      "8780: accuracy:0.12 loss: nan\n",
      "8790: accuracy:0.09 loss: nan\n",
      "8800: accuracy:0.05 loss: nan\n",
      "8800: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8810: accuracy:0.11 loss: nan\n",
      "8820: accuracy:0.11 loss: nan\n",
      "8830: accuracy:0.07 loss: nan\n",
      "8840: accuracy:0.06 loss: nan\n",
      "8850: accuracy:0.12 loss: nan\n",
      "8850: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8860: accuracy:0.11 loss: nan\n",
      "8870: accuracy:0.09 loss: nan\n",
      "8880: accuracy:0.1 loss: nan\n",
      "8890: accuracy:0.11 loss: nan\n",
      "8900: accuracy:0.09 loss: nan\n",
      "8900: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8910: accuracy:0.18 loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8920: accuracy:0.13 loss: nan\n",
      "8930: accuracy:0.1 loss: nan\n",
      "8940: accuracy:0.05 loss: nan\n",
      "8950: accuracy:0.12 loss: nan\n",
      "8950: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8960: accuracy:0.09 loss: nan\n",
      "8970: accuracy:0.15 loss: nan\n",
      "8980: accuracy:0.1 loss: nan\n",
      "8990: accuracy:0.13 loss: nan\n",
      "9000: accuracy:0.06 loss: nan\n",
      "9000: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9010: accuracy:0.11 loss: nan\n",
      "9020: accuracy:0.06 loss: nan\n",
      "9030: accuracy:0.09 loss: nan\n",
      "9040: accuracy:0.16 loss: nan\n",
      "9050: accuracy:0.09 loss: nan\n",
      "9050: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9060: accuracy:0.13 loss: nan\n",
      "9070: accuracy:0.09 loss: nan\n",
      "9080: accuracy:0.04 loss: nan\n",
      "9090: accuracy:0.07 loss: nan\n",
      "9100: accuracy:0.06 loss: nan\n",
      "9100: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9110: accuracy:0.08 loss: nan\n",
      "9120: accuracy:0.05 loss: nan\n",
      "9130: accuracy:0.11 loss: nan\n",
      "9140: accuracy:0.06 loss: nan\n",
      "9150: accuracy:0.13 loss: nan\n",
      "9150: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9160: accuracy:0.09 loss: nan\n",
      "9170: accuracy:0.11 loss: nan\n",
      "9180: accuracy:0.07 loss: nan\n",
      "9190: accuracy:0.08 loss: nan\n",
      "9200: accuracy:0.06 loss: nan\n",
      "9200: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9210: accuracy:0.09 loss: nan\n",
      "9220: accuracy:0.07 loss: nan\n",
      "9230: accuracy:0.07 loss: nan\n",
      "9240: accuracy:0.07 loss: nan\n",
      "9250: accuracy:0.13 loss: nan\n",
      "9250: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9260: accuracy:0.13 loss: nan\n",
      "9270: accuracy:0.05 loss: nan\n",
      "9280: accuracy:0.14 loss: nan\n",
      "9290: accuracy:0.12 loss: nan\n",
      "9300: accuracy:0.04 loss: nan\n",
      "9300: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9310: accuracy:0.19 loss: nan\n",
      "9320: accuracy:0.09 loss: nan\n",
      "9330: accuracy:0.07 loss: nan\n",
      "9340: accuracy:0.1 loss: nan\n",
      "9350: accuracy:0.08 loss: nan\n",
      "9350: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9360: accuracy:0.09 loss: nan\n",
      "9370: accuracy:0.07 loss: nan\n",
      "9380: accuracy:0.05 loss: nan\n",
      "9390: accuracy:0.11 loss: nan\n",
      "9400: accuracy:0.1 loss: nan\n",
      "9400: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9410: accuracy:0.09 loss: nan\n",
      "9420: accuracy:0.18 loss: nan\n",
      "9430: accuracy:0.09 loss: nan\n",
      "9440: accuracy:0.1 loss: nan\n",
      "9450: accuracy:0.09 loss: nan\n",
      "9450: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9460: accuracy:0.1 loss: nan\n",
      "9470: accuracy:0.1 loss: nan\n",
      "9480: accuracy:0.09 loss: nan\n",
      "9490: accuracy:0.09 loss: nan\n",
      "9500: accuracy:0.12 loss: nan\n",
      "9500: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9510: accuracy:0.13 loss: nan\n",
      "9520: accuracy:0.08 loss: nan\n",
      "9530: accuracy:0.09 loss: nan\n",
      "9540: accuracy:0.09 loss: nan\n",
      "9550: accuracy:0.08 loss: nan\n",
      "9550: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9560: accuracy:0.17 loss: nan\n",
      "9570: accuracy:0.07 loss: nan\n",
      "9580: accuracy:0.11 loss: nan\n",
      "9590: accuracy:0.04 loss: nan\n",
      "9600: accuracy:0.06 loss: nan\n",
      "9600: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9610: accuracy:0.09 loss: nan\n",
      "9620: accuracy:0.11 loss: nan\n",
      "9630: accuracy:0.16 loss: nan\n",
      "9640: accuracy:0.05 loss: nan\n",
      "9650: accuracy:0.1 loss: nan\n",
      "9650: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9660: accuracy:0.05 loss: nan\n",
      "9670: accuracy:0.14 loss: nan\n",
      "9680: accuracy:0.12 loss: nan\n",
      "9690: accuracy:0.11 loss: nan\n",
      "9700: accuracy:0.12 loss: nan\n",
      "9700: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9710: accuracy:0.09 loss: nan\n",
      "9720: accuracy:0.09 loss: nan\n",
      "9730: accuracy:0.08 loss: nan\n",
      "9740: accuracy:0.11 loss: nan\n",
      "9750: accuracy:0.14 loss: nan\n",
      "9750: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9760: accuracy:0.13 loss: nan\n",
      "9770: accuracy:0.12 loss: nan\n",
      "9780: accuracy:0.15 loss: nan\n",
      "9790: accuracy:0.11 loss: nan\n",
      "9800: accuracy:0.1 loss: nan\n",
      "9800: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9810: accuracy:0.14 loss: nan\n",
      "9820: accuracy:0.11 loss: nan\n",
      "9830: accuracy:0.12 loss: nan\n",
      "9840: accuracy:0.11 loss: nan\n",
      "9850: accuracy:0.12 loss: nan\n",
      "9850: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9860: accuracy:0.14 loss: nan\n",
      "9870: accuracy:0.07 loss: nan\n",
      "9880: accuracy:0.12 loss: nan\n",
      "9890: accuracy:0.03 loss: nan\n",
      "9900: accuracy:0.12 loss: nan\n",
      "9900: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9910: accuracy:0.09 loss: nan\n",
      "9920: accuracy:0.12 loss: nan\n",
      "9930: accuracy:0.13 loss: nan\n",
      "9940: accuracy:0.05 loss: nan\n",
      "9950: accuracy:0.09 loss: nan\n",
      "9950: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9960: accuracy:0.11 loss: nan\n",
      "9970: accuracy:0.1 loss: nan\n",
      "9980: accuracy:0.13 loss: nan\n",
      "9990: accuracy:0.13 loss: nan\n",
      "10000: accuracy:0.06 loss: nan\n",
      "10000: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "max test accuracy: 0.9817\n"
     ]
    }
   ],
   "source": [
    "# input X => 28x28 grayscale dimensions ('None' indexes images in mini-batch)\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "\n",
    "# correct answers\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights\n",
    "W1 = tf.Variable(tf.truncated_normal([28*28, 200], stddev=0.1))\n",
    "W2 = tf.Variable(tf.truncated_normal([200, 100], stddev=0.1))\n",
    "W3 = tf.Variable(tf.truncated_normal([100, 10], stddev=0.1))\n",
    "\n",
    "# biases\n",
    "b1 = tf.Variable(tf.zeros([200]))\n",
    "b2 = tf.Variable(tf.zeros([100]))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# flatten images into single line\n",
    "XX = tf.reshape(X, [-1, 28*28])\n",
    "\n",
    "# model\n",
    "Y1 = tf.nn.relu(tf.matmul(XX, W1) + b1)\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)\n",
    "Y = tf.nn.softmax(tf.matmul(Y2, W3) + b3)\n",
    "\n",
    "# cross entropy\n",
    "cross_entropy = -tf.reduce_mean(Y_ * tf.log(Y)) * 1000.0\n",
    "\n",
    "# accuracy of the trained model [0, 1] ([worst, best])\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# training (learning rate is 0.003)\n",
    "train_step = tf.train.AdamOptimizer(0.003).minimize(cross_entropy)\n",
    "\n",
    "# initialization\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# call in loop to train the model 100 images at a time\n",
    "max_test_accuracy = 0.0\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    # train on batches of 100 images w/ 100 labels\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    # compute training values for visualisation\n",
    "    if update_train_data:\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: batch_X, Y_: batch_Y})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c))\n",
    "    # compute test values for visualization\n",
    "    if update_test_data:\n",
    "        global max_test_accuracy\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: mnist.test.images, Y_: mnist.test.labels})\n",
    "        if (a > max_test_accuracy):\n",
    "            max_test_accuracy = a\n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \n",
    "              \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        \n",
    "    # back-propogation training step\n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y})\n",
    "\n",
    "# text visualization of process\n",
    "for i in range(10000+1):\n",
    "    training_step(i, i % 50 == 0, i % 10 == 0)\n",
    "    \n",
    "# display the final max accuracy\n",
    "print(\"max test accuracy: \" + str(max_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Convergence with Random Initializations\n",
    "The following cell contains modificaitons to the code above to include random initializations for weights (already being done) and small, positive values for biases. This allows the neurons to operate in the non-zero range of the RELU initially. With 10k iterations, this maxes out with an accuracy of ~98% (includes NaN!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.13 loss: 241.93651\n",
      "0: ********* epoch 1 ********* test accuracy:0.0952 test loss: 241.49664\n",
      "10: accuracy:0.75 loss: 81.530785\n",
      "20: accuracy:0.83 loss: 53.43902\n",
      "30: accuracy:0.9 loss: 33.85511\n",
      "40: accuracy:0.88 loss: 46.384666\n",
      "50: accuracy:0.94 loss: 36.042404\n",
      "50: ********* epoch 1 ********* test accuracy:0.9028 test loss: 31.947311\n",
      "60: accuracy:0.89 loss: 27.614103\n",
      "70: accuracy:0.92 loss: 36.936935\n",
      "80: accuracy:0.91 loss: 40.84304\n",
      "90: accuracy:0.93 loss: 27.475853\n",
      "100: accuracy:0.95 loss: 15.02158\n",
      "100: ********* epoch 1 ********* test accuracy:0.9262 test loss: 24.532827\n",
      "110: accuracy:0.94 loss: 24.936214\n",
      "120: accuracy:0.93 loss: 26.347145\n",
      "130: accuracy:0.94 loss: 21.897276\n",
      "140: accuracy:0.92 loss: 22.09547\n",
      "150: accuracy:0.95 loss: 14.842583\n",
      "150: ********* epoch 1 ********* test accuracy:0.9371 test loss: 19.627193\n",
      "160: accuracy:0.95 loss: 13.662666\n",
      "170: accuracy:0.92 loss: 23.529816\n",
      "180: accuracy:0.9 loss: 18.455608\n",
      "190: accuracy:0.98 loss: 10.2539\n",
      "200: accuracy:0.91 loss: 22.30212\n",
      "200: ********* epoch 1 ********* test accuracy:0.942 test loss: 18.324492\n",
      "210: accuracy:0.89 loss: 30.435429\n",
      "220: accuracy:0.91 loss: 38.601723\n",
      "230: accuracy:0.94 loss: 25.246326\n",
      "240: accuracy:0.98 loss: 7.9308662\n",
      "250: accuracy:0.92 loss: 27.846334\n",
      "250: ********* epoch 1 ********* test accuracy:0.9505 test loss: 16.267824\n",
      "260: accuracy:0.93 loss: 19.123003\n",
      "270: accuracy:0.98 loss: 9.945631\n",
      "280: accuracy:0.96 loss: 15.743887\n",
      "290: accuracy:0.94 loss: 22.742645\n",
      "300: accuracy:0.94 loss: 15.59267\n",
      "300: ********* epoch 1 ********* test accuracy:0.9483 test loss: 16.048073\n",
      "310: accuracy:0.94 loss: 16.135738\n",
      "320: accuracy:0.97 loss: 12.051184\n",
      "330: accuracy:0.99 loss: 5.250989\n",
      "340: accuracy:0.93 loss: 23.916296\n",
      "350: accuracy:0.94 loss: 18.309374\n",
      "350: ********* epoch 1 ********* test accuracy:0.9507 test loss: 15.334427\n",
      "360: accuracy:0.93 loss: 18.975788\n",
      "370: accuracy:0.96 loss: 9.259195\n",
      "380: accuracy:0.96 loss: 12.561837\n",
      "390: accuracy:0.94 loss: 18.145355\n",
      "400: accuracy:0.98 loss: 9.320253\n",
      "400: ********* epoch 1 ********* test accuracy:0.9603 test loss: 12.700686\n",
      "410: accuracy:0.98 loss: 7.0032787\n",
      "420: accuracy:0.94 loss: 16.69063\n",
      "430: accuracy:0.96 loss: 16.960514\n",
      "440: accuracy:0.96 loss: 7.705141\n",
      "450: accuracy:0.98 loss: 6.130456\n",
      "450: ********* epoch 1 ********* test accuracy:0.9581 test loss: 12.78622\n",
      "460: accuracy:0.97 loss: 10.042942\n",
      "470: accuracy:0.99 loss: 7.6938953\n",
      "480: accuracy:0.95 loss: 11.614016\n",
      "490: accuracy:0.96 loss: 13.879593\n",
      "500: accuracy:0.97 loss: 9.531429\n",
      "500: ********* epoch 1 ********* test accuracy:0.9638 test loss: 11.794572\n",
      "510: accuracy:0.96 loss: 9.378103\n",
      "520: accuracy:0.98 loss: 8.584686\n",
      "530: accuracy:0.94 loss: 13.742125\n",
      "540: accuracy:0.96 loss: 11.80704\n",
      "550: accuracy:0.99 loss: 7.980982\n",
      "550: ********* epoch 1 ********* test accuracy:0.961 test loss: 13.014396\n",
      "560: accuracy:0.93 loss: 36.639336\n",
      "570: accuracy:0.97 loss: 13.18961\n",
      "580: accuracy:0.96 loss: 8.527836\n",
      "590: accuracy:0.96 loss: 11.846218\n",
      "600: accuracy:0.98 loss: 6.4437895\n",
      "600: ********* epoch 2 ********* test accuracy:0.9621 test loss: 11.671822\n",
      "610: accuracy:0.95 loss: 10.340537\n",
      "620: accuracy:0.94 loss: 13.293729\n",
      "630: accuracy:0.94 loss: 11.700188\n",
      "640: accuracy:0.97 loss: 9.188133\n",
      "650: accuracy:0.97 loss: 8.473013\n",
      "650: ********* epoch 2 ********* test accuracy:0.9681 test loss: 10.235168\n",
      "660: accuracy:0.98 loss: 8.610548\n",
      "670: accuracy:0.95 loss: 11.035336\n",
      "680: accuracy:0.97 loss: 11.514891\n",
      "690: accuracy:0.96 loss: 9.3168745\n",
      "700: accuracy:0.97 loss: 6.8243723\n",
      "700: ********* epoch 2 ********* test accuracy:0.9613 test loss: 11.97437\n",
      "710: accuracy:0.95 loss: 11.313944\n",
      "720: accuracy:0.98 loss: 5.1630597\n",
      "730: accuracy:0.93 loss: 13.228154\n",
      "740: accuracy:0.98 loss: 5.6711597\n",
      "750: accuracy:0.98 loss: 5.1971765\n",
      "750: ********* epoch 2 ********* test accuracy:0.9671 test loss: 10.327203\n",
      "760: accuracy:0.97 loss: 7.749888\n",
      "770: accuracy:0.95 loss: 18.191023\n",
      "780: accuracy:0.93 loss: 17.744492\n",
      "790: accuracy:0.98 loss: 13.449137\n",
      "800: accuracy:0.95 loss: 11.829903\n",
      "800: ********* epoch 2 ********* test accuracy:0.9643 test loss: 11.015861\n",
      "810: accuracy:0.99 loss: 6.5813885\n",
      "820: accuracy:0.98 loss: 6.0734615\n",
      "830: accuracy:0.98 loss: 7.907537\n",
      "840: accuracy:0.99 loss: 6.9705973\n",
      "850: accuracy:0.96 loss: 17.49889\n",
      "850: ********* epoch 2 ********* test accuracy:0.962 test loss: 12.416672\n",
      "860: accuracy:0.99 loss: 3.4101253\n",
      "870: accuracy:0.97 loss: 7.9838524\n",
      "880: accuracy:0.98 loss: 6.141679\n",
      "890: accuracy:0.96 loss: 8.492222\n",
      "900: accuracy:0.97 loss: 10.754484\n",
      "900: ********* epoch 2 ********* test accuracy:0.9684 test loss: 9.97721\n",
      "910: accuracy:0.96 loss: 8.590134\n",
      "920: accuracy:0.95 loss: 10.619988\n",
      "930: accuracy:0.98 loss: 5.0761743\n",
      "940: accuracy:0.96 loss: 15.287829\n",
      "950: accuracy:0.97 loss: 8.183023\n",
      "950: ********* epoch 2 ********* test accuracy:0.9705 test loss: 9.526776\n",
      "960: accuracy:0.98 loss: 6.448908\n",
      "970: accuracy:0.97 loss: 11.662657\n",
      "980: accuracy:0.98 loss: 7.680712\n",
      "990: accuracy:0.98 loss: 9.26843\n",
      "1000: accuracy:0.99 loss: 3.1223207\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9719 test loss: 8.641129\n",
      "1010: accuracy:0.98 loss: 5.551387\n",
      "1020: accuracy:0.97 loss: 10.473793\n",
      "1030: accuracy:0.97 loss: 5.2215233\n",
      "1040: accuracy:0.97 loss: 7.340851\n",
      "1050: accuracy:1.0 loss: 2.990691\n",
      "1050: ********* epoch 2 ********* test accuracy:0.9744 test loss: 8.322142\n",
      "1060: accuracy:0.98 loss: 6.076589\n",
      "1070: accuracy:0.96 loss: 9.909967\n",
      "1080: accuracy:0.97 loss: 7.772636\n",
      "1090: accuracy:0.98 loss: 6.7531805\n",
      "1100: accuracy:0.99 loss: 2.3875809\n",
      "1100: ********* epoch 2 ********* test accuracy:0.9724 test loss: 8.579585\n",
      "1110: accuracy:0.98 loss: 5.731386\n",
      "1120: accuracy:0.98 loss: 7.9481807\n",
      "1130: accuracy:0.96 loss: 11.953233\n",
      "1140: accuracy:0.96 loss: 7.220499\n",
      "1150: accuracy:0.99 loss: 3.4388452\n",
      "1150: ********* epoch 2 ********* test accuracy:0.9734 test loss: 8.604134\n",
      "1160: accuracy:0.98 loss: 4.8902235\n",
      "1170: accuracy:0.96 loss: 9.312958\n",
      "1180: accuracy:0.99 loss: 2.1768417\n",
      "1190: accuracy:0.93 loss: 15.640853\n",
      "1200: accuracy:0.97 loss: 10.576481\n",
      "1200: ********* epoch 3 ********* test accuracy:0.972 test loss: 8.842776\n",
      "1210: accuracy:0.95 loss: 17.281704\n",
      "1220: accuracy:0.98 loss: 4.8128967\n",
      "1230: accuracy:0.95 loss: 9.38193\n",
      "1240: accuracy:0.98 loss: 5.8266954\n",
      "1250: accuracy:0.96 loss: 10.326746\n",
      "1250: ********* epoch 3 ********* test accuracy:0.973 test loss: 8.6132965\n",
      "1260: accuracy:1.0 loss: 1.6369221\n",
      "1270: accuracy:0.97 loss: 9.862279\n",
      "1280: accuracy:0.96 loss: 11.643463\n",
      "1290: accuracy:0.99 loss: 3.2427952\n",
      "1300: accuracy:0.92 loss: 20.871037\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9741 test loss: 9.334828\n",
      "1310: accuracy:0.99 loss: 3.38246\n",
      "1320: accuracy:0.96 loss: 10.125874\n",
      "1330: accuracy:0.97 loss: 5.340234\n",
      "1340: accuracy:1.0 loss: 1.6354618\n",
      "1350: accuracy:0.95 loss: 13.46167\n",
      "1350: ********* epoch 3 ********* test accuracy:0.9715 test loss: 9.1747675\n",
      "1360: accuracy:0.98 loss: 8.414493\n",
      "1370: accuracy:0.98 loss: 4.936502\n",
      "1380: accuracy:0.99 loss: 3.5746098\n",
      "1390: accuracy:0.95 loss: 21.079714\n",
      "1400: accuracy:0.98 loss: 5.893667\n",
      "1400: ********* epoch 3 ********* test accuracy:0.9744 test loss: 8.215698\n",
      "1410: accuracy:0.99 loss: 2.599114\n",
      "1420: accuracy:0.98 loss: 2.7193074\n",
      "1430: accuracy:0.97 loss: 12.923677\n",
      "1440: accuracy:0.95 loss: 12.48518\n",
      "1450: accuracy:0.97 loss: 5.4391074\n",
      "1450: ********* epoch 3 ********* test accuracy:0.9671 test loss: 10.803344\n",
      "1460: accuracy:0.99 loss: 3.2798529\n",
      "1470: accuracy:0.98 loss: 3.9339988\n",
      "1480: accuracy:0.98 loss: 4.1519594\n",
      "1490: accuracy:0.97 loss: 8.098542\n",
      "1500: accuracy:0.97 loss: 7.169133\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9717 test loss: 9.729636\n",
      "1510: accuracy:0.97 loss: 10.422459\n",
      "1520: accuracy:0.99 loss: 6.3247123\n",
      "1530: accuracy:0.95 loss: 11.039836\n",
      "1540: accuracy:0.97 loss: 6.4919825\n",
      "1550: accuracy:0.97 loss: 6.3021355\n",
      "1550: ********* epoch 3 ********* test accuracy:0.9773 test loss: 7.45139\n",
      "1560: accuracy:0.98 loss: 4.5640144\n",
      "1570: accuracy:1.0 loss: 1.5439777\n",
      "1580: accuracy:0.98 loss: 6.834348\n",
      "1590: accuracy:0.99 loss: 3.950444\n",
      "1600: accuracy:0.97 loss: 6.9266143\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9791 test loss: 7.106573\n",
      "1610: accuracy:1.0 loss: 1.2082834\n",
      "1620: accuracy:0.99 loss: 3.081018\n",
      "1630: accuracy:0.99 loss: 2.728375\n",
      "1640: accuracy:0.97 loss: 9.090069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1650: accuracy:0.99 loss: 7.004283\n",
      "1650: ********* epoch 3 ********* test accuracy:0.9714 test loss: 9.704849\n",
      "1660: accuracy:0.97 loss: 9.485522\n",
      "1670: accuracy:0.98 loss: 4.5669045\n",
      "1680: accuracy:0.99 loss: 4.2464676\n",
      "1690: accuracy:0.98 loss: 4.4260793\n",
      "1700: accuracy:1.0 loss: 1.6195989\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9741 test loss: 8.014882\n",
      "1710: accuracy:0.99 loss: 1.9380758\n",
      "1720: accuracy:0.97 loss: 7.663666\n",
      "1730: accuracy:0.99 loss: 1.771312\n",
      "1740: accuracy:0.97 loss: 5.260825\n",
      "1750: accuracy:0.97 loss: 10.039341\n",
      "1750: ********* epoch 3 ********* test accuracy:0.9751 test loss: 8.461765\n",
      "1760: accuracy:0.96 loss: 9.032226\n",
      "1770: accuracy:0.97 loss: 6.3103166\n",
      "1780: accuracy:0.99 loss: 7.568252\n",
      "1790: accuracy:0.99 loss: 4.443781\n",
      "1800: accuracy:0.97 loss: 11.533041\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9778 test loss: 7.5744166\n",
      "1810: accuracy:0.99 loss: 1.9122939\n",
      "1820: accuracy:0.98 loss: 4.288896\n",
      "1830: accuracy:1.0 loss: 1.3422307\n",
      "1840: accuracy:0.98 loss: 3.7580247\n",
      "1850: accuracy:1.0 loss: 3.34383\n",
      "1850: ********* epoch 4 ********* test accuracy:0.9747 test loss: 8.824965\n",
      "1860: accuracy:0.97 loss: 6.471836\n",
      "1870: accuracy:0.99 loss: 3.9265404\n",
      "1880: accuracy:0.98 loss: 6.9691453\n",
      "1890: accuracy:0.98 loss: 9.780688\n",
      "1900: accuracy:0.96 loss: 10.46395\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9753 test loss: 8.743295\n",
      "1910: accuracy:1.0 loss: 1.8835341\n",
      "1920: accuracy:0.98 loss: 14.663385\n",
      "1930: accuracy:0.99 loss: 3.8891416\n",
      "1940: accuracy:0.99 loss: 2.9010558\n",
      "1950: accuracy:0.96 loss: 13.695089\n",
      "1950: ********* epoch 4 ********* test accuracy:0.9735 test loss: 8.915666\n",
      "1960: accuracy:0.99 loss: 2.178608\n",
      "1970: accuracy:0.99 loss: 2.4158556\n",
      "1980: accuracy:0.98 loss: 4.174187\n",
      "1990: accuracy:0.98 loss: 10.470221\n",
      "2000: accuracy:0.96 loss: 8.063223\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9733 test loss: 8.766186\n",
      "2010: accuracy:1.0 loss: 0.48599157\n",
      "2020: accuracy:0.98 loss: 3.7875757\n",
      "2030: accuracy:0.98 loss: 6.093294\n",
      "2040: accuracy:0.98 loss: 3.9432416\n",
      "2050: accuracy:0.97 loss: 6.1817884\n",
      "2050: ********* epoch 4 ********* test accuracy:0.9721 test loss: 9.263921\n",
      "2060: accuracy:0.98 loss: 8.934747\n",
      "2070: accuracy:0.99 loss: 4.219102\n",
      "2080: accuracy:0.98 loss: 6.7331753\n",
      "2090: accuracy:0.98 loss: 5.2666025\n",
      "2100: accuracy:0.98 loss: 4.7944508\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9789 test loss: 7.062166\n",
      "2110: accuracy:0.98 loss: 3.944503\n",
      "2120: accuracy:0.98 loss: 4.3114243\n",
      "2130: accuracy:0.98 loss: 8.656577\n",
      "2140: accuracy:0.98 loss: 3.5180223\n",
      "2150: accuracy:0.97 loss: 7.6437483\n",
      "2150: ********* epoch 4 ********* test accuracy:0.9777 test loss: 7.6346636\n",
      "2160: accuracy:0.99 loss: 1.6018529\n",
      "2170: accuracy:0.99 loss: 2.4176428\n",
      "2180: accuracy:0.98 loss: 5.491462\n",
      "2190: accuracy:1.0 loss: 1.4020838\n",
      "2200: accuracy:0.98 loss: 4.493549\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9782 test loss: 7.528227\n",
      "2210: accuracy:1.0 loss: 2.6752846\n",
      "2220: accuracy:0.99 loss: 3.025304\n",
      "2230: accuracy:0.98 loss: 4.449855\n",
      "2240: accuracy:1.0 loss: 0.7166629\n",
      "2250: accuracy:0.98 loss: 7.5573707\n",
      "2250: ********* epoch 4 ********* test accuracy:0.9795 test loss: 7.3149514\n",
      "2260: accuracy:0.97 loss: 8.862859\n",
      "2270: accuracy:1.0 loss: 0.6245043\n",
      "2280: accuracy:0.99 loss: 2.0739152\n",
      "2290: accuracy:0.97 loss: 6.291245\n",
      "2300: accuracy:0.99 loss: 3.4470906\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9793 test loss: 7.4726577\n",
      "2310: accuracy:0.97 loss: 5.270359\n",
      "2320: accuracy:0.99 loss: 6.6586633\n",
      "2330: accuracy:1.0 loss: 1.6217909\n",
      "2340: accuracy:0.99 loss: 3.4564674\n",
      "2350: accuracy:0.99 loss: 2.0360746\n",
      "2350: ********* epoch 4 ********* test accuracy:0.9775 test loss: 8.359004\n",
      "2360: accuracy:1.0 loss: 1.0906162\n",
      "2370: accuracy:0.99 loss: 1.8994007\n",
      "2380: accuracy:1.0 loss: 0.5347545\n",
      "2390: accuracy:1.0 loss: 3.0996087\n",
      "2400: accuracy:1.0 loss: 0.86143935\n",
      "2400: ********* epoch 5 ********* test accuracy:0.9791 test loss: 7.155521\n",
      "2410: accuracy:1.0 loss: 1.0895166\n",
      "2420: accuracy:0.98 loss: 4.3806005\n",
      "2430: accuracy:0.99 loss: 3.1282747\n",
      "2440: accuracy:0.99 loss: 1.6332004\n",
      "2450: accuracy:0.98 loss: 3.335907\n",
      "2450: ********* epoch 5 ********* test accuracy:0.9766 test loss: 8.181661\n",
      "2460: accuracy:0.99 loss: 2.5254493\n",
      "2470: accuracy:0.96 loss: 15.216865\n",
      "2480: accuracy:0.98 loss: 4.979257\n",
      "2490: accuracy:0.95 loss: 9.221636\n",
      "2500: accuracy:1.0 loss: 1.2375115\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9753 test loss: 8.964697\n",
      "2510: accuracy:0.99 loss: 4.229733\n",
      "2520: accuracy:0.98 loss: 3.0520434\n",
      "2530: accuracy:0.97 loss: 5.7461205\n",
      "2540: accuracy:1.0 loss: 1.4715626\n",
      "2550: accuracy:0.98 loss: 7.643448\n",
      "2550: ********* epoch 5 ********* test accuracy:0.9736 test loss: 9.469082\n",
      "2560: accuracy:0.97 loss: 5.613385\n",
      "2570: accuracy:1.0 loss: 1.0617012\n",
      "2580: accuracy:1.0 loss: 1.5584615\n",
      "2590: accuracy:1.0 loss: 1.014184\n",
      "2600: accuracy:0.98 loss: 10.956704\n",
      "2600: ********* epoch 5 ********* test accuracy:0.977 test loss: 8.21521\n",
      "2610: accuracy:1.0 loss: 0.5252824\n",
      "2620: accuracy:0.99 loss: 4.1034884\n",
      "2630: accuracy:1.0 loss: 0.37358993\n",
      "2640: accuracy:0.99 loss: 2.102398\n",
      "2650: accuracy:0.99 loss: 1.9294494\n",
      "2650: ********* epoch 5 ********* test accuracy:0.975 test loss: 8.141385\n",
      "2660: accuracy:1.0 loss: 0.64550966\n",
      "2670: accuracy:0.98 loss: 5.303579\n",
      "2680: accuracy:0.99 loss: 2.1369908\n",
      "2690: accuracy:0.99 loss: 2.7034042\n",
      "2700: accuracy:0.99 loss: 5.810767\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9752 test loss: 8.759994\n",
      "2710: accuracy:0.97 loss: 9.685465\n",
      "2720: accuracy:1.0 loss: 2.1562326\n",
      "2730: accuracy:1.0 loss: 0.8366785\n",
      "2740: accuracy:0.98 loss: 4.2546535\n",
      "2750: accuracy:0.98 loss: 6.3445415\n",
      "2750: ********* epoch 5 ********* test accuracy:0.972 test loss: 9.667767\n",
      "2760: accuracy:0.99 loss: 1.540523\n",
      "2770: accuracy:0.97 loss: 8.501125\n",
      "2780: accuracy:1.0 loss: 1.7935618\n",
      "2790: accuracy:1.0 loss: 0.46925032\n",
      "2800: accuracy:0.99 loss: 4.495172\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9784 test loss: 7.6626267\n",
      "2810: accuracy:1.0 loss: 0.38191915\n",
      "2820: accuracy:0.99 loss: 1.8970939\n",
      "2830: accuracy:0.98 loss: 3.3919892\n",
      "2840: accuracy:0.99 loss: 2.6802943\n",
      "2850: accuracy:1.0 loss: 0.3252724\n",
      "2850: ********* epoch 5 ********* test accuracy:0.9752 test loss: 8.22467\n",
      "2860: accuracy:0.96 loss: 8.788473\n",
      "2870: accuracy:1.0 loss: 0.63554597\n",
      "2880: accuracy:0.99 loss: 3.529063\n",
      "2890: accuracy:0.99 loss: 2.2635212\n",
      "2900: accuracy:0.99 loss: 3.2163107\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9783 test loss: 7.5862346\n",
      "2910: accuracy:1.0 loss: 1.7542617\n",
      "2920: accuracy:1.0 loss: 0.3737519\n",
      "2930: accuracy:0.98 loss: 4.613696\n",
      "2940: accuracy:1.0 loss: 1.1492529\n",
      "2950: accuracy:1.0 loss: 0.5930183\n",
      "2950: ********* epoch 5 ********* test accuracy:0.9779 test loss: 8.790131\n",
      "2960: accuracy:0.99 loss: 4.8493767\n",
      "2970: accuracy:0.97 loss: 7.9127736\n",
      "2980: accuracy:0.99 loss: 1.9476581\n",
      "2990: accuracy:0.98 loss: 4.5976267\n",
      "3000: accuracy:0.99 loss: 2.7029994\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9777 test loss: 8.015176\n",
      "3010: accuracy:0.97 loss: 4.855787\n",
      "3020: accuracy:0.99 loss: 1.975979\n",
      "3030: accuracy:0.98 loss: 4.148152\n",
      "3040: accuracy:0.98 loss: 3.401645\n",
      "3050: accuracy:0.97 loss: 4.6858606\n",
      "3050: ********* epoch 6 ********* test accuracy:0.9795 test loss: 7.4380474\n",
      "3060: accuracy:0.99 loss: 3.3513455\n",
      "3070: accuracy:1.0 loss: 1.6672796\n",
      "3080: accuracy:0.99 loss: 8.892582\n",
      "3090: accuracy:0.97 loss: 7.326352\n",
      "3100: accuracy:0.99 loss: 2.253495\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9781 test loss: 8.253564\n",
      "3110: accuracy:0.98 loss: 5.7294407\n",
      "3120: accuracy:0.99 loss: 1.4065415\n",
      "3130: accuracy:1.0 loss: 0.341024\n",
      "3140: accuracy:0.98 loss: 5.1496973\n",
      "3150: accuracy:0.99 loss: 3.1796062\n",
      "3150: ********* epoch 6 ********* test accuracy:0.978 test loss: 8.891041\n",
      "3160: accuracy:0.99 loss: 2.7447288\n",
      "3170: accuracy:0.99 loss: 3.5291061\n",
      "3180: accuracy:0.99 loss: 1.2617738\n",
      "3190: accuracy:1.0 loss: 1.1680131\n",
      "3200: accuracy:0.98 loss: 6.4614296\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9762 test loss: 9.328871\n",
      "3210: accuracy:0.97 loss: 12.452595\n",
      "3220: accuracy:0.98 loss: 11.633116\n",
      "3230: accuracy:0.95 loss: 8.38183\n",
      "3240: accuracy:1.0 loss: 0.7945337\n",
      "3250: accuracy:0.99 loss: 2.2589617\n",
      "3250: ********* epoch 6 ********* test accuracy:0.9743 test loss: 9.948131\n",
      "3260: accuracy:1.0 loss: 0.47471017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3270: accuracy:0.99 loss: 1.3679932\n",
      "3280: accuracy:0.99 loss: 2.7335682\n",
      "3290: accuracy:0.98 loss: 6.315503\n",
      "3300: accuracy:0.98 loss: 9.298469\n",
      "3300: ********* epoch 6 ********* test accuracy:0.9752 test loss: 9.133364\n",
      "3310: accuracy:0.97 loss: 6.3731585\n",
      "3320: accuracy:1.0 loss: 0.6320039\n",
      "3330: accuracy:0.98 loss: 5.254186\n",
      "3340: accuracy:0.98 loss: 11.7253475\n",
      "3350: accuracy:0.95 loss: 16.604034\n",
      "3350: ********* epoch 6 ********* test accuracy:0.9779 test loss: 8.575542\n",
      "3360: accuracy:0.99 loss: 1.7549914\n",
      "3370: accuracy:0.97 loss: 7.069659\n",
      "3380: accuracy:1.0 loss: 0.27046153\n",
      "3390: accuracy:1.0 loss: 3.379407\n",
      "3400: accuracy:1.0 loss: 0.16767256\n",
      "3400: ********* epoch 6 ********* test accuracy:0.9767 test loss: 8.490525\n",
      "3410: accuracy:1.0 loss: 0.35378432\n",
      "3420: accuracy:1.0 loss: 0.26230046\n",
      "3430: accuracy:0.98 loss: 4.512098\n",
      "3440: accuracy:0.99 loss: 1.468622\n",
      "3450: accuracy:0.99 loss: 2.0856283\n",
      "3450: ********* epoch 6 ********* test accuracy:0.9784 test loss: 8.257659\n",
      "3460: accuracy:0.98 loss: 13.497297\n",
      "3470: accuracy:1.0 loss: 0.84140885\n",
      "3480: accuracy:0.98 loss: 12.028062\n",
      "3490: accuracy:0.98 loss: 4.2593474\n",
      "3500: accuracy:1.0 loss: 0.48627332\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9756 test loss: 9.642\n",
      "3510: accuracy:0.99 loss: 5.611808\n",
      "3520: accuracy:0.99 loss: 1.8441246\n",
      "3530: accuracy:1.0 loss: 1.0627096\n",
      "3540: accuracy:0.99 loss: 3.7659516\n",
      "3550: accuracy:0.99 loss: 1.4496977\n",
      "3550: ********* epoch 6 ********* test accuracy:0.9767 test loss: 9.093552\n",
      "3560: accuracy:1.0 loss: 0.92265654\n",
      "3570: accuracy:1.0 loss: 0.43526536\n",
      "3580: accuracy:0.98 loss: 2.3554144\n",
      "3590: accuracy:1.0 loss: 0.595508\n",
      "3600: accuracy:0.99 loss: 7.9237947\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9799 test loss: 7.4340515\n",
      "3610: accuracy:0.99 loss: 1.278048\n",
      "3620: accuracy:1.0 loss: 0.626878\n",
      "3630: accuracy:0.99 loss: 1.9450502\n",
      "3640: accuracy:0.99 loss: 4.615146\n",
      "3650: accuracy:0.98 loss: 7.0271006\n",
      "3650: ********* epoch 7 ********* test accuracy:0.974 test loss: nan\n",
      "3660: accuracy:1.0 loss: 0.036810774\n",
      "3670: accuracy:1.0 loss: 0.29671252\n",
      "3680: accuracy:0.97 loss: 6.9434185\n",
      "3690: accuracy:0.99 loss: 3.670058\n",
      "3700: accuracy:1.0 loss: 0.111839406\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9756 test loss: nan\n",
      "3710: accuracy:1.0 loss: 1.9402748\n",
      "3720: accuracy:0.12 loss: nan\n",
      "3730: accuracy:0.13 loss: nan\n",
      "3740: accuracy:0.09 loss: nan\n",
      "3750: accuracy:0.08 loss: nan\n",
      "3750: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "3760: accuracy:0.13 loss: nan\n",
      "3770: accuracy:0.15 loss: nan\n",
      "3780: accuracy:0.14 loss: nan\n",
      "3790: accuracy:0.07 loss: nan\n",
      "3800: accuracy:0.14 loss: nan\n",
      "3800: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "3810: accuracy:0.12 loss: nan\n",
      "3820: accuracy:0.09 loss: nan\n",
      "3830: accuracy:0.09 loss: nan\n",
      "3840: accuracy:0.1 loss: nan\n",
      "3850: accuracy:0.09 loss: nan\n",
      "3850: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "3860: accuracy:0.09 loss: nan\n",
      "3870: accuracy:0.08 loss: nan\n",
      "3880: accuracy:0.12 loss: nan\n",
      "3890: accuracy:0.08 loss: nan\n",
      "3900: accuracy:0.11 loss: nan\n",
      "3900: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "3910: accuracy:0.14 loss: nan\n",
      "3920: accuracy:0.06 loss: nan\n",
      "3930: accuracy:0.1 loss: nan\n",
      "3940: accuracy:0.1 loss: nan\n",
      "3950: accuracy:0.08 loss: nan\n",
      "3950: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "3960: accuracy:0.08 loss: nan\n",
      "3970: accuracy:0.14 loss: nan\n",
      "3980: accuracy:0.09 loss: nan\n",
      "3990: accuracy:0.09 loss: nan\n",
      "4000: accuracy:0.09 loss: nan\n",
      "4000: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "4010: accuracy:0.06 loss: nan\n",
      "4020: accuracy:0.1 loss: nan\n",
      "4030: accuracy:0.11 loss: nan\n",
      "4040: accuracy:0.1 loss: nan\n",
      "4050: accuracy:0.08 loss: nan\n",
      "4050: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "4060: accuracy:0.07 loss: nan\n",
      "4070: accuracy:0.09 loss: nan\n",
      "4080: accuracy:0.15 loss: nan\n",
      "4090: accuracy:0.07 loss: nan\n",
      "4100: accuracy:0.11 loss: nan\n",
      "4100: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "4110: accuracy:0.06 loss: nan\n",
      "4120: accuracy:0.06 loss: nan\n",
      "4130: accuracy:0.08 loss: nan\n",
      "4140: accuracy:0.09 loss: nan\n",
      "4150: accuracy:0.05 loss: nan\n",
      "4150: ********* epoch 7 ********* test accuracy:0.098 test loss: nan\n",
      "4160: accuracy:0.08 loss: nan\n",
      "4170: accuracy:0.08 loss: nan\n",
      "4180: accuracy:0.1 loss: nan\n",
      "4190: accuracy:0.05 loss: nan\n",
      "4200: accuracy:0.09 loss: nan\n",
      "4200: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4210: accuracy:0.14 loss: nan\n",
      "4220: accuracy:0.13 loss: nan\n",
      "4230: accuracy:0.05 loss: nan\n",
      "4240: accuracy:0.1 loss: nan\n",
      "4250: accuracy:0.11 loss: nan\n",
      "4250: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4260: accuracy:0.09 loss: nan\n",
      "4270: accuracy:0.07 loss: nan\n",
      "4280: accuracy:0.07 loss: nan\n",
      "4290: accuracy:0.09 loss: nan\n",
      "4300: accuracy:0.14 loss: nan\n",
      "4300: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4310: accuracy:0.07 loss: nan\n",
      "4320: accuracy:0.16 loss: nan\n",
      "4330: accuracy:0.09 loss: nan\n",
      "4340: accuracy:0.09 loss: nan\n",
      "4350: accuracy:0.12 loss: nan\n",
      "4350: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4360: accuracy:0.07 loss: nan\n",
      "4370: accuracy:0.08 loss: nan\n",
      "4380: accuracy:0.09 loss: nan\n",
      "4390: accuracy:0.18 loss: nan\n",
      "4400: accuracy:0.09 loss: nan\n",
      "4400: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4410: accuracy:0.14 loss: nan\n",
      "4420: accuracy:0.06 loss: nan\n",
      "4430: accuracy:0.16 loss: nan\n",
      "4440: accuracy:0.06 loss: nan\n",
      "4450: accuracy:0.13 loss: nan\n",
      "4450: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4460: accuracy:0.11 loss: nan\n",
      "4470: accuracy:0.09 loss: nan\n",
      "4480: accuracy:0.06 loss: nan\n",
      "4490: accuracy:0.06 loss: nan\n",
      "4500: accuracy:0.1 loss: nan\n",
      "4500: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4510: accuracy:0.11 loss: nan\n",
      "4520: accuracy:0.1 loss: nan\n",
      "4530: accuracy:0.1 loss: nan\n",
      "4540: accuracy:0.11 loss: nan\n",
      "4550: accuracy:0.11 loss: nan\n",
      "4550: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4560: accuracy:0.09 loss: nan\n",
      "4570: accuracy:0.12 loss: nan\n",
      "4580: accuracy:0.13 loss: nan\n",
      "4590: accuracy:0.07 loss: nan\n",
      "4600: accuracy:0.09 loss: nan\n",
      "4600: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4610: accuracy:0.14 loss: nan\n",
      "4620: accuracy:0.12 loss: nan\n",
      "4630: accuracy:0.09 loss: nan\n",
      "4640: accuracy:0.09 loss: nan\n",
      "4650: accuracy:0.07 loss: nan\n",
      "4650: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4660: accuracy:0.12 loss: nan\n",
      "4670: accuracy:0.08 loss: nan\n",
      "4680: accuracy:0.1 loss: nan\n",
      "4690: accuracy:0.07 loss: nan\n",
      "4700: accuracy:0.07 loss: nan\n",
      "4700: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4710: accuracy:0.1 loss: nan\n",
      "4720: accuracy:0.08 loss: nan\n",
      "4730: accuracy:0.12 loss: nan\n",
      "4740: accuracy:0.08 loss: nan\n",
      "4750: accuracy:0.07 loss: nan\n",
      "4750: ********* epoch 8 ********* test accuracy:0.098 test loss: nan\n",
      "4760: accuracy:0.07 loss: nan\n",
      "4770: accuracy:0.1 loss: nan\n",
      "4780: accuracy:0.07 loss: nan\n",
      "4790: accuracy:0.08 loss: nan\n",
      "4800: accuracy:0.06 loss: nan\n",
      "4800: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "4810: accuracy:0.09 loss: nan\n",
      "4820: accuracy:0.12 loss: nan\n",
      "4830: accuracy:0.14 loss: nan\n",
      "4840: accuracy:0.16 loss: nan\n",
      "4850: accuracy:0.08 loss: nan\n",
      "4850: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "4860: accuracy:0.13 loss: nan\n",
      "4870: accuracy:0.07 loss: nan\n",
      "4880: accuracy:0.09 loss: nan\n",
      "4890: accuracy:0.11 loss: nan\n",
      "4900: accuracy:0.12 loss: nan\n",
      "4900: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "4910: accuracy:0.09 loss: nan\n",
      "4920: accuracy:0.11 loss: nan\n",
      "4930: accuracy:0.08 loss: nan\n",
      "4940: accuracy:0.08 loss: nan\n",
      "4950: accuracy:0.08 loss: nan\n",
      "4950: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "4960: accuracy:0.06 loss: nan\n",
      "4970: accuracy:0.11 loss: nan\n",
      "4980: accuracy:0.12 loss: nan\n",
      "4990: accuracy:0.08 loss: nan\n",
      "5000: accuracy:0.14 loss: nan\n",
      "5000: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5010: accuracy:0.08 loss: nan\n",
      "5020: accuracy:0.08 loss: nan\n",
      "5030: accuracy:0.04 loss: nan\n",
      "5040: accuracy:0.11 loss: nan\n",
      "5050: accuracy:0.12 loss: nan\n",
      "5050: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5060: accuracy:0.13 loss: nan\n",
      "5070: accuracy:0.09 loss: nan\n",
      "5080: accuracy:0.06 loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5090: accuracy:0.12 loss: nan\n",
      "5100: accuracy:0.08 loss: nan\n",
      "5100: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5110: accuracy:0.1 loss: nan\n",
      "5120: accuracy:0.15 loss: nan\n",
      "5130: accuracy:0.13 loss: nan\n",
      "5140: accuracy:0.09 loss: nan\n",
      "5150: accuracy:0.09 loss: nan\n",
      "5150: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5160: accuracy:0.08 loss: nan\n",
      "5170: accuracy:0.07 loss: nan\n",
      "5180: accuracy:0.09 loss: nan\n",
      "5190: accuracy:0.15 loss: nan\n",
      "5200: accuracy:0.1 loss: nan\n",
      "5200: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5210: accuracy:0.1 loss: nan\n",
      "5220: accuracy:0.17 loss: nan\n",
      "5230: accuracy:0.08 loss: nan\n",
      "5240: accuracy:0.07 loss: nan\n",
      "5250: accuracy:0.06 loss: nan\n",
      "5250: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5260: accuracy:0.1 loss: nan\n",
      "5270: accuracy:0.12 loss: nan\n",
      "5280: accuracy:0.11 loss: nan\n",
      "5290: accuracy:0.1 loss: nan\n",
      "5300: accuracy:0.09 loss: nan\n",
      "5300: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5310: accuracy:0.11 loss: nan\n",
      "5320: accuracy:0.09 loss: nan\n",
      "5330: accuracy:0.09 loss: nan\n",
      "5340: accuracy:0.12 loss: nan\n",
      "5350: accuracy:0.13 loss: nan\n",
      "5350: ********* epoch 9 ********* test accuracy:0.098 test loss: nan\n",
      "5360: accuracy:0.05 loss: nan\n",
      "5370: accuracy:0.07 loss: nan\n",
      "5380: accuracy:0.09 loss: nan\n",
      "5390: accuracy:0.13 loss: nan\n",
      "5400: accuracy:0.06 loss: nan\n",
      "5400: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5410: accuracy:0.06 loss: nan\n",
      "5420: accuracy:0.09 loss: nan\n",
      "5430: accuracy:0.11 loss: nan\n",
      "5440: accuracy:0.08 loss: nan\n",
      "5450: accuracy:0.1 loss: nan\n",
      "5450: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5460: accuracy:0.12 loss: nan\n",
      "5470: accuracy:0.15 loss: nan\n",
      "5480: accuracy:0.11 loss: nan\n",
      "5490: accuracy:0.16 loss: nan\n",
      "5500: accuracy:0.06 loss: nan\n",
      "5500: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5510: accuracy:0.08 loss: nan\n",
      "5520: accuracy:0.14 loss: nan\n",
      "5530: accuracy:0.12 loss: nan\n",
      "5540: accuracy:0.09 loss: nan\n",
      "5550: accuracy:0.06 loss: nan\n",
      "5550: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5560: accuracy:0.12 loss: nan\n",
      "5570: accuracy:0.11 loss: nan\n",
      "5580: accuracy:0.13 loss: nan\n",
      "5590: accuracy:0.07 loss: nan\n",
      "5600: accuracy:0.08 loss: nan\n",
      "5600: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5610: accuracy:0.18 loss: nan\n",
      "5620: accuracy:0.06 loss: nan\n",
      "5630: accuracy:0.09 loss: nan\n",
      "5640: accuracy:0.12 loss: nan\n",
      "5650: accuracy:0.08 loss: nan\n",
      "5650: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5660: accuracy:0.07 loss: nan\n",
      "5670: accuracy:0.07 loss: nan\n",
      "5680: accuracy:0.1 loss: nan\n",
      "5690: accuracy:0.07 loss: nan\n",
      "5700: accuracy:0.13 loss: nan\n",
      "5700: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5710: accuracy:0.08 loss: nan\n",
      "5720: accuracy:0.16 loss: nan\n",
      "5730: accuracy:0.11 loss: nan\n",
      "5740: accuracy:0.1 loss: nan\n",
      "5750: accuracy:0.15 loss: nan\n",
      "5750: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5760: accuracy:0.09 loss: nan\n",
      "5770: accuracy:0.1 loss: nan\n",
      "5780: accuracy:0.15 loss: nan\n",
      "5790: accuracy:0.08 loss: nan\n",
      "5800: accuracy:0.08 loss: nan\n",
      "5800: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5810: accuracy:0.06 loss: nan\n",
      "5820: accuracy:0.11 loss: nan\n",
      "5830: accuracy:0.11 loss: nan\n",
      "5840: accuracy:0.13 loss: nan\n",
      "5850: accuracy:0.05 loss: nan\n",
      "5850: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5860: accuracy:0.1 loss: nan\n",
      "5870: accuracy:0.12 loss: nan\n",
      "5880: accuracy:0.11 loss: nan\n",
      "5890: accuracy:0.11 loss: nan\n",
      "5900: accuracy:0.15 loss: nan\n",
      "5900: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5910: accuracy:0.12 loss: nan\n",
      "5920: accuracy:0.05 loss: nan\n",
      "5930: accuracy:0.13 loss: nan\n",
      "5940: accuracy:0.09 loss: nan\n",
      "5950: accuracy:0.12 loss: nan\n",
      "5950: ********* epoch 10 ********* test accuracy:0.098 test loss: nan\n",
      "5960: accuracy:0.09 loss: nan\n",
      "5970: accuracy:0.11 loss: nan\n",
      "5980: accuracy:0.13 loss: nan\n",
      "5990: accuracy:0.14 loss: nan\n",
      "6000: accuracy:0.09 loss: nan\n",
      "6000: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6010: accuracy:0.09 loss: nan\n",
      "6020: accuracy:0.09 loss: nan\n",
      "6030: accuracy:0.11 loss: nan\n",
      "6040: accuracy:0.1 loss: nan\n",
      "6050: accuracy:0.13 loss: nan\n",
      "6050: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6060: accuracy:0.08 loss: nan\n",
      "6070: accuracy:0.06 loss: nan\n",
      "6080: accuracy:0.14 loss: nan\n",
      "6090: accuracy:0.08 loss: nan\n",
      "6100: accuracy:0.07 loss: nan\n",
      "6100: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6110: accuracy:0.1 loss: nan\n",
      "6120: accuracy:0.11 loss: nan\n",
      "6130: accuracy:0.09 loss: nan\n",
      "6140: accuracy:0.11 loss: nan\n",
      "6150: accuracy:0.12 loss: nan\n",
      "6150: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6160: accuracy:0.11 loss: nan\n",
      "6170: accuracy:0.12 loss: nan\n",
      "6180: accuracy:0.07 loss: nan\n",
      "6190: accuracy:0.11 loss: nan\n",
      "6200: accuracy:0.1 loss: nan\n",
      "6200: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6210: accuracy:0.16 loss: nan\n",
      "6220: accuracy:0.14 loss: nan\n",
      "6230: accuracy:0.12 loss: nan\n",
      "6240: accuracy:0.1 loss: nan\n",
      "6250: accuracy:0.07 loss: nan\n",
      "6250: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6260: accuracy:0.11 loss: nan\n",
      "6270: accuracy:0.06 loss: nan\n",
      "6280: accuracy:0.12 loss: nan\n",
      "6290: accuracy:0.16 loss: nan\n",
      "6300: accuracy:0.09 loss: nan\n",
      "6300: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6310: accuracy:0.1 loss: nan\n",
      "6320: accuracy:0.11 loss: nan\n",
      "6330: accuracy:0.08 loss: nan\n",
      "6340: accuracy:0.06 loss: nan\n",
      "6350: accuracy:0.13 loss: nan\n",
      "6350: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6360: accuracy:0.11 loss: nan\n",
      "6370: accuracy:0.13 loss: nan\n",
      "6380: accuracy:0.11 loss: nan\n",
      "6390: accuracy:0.12 loss: nan\n",
      "6400: accuracy:0.06 loss: nan\n",
      "6400: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6410: accuracy:0.12 loss: nan\n",
      "6420: accuracy:0.13 loss: nan\n",
      "6430: accuracy:0.1 loss: nan\n",
      "6440: accuracy:0.09 loss: nan\n",
      "6450: accuracy:0.1 loss: nan\n",
      "6450: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6460: accuracy:0.15 loss: nan\n",
      "6470: accuracy:0.1 loss: nan\n",
      "6480: accuracy:0.12 loss: nan\n",
      "6490: accuracy:0.11 loss: nan\n",
      "6500: accuracy:0.1 loss: nan\n",
      "6500: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6510: accuracy:0.13 loss: nan\n",
      "6520: accuracy:0.09 loss: nan\n",
      "6530: accuracy:0.09 loss: nan\n",
      "6540: accuracy:0.08 loss: nan\n",
      "6550: accuracy:0.15 loss: nan\n",
      "6550: ********* epoch 11 ********* test accuracy:0.098 test loss: nan\n",
      "6560: accuracy:0.12 loss: nan\n",
      "6570: accuracy:0.11 loss: nan\n",
      "6580: accuracy:0.07 loss: nan\n",
      "6590: accuracy:0.16 loss: nan\n",
      "6600: accuracy:0.09 loss: nan\n",
      "6600: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6610: accuracy:0.09 loss: nan\n",
      "6620: accuracy:0.05 loss: nan\n",
      "6630: accuracy:0.04 loss: nan\n",
      "6640: accuracy:0.1 loss: nan\n",
      "6650: accuracy:0.14 loss: nan\n",
      "6650: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6660: accuracy:0.04 loss: nan\n",
      "6670: accuracy:0.07 loss: nan\n",
      "6680: accuracy:0.18 loss: nan\n",
      "6690: accuracy:0.12 loss: nan\n",
      "6700: accuracy:0.09 loss: nan\n",
      "6700: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6710: accuracy:0.15 loss: nan\n",
      "6720: accuracy:0.11 loss: nan\n",
      "6730: accuracy:0.11 loss: nan\n",
      "6740: accuracy:0.08 loss: nan\n",
      "6750: accuracy:0.07 loss: nan\n",
      "6750: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6760: accuracy:0.07 loss: nan\n",
      "6770: accuracy:0.05 loss: nan\n",
      "6780: accuracy:0.09 loss: nan\n",
      "6790: accuracy:0.08 loss: nan\n",
      "6800: accuracy:0.11 loss: nan\n",
      "6800: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6810: accuracy:0.13 loss: nan\n",
      "6820: accuracy:0.12 loss: nan\n",
      "6830: accuracy:0.06 loss: nan\n",
      "6840: accuracy:0.11 loss: nan\n",
      "6850: accuracy:0.11 loss: nan\n",
      "6850: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6860: accuracy:0.08 loss: nan\n",
      "6870: accuracy:0.12 loss: nan\n",
      "6880: accuracy:0.06 loss: nan\n",
      "6890: accuracy:0.11 loss: nan\n",
      "6900: accuracy:0.06 loss: nan\n",
      "6900: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6910: accuracy:0.17 loss: nan\n",
      "6920: accuracy:0.09 loss: nan\n",
      "6930: accuracy:0.12 loss: nan\n",
      "6940: accuracy:0.09 loss: nan\n",
      "6950: accuracy:0.09 loss: nan\n",
      "6950: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "6960: accuracy:0.11 loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6970: accuracy:0.13 loss: nan\n",
      "6980: accuracy:0.13 loss: nan\n",
      "6990: accuracy:0.07 loss: nan\n",
      "7000: accuracy:0.1 loss: nan\n",
      "7000: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "7010: accuracy:0.08 loss: nan\n",
      "7020: accuracy:0.1 loss: nan\n",
      "7030: accuracy:0.08 loss: nan\n",
      "7040: accuracy:0.08 loss: nan\n",
      "7050: accuracy:0.1 loss: nan\n",
      "7050: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "7060: accuracy:0.11 loss: nan\n",
      "7070: accuracy:0.09 loss: nan\n",
      "7080: accuracy:0.12 loss: nan\n",
      "7090: accuracy:0.13 loss: nan\n",
      "7100: accuracy:0.11 loss: nan\n",
      "7100: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "7110: accuracy:0.07 loss: nan\n",
      "7120: accuracy:0.09 loss: nan\n",
      "7130: accuracy:0.15 loss: nan\n",
      "7140: accuracy:0.13 loss: nan\n",
      "7150: accuracy:0.12 loss: nan\n",
      "7150: ********* epoch 12 ********* test accuracy:0.098 test loss: nan\n",
      "7160: accuracy:0.08 loss: nan\n",
      "7170: accuracy:0.09 loss: nan\n",
      "7180: accuracy:0.13 loss: nan\n",
      "7190: accuracy:0.09 loss: nan\n",
      "7200: accuracy:0.11 loss: nan\n",
      "7200: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7210: accuracy:0.08 loss: nan\n",
      "7220: accuracy:0.13 loss: nan\n",
      "7230: accuracy:0.1 loss: nan\n",
      "7240: accuracy:0.13 loss: nan\n",
      "7250: accuracy:0.09 loss: nan\n",
      "7250: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7260: accuracy:0.09 loss: nan\n",
      "7270: accuracy:0.14 loss: nan\n",
      "7280: accuracy:0.14 loss: nan\n",
      "7290: accuracy:0.12 loss: nan\n",
      "7300: accuracy:0.12 loss: nan\n",
      "7300: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7310: accuracy:0.08 loss: nan\n",
      "7320: accuracy:0.11 loss: nan\n",
      "7330: accuracy:0.04 loss: nan\n",
      "7340: accuracy:0.11 loss: nan\n",
      "7350: accuracy:0.11 loss: nan\n",
      "7350: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7360: accuracy:0.09 loss: nan\n",
      "7370: accuracy:0.1 loss: nan\n",
      "7380: accuracy:0.12 loss: nan\n",
      "7390: accuracy:0.1 loss: nan\n",
      "7400: accuracy:0.16 loss: nan\n",
      "7400: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7410: accuracy:0.12 loss: nan\n",
      "7420: accuracy:0.1 loss: nan\n",
      "7430: accuracy:0.15 loss: nan\n",
      "7440: accuracy:0.05 loss: nan\n",
      "7450: accuracy:0.08 loss: nan\n",
      "7450: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7460: accuracy:0.11 loss: nan\n",
      "7470: accuracy:0.17 loss: nan\n",
      "7480: accuracy:0.13 loss: nan\n",
      "7490: accuracy:0.13 loss: nan\n",
      "7500: accuracy:0.07 loss: nan\n",
      "7500: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7510: accuracy:0.13 loss: nan\n",
      "7520: accuracy:0.1 loss: nan\n",
      "7530: accuracy:0.11 loss: nan\n",
      "7540: accuracy:0.1 loss: nan\n",
      "7550: accuracy:0.11 loss: nan\n",
      "7550: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7560: accuracy:0.09 loss: nan\n",
      "7570: accuracy:0.07 loss: nan\n",
      "7580: accuracy:0.06 loss: nan\n",
      "7590: accuracy:0.11 loss: nan\n",
      "7600: accuracy:0.1 loss: nan\n",
      "7600: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7610: accuracy:0.13 loss: nan\n",
      "7620: accuracy:0.13 loss: nan\n",
      "7630: accuracy:0.08 loss: nan\n",
      "7640: accuracy:0.06 loss: nan\n",
      "7650: accuracy:0.07 loss: nan\n",
      "7650: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7660: accuracy:0.1 loss: nan\n",
      "7670: accuracy:0.13 loss: nan\n",
      "7680: accuracy:0.07 loss: nan\n",
      "7690: accuracy:0.08 loss: nan\n",
      "7700: accuracy:0.08 loss: nan\n",
      "7700: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7710: accuracy:0.07 loss: nan\n",
      "7720: accuracy:0.13 loss: nan\n",
      "7730: accuracy:0.13 loss: nan\n",
      "7740: accuracy:0.15 loss: nan\n",
      "7750: accuracy:0.08 loss: nan\n",
      "7750: ********* epoch 13 ********* test accuracy:0.098 test loss: nan\n",
      "7760: accuracy:0.11 loss: nan\n",
      "7770: accuracy:0.09 loss: nan\n",
      "7780: accuracy:0.05 loss: nan\n",
      "7790: accuracy:0.07 loss: nan\n",
      "7800: accuracy:0.11 loss: nan\n",
      "7800: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "7810: accuracy:0.12 loss: nan\n",
      "7820: accuracy:0.14 loss: nan\n",
      "7830: accuracy:0.07 loss: nan\n",
      "7840: accuracy:0.08 loss: nan\n",
      "7850: accuracy:0.17 loss: nan\n",
      "7850: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "7860: accuracy:0.17 loss: nan\n",
      "7870: accuracy:0.13 loss: nan\n",
      "7880: accuracy:0.12 loss: nan\n",
      "7890: accuracy:0.11 loss: nan\n",
      "7900: accuracy:0.08 loss: nan\n",
      "7900: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "7910: accuracy:0.07 loss: nan\n",
      "7920: accuracy:0.1 loss: nan\n",
      "7930: accuracy:0.06 loss: nan\n",
      "7940: accuracy:0.13 loss: nan\n",
      "7950: accuracy:0.06 loss: nan\n",
      "7950: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "7960: accuracy:0.04 loss: nan\n",
      "7970: accuracy:0.12 loss: nan\n",
      "7980: accuracy:0.19 loss: nan\n",
      "7990: accuracy:0.08 loss: nan\n",
      "8000: accuracy:0.12 loss: nan\n",
      "8000: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8010: accuracy:0.08 loss: nan\n",
      "8020: accuracy:0.13 loss: nan\n",
      "8030: accuracy:0.13 loss: nan\n",
      "8040: accuracy:0.1 loss: nan\n",
      "8050: accuracy:0.07 loss: nan\n",
      "8050: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8060: accuracy:0.13 loss: nan\n",
      "8070: accuracy:0.08 loss: nan\n",
      "8080: accuracy:0.06 loss: nan\n",
      "8090: accuracy:0.04 loss: nan\n",
      "8100: accuracy:0.09 loss: nan\n",
      "8100: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8110: accuracy:0.12 loss: nan\n",
      "8120: accuracy:0.15 loss: nan\n",
      "8130: accuracy:0.06 loss: nan\n",
      "8140: accuracy:0.11 loss: nan\n",
      "8150: accuracy:0.06 loss: nan\n",
      "8150: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8160: accuracy:0.1 loss: nan\n",
      "8170: accuracy:0.1 loss: nan\n",
      "8180: accuracy:0.14 loss: nan\n",
      "8190: accuracy:0.08 loss: nan\n",
      "8200: accuracy:0.08 loss: nan\n",
      "8200: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8210: accuracy:0.07 loss: nan\n",
      "8220: accuracy:0.15 loss: nan\n",
      "8230: accuracy:0.13 loss: nan\n",
      "8240: accuracy:0.07 loss: nan\n",
      "8250: accuracy:0.14 loss: nan\n",
      "8250: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8260: accuracy:0.05 loss: nan\n",
      "8270: accuracy:0.06 loss: nan\n",
      "8280: accuracy:0.12 loss: nan\n",
      "8290: accuracy:0.11 loss: nan\n",
      "8300: accuracy:0.12 loss: nan\n",
      "8300: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8310: accuracy:0.13 loss: nan\n",
      "8320: accuracy:0.04 loss: nan\n",
      "8330: accuracy:0.06 loss: nan\n",
      "8340: accuracy:0.16 loss: nan\n",
      "8350: accuracy:0.09 loss: nan\n",
      "8350: ********* epoch 14 ********* test accuracy:0.098 test loss: nan\n",
      "8360: accuracy:0.12 loss: nan\n",
      "8370: accuracy:0.08 loss: nan\n",
      "8380: accuracy:0.09 loss: nan\n",
      "8390: accuracy:0.15 loss: nan\n",
      "8400: accuracy:0.04 loss: nan\n",
      "8400: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8410: accuracy:0.05 loss: nan\n",
      "8420: accuracy:0.09 loss: nan\n",
      "8430: accuracy:0.1 loss: nan\n",
      "8440: accuracy:0.08 loss: nan\n",
      "8450: accuracy:0.11 loss: nan\n",
      "8450: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8460: accuracy:0.11 loss: nan\n",
      "8470: accuracy:0.07 loss: nan\n",
      "8480: accuracy:0.1 loss: nan\n",
      "8490: accuracy:0.11 loss: nan\n",
      "8500: accuracy:0.1 loss: nan\n",
      "8500: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8510: accuracy:0.11 loss: nan\n",
      "8520: accuracy:0.14 loss: nan\n",
      "8530: accuracy:0.02 loss: nan\n",
      "8540: accuracy:0.08 loss: nan\n",
      "8550: accuracy:0.1 loss: nan\n",
      "8550: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8560: accuracy:0.1 loss: nan\n",
      "8570: accuracy:0.07 loss: nan\n",
      "8580: accuracy:0.07 loss: nan\n",
      "8590: accuracy:0.08 loss: nan\n",
      "8600: accuracy:0.07 loss: nan\n",
      "8600: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8610: accuracy:0.12 loss: nan\n",
      "8620: accuracy:0.07 loss: nan\n",
      "8630: accuracy:0.14 loss: nan\n",
      "8640: accuracy:0.08 loss: nan\n",
      "8650: accuracy:0.09 loss: nan\n",
      "8650: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8660: accuracy:0.06 loss: nan\n",
      "8670: accuracy:0.12 loss: nan\n",
      "8680: accuracy:0.08 loss: nan\n",
      "8690: accuracy:0.08 loss: nan\n",
      "8700: accuracy:0.09 loss: nan\n",
      "8700: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8710: accuracy:0.09 loss: nan\n",
      "8720: accuracy:0.1 loss: nan\n",
      "8730: accuracy:0.07 loss: nan\n",
      "8740: accuracy:0.11 loss: nan\n",
      "8750: accuracy:0.08 loss: nan\n",
      "8750: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8760: accuracy:0.1 loss: nan\n",
      "8770: accuracy:0.07 loss: nan\n",
      "8780: accuracy:0.17 loss: nan\n",
      "8790: accuracy:0.11 loss: nan\n",
      "8800: accuracy:0.08 loss: nan\n",
      "8800: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8810: accuracy:0.11 loss: nan\n",
      "8820: accuracy:0.14 loss: nan\n",
      "8830: accuracy:0.1 loss: nan\n",
      "8840: accuracy:0.05 loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8850: accuracy:0.07 loss: nan\n",
      "8850: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8860: accuracy:0.09 loss: nan\n",
      "8870: accuracy:0.09 loss: nan\n",
      "8880: accuracy:0.13 loss: nan\n",
      "8890: accuracy:0.12 loss: nan\n",
      "8900: accuracy:0.06 loss: nan\n",
      "8900: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8910: accuracy:0.15 loss: nan\n",
      "8920: accuracy:0.1 loss: nan\n",
      "8930: accuracy:0.1 loss: nan\n",
      "8940: accuracy:0.15 loss: nan\n",
      "8950: accuracy:0.1 loss: nan\n",
      "8950: ********* epoch 15 ********* test accuracy:0.098 test loss: nan\n",
      "8960: accuracy:0.09 loss: nan\n",
      "8970: accuracy:0.06 loss: nan\n",
      "8980: accuracy:0.09 loss: nan\n",
      "8990: accuracy:0.08 loss: nan\n",
      "9000: accuracy:0.12 loss: nan\n",
      "9000: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9010: accuracy:0.1 loss: nan\n",
      "9020: accuracy:0.09 loss: nan\n",
      "9030: accuracy:0.11 loss: nan\n",
      "9040: accuracy:0.14 loss: nan\n",
      "9050: accuracy:0.12 loss: nan\n",
      "9050: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9060: accuracy:0.1 loss: nan\n",
      "9070: accuracy:0.08 loss: nan\n",
      "9080: accuracy:0.14 loss: nan\n",
      "9090: accuracy:0.09 loss: nan\n",
      "9100: accuracy:0.08 loss: nan\n",
      "9100: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9110: accuracy:0.1 loss: nan\n",
      "9120: accuracy:0.12 loss: nan\n",
      "9130: accuracy:0.1 loss: nan\n",
      "9140: accuracy:0.13 loss: nan\n",
      "9150: accuracy:0.07 loss: nan\n",
      "9150: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9160: accuracy:0.13 loss: nan\n",
      "9170: accuracy:0.08 loss: nan\n",
      "9180: accuracy:0.14 loss: nan\n",
      "9190: accuracy:0.15 loss: nan\n",
      "9200: accuracy:0.07 loss: nan\n",
      "9200: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9210: accuracy:0.08 loss: nan\n",
      "9220: accuracy:0.08 loss: nan\n",
      "9230: accuracy:0.09 loss: nan\n",
      "9240: accuracy:0.07 loss: nan\n",
      "9250: accuracy:0.11 loss: nan\n",
      "9250: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9260: accuracy:0.1 loss: nan\n",
      "9270: accuracy:0.13 loss: nan\n",
      "9280: accuracy:0.11 loss: nan\n",
      "9290: accuracy:0.1 loss: nan\n",
      "9300: accuracy:0.13 loss: nan\n",
      "9300: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9310: accuracy:0.08 loss: nan\n",
      "9320: accuracy:0.03 loss: nan\n",
      "9330: accuracy:0.07 loss: nan\n",
      "9340: accuracy:0.08 loss: nan\n",
      "9350: accuracy:0.07 loss: nan\n",
      "9350: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9360: accuracy:0.09 loss: nan\n",
      "9370: accuracy:0.07 loss: nan\n",
      "9380: accuracy:0.08 loss: nan\n",
      "9390: accuracy:0.08 loss: nan\n",
      "9400: accuracy:0.08 loss: nan\n",
      "9400: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9410: accuracy:0.09 loss: nan\n",
      "9420: accuracy:0.12 loss: nan\n",
      "9430: accuracy:0.06 loss: nan\n",
      "9440: accuracy:0.1 loss: nan\n",
      "9450: accuracy:0.15 loss: nan\n",
      "9450: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9460: accuracy:0.06 loss: nan\n",
      "9470: accuracy:0.07 loss: nan\n",
      "9480: accuracy:0.09 loss: nan\n",
      "9490: accuracy:0.07 loss: nan\n",
      "9500: accuracy:0.09 loss: nan\n",
      "9500: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9510: accuracy:0.17 loss: nan\n",
      "9520: accuracy:0.07 loss: nan\n",
      "9530: accuracy:0.1 loss: nan\n",
      "9540: accuracy:0.09 loss: nan\n",
      "9550: accuracy:0.06 loss: nan\n",
      "9550: ********* epoch 16 ********* test accuracy:0.098 test loss: nan\n",
      "9560: accuracy:0.13 loss: nan\n",
      "9570: accuracy:0.11 loss: nan\n",
      "9580: accuracy:0.08 loss: nan\n",
      "9590: accuracy:0.06 loss: nan\n",
      "9600: accuracy:0.07 loss: nan\n",
      "9600: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9610: accuracy:0.09 loss: nan\n",
      "9620: accuracy:0.09 loss: nan\n",
      "9630: accuracy:0.09 loss: nan\n",
      "9640: accuracy:0.1 loss: nan\n",
      "9650: accuracy:0.06 loss: nan\n",
      "9650: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9660: accuracy:0.1 loss: nan\n",
      "9670: accuracy:0.15 loss: nan\n",
      "9680: accuracy:0.07 loss: nan\n",
      "9690: accuracy:0.13 loss: nan\n",
      "9700: accuracy:0.13 loss: nan\n",
      "9700: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9710: accuracy:0.09 loss: nan\n",
      "9720: accuracy:0.12 loss: nan\n",
      "9730: accuracy:0.14 loss: nan\n",
      "9740: accuracy:0.09 loss: nan\n",
      "9750: accuracy:0.08 loss: nan\n",
      "9750: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9760: accuracy:0.12 loss: nan\n",
      "9770: accuracy:0.1 loss: nan\n",
      "9780: accuracy:0.1 loss: nan\n",
      "9790: accuracy:0.08 loss: nan\n",
      "9800: accuracy:0.15 loss: nan\n",
      "9800: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9810: accuracy:0.06 loss: nan\n",
      "9820: accuracy:0.14 loss: nan\n",
      "9830: accuracy:0.09 loss: nan\n",
      "9840: accuracy:0.08 loss: nan\n",
      "9850: accuracy:0.09 loss: nan\n",
      "9850: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9860: accuracy:0.08 loss: nan\n",
      "9870: accuracy:0.1 loss: nan\n",
      "9880: accuracy:0.12 loss: nan\n",
      "9890: accuracy:0.16 loss: nan\n",
      "9900: accuracy:0.05 loss: nan\n",
      "9900: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9910: accuracy:0.09 loss: nan\n",
      "9920: accuracy:0.11 loss: nan\n",
      "9930: accuracy:0.14 loss: nan\n",
      "9940: accuracy:0.13 loss: nan\n",
      "9950: accuracy:0.11 loss: nan\n",
      "9950: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "9960: accuracy:0.08 loss: nan\n",
      "9970: accuracy:0.04 loss: nan\n",
      "9980: accuracy:0.09 loss: nan\n",
      "9990: accuracy:0.11 loss: nan\n",
      "10000: accuracy:0.1 loss: nan\n",
      "10000: ********* epoch 17 ********* test accuracy:0.098 test loss: nan\n",
      "max test accuracy: 0.9799\n"
     ]
    }
   ],
   "source": [
    "# input X => 28x28 grayscale dimensions ('None' indexes images in mini-batch)\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "\n",
    "# correct answers\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights\n",
    "W1 = tf.Variable(tf.truncated_normal([28*28, 200], stddev=0.1))\n",
    "W2 = tf.Variable(tf.truncated_normal([200, 100], stddev=0.1))\n",
    "W3 = tf.Variable(tf.truncated_normal([100, 10], stddev=0.1))\n",
    "\n",
    "# biases\n",
    "b1 = tf.Variable(tf.ones([200]) / 10)\n",
    "b2 = tf.Variable(tf.ones([100]) / 10)\n",
    "b3 = tf.Variable(tf.ones([10]) / 10)\n",
    "\n",
    "# flatten images into single line\n",
    "XX = tf.reshape(X, [-1, 28*28])\n",
    "\n",
    "# model\n",
    "Y1 = tf.nn.relu(tf.matmul(XX, W1) + b1)\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)\n",
    "Y = tf.nn.softmax(tf.matmul(Y2, W3) + b3)\n",
    "\n",
    "# cross entropy\n",
    "cross_entropy = -tf.reduce_mean(Y_ * tf.log(Y)) * 1000.0\n",
    "\n",
    "# accuracy of the trained model [0, 1] ([worst, best])\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# training (learning rate is 0.003)\n",
    "train_step = tf.train.AdamOptimizer(0.003).minimize(cross_entropy)\n",
    "\n",
    "# initialization\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# call in loop to train the model 100 images at a time\n",
    "max_test_accuracy = 0.0\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    # train on batches of 100 images w/ 100 labels\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    # compute training values for visualisation\n",
    "    if update_train_data:\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: batch_X, Y_: batch_Y})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c))\n",
    "    # compute test values for visualization\n",
    "    if update_test_data:\n",
    "        global max_test_accuracy\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: mnist.test.images, Y_: mnist.test.labels})\n",
    "        if (a > max_test_accuracy):\n",
    "            max_test_accuracy = a\n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \n",
    "              \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        \n",
    "    # back-propogation training step\n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y})\n",
    "\n",
    "# text visualization of process\n",
    "for i in range(10000+1):\n",
    "    training_step(i, i % 50 == 0, i % 10 == 0)\n",
    "    \n",
    "# display the final max accuracy\n",
    "print(\"max test accuracy: \" + str(max_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Convergence by Removing NaN\n",
    "The following cell contains modifications to remove the instances of NaN for cross entropy occurring in some of the previous cells. This was a result of computing log(0), which occurred after an operation of ~exp(-100). The fix comes in the form of logits. With 10k iterations, this maxes out with an accuracy of ~98% (no NaN!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-ff8f6db45cfc>:27: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "0: accuracy:0.11 loss: 249.59492\n",
      "0: ********* epoch 1 ********* test accuracy:0.0923 test loss: 250.09544\n",
      "10: accuracy:0.81 loss: 73.38991\n",
      "20: accuracy:0.79 loss: 55.71934\n",
      "30: accuracy:0.86 loss: 38.35451\n",
      "40: accuracy:0.88 loss: 36.331123\n",
      "50: accuracy:0.9 loss: 29.884842\n",
      "50: ********* epoch 1 ********* test accuracy:0.902 test loss: 32.857964\n",
      "60: accuracy:0.85 loss: 34.840424\n",
      "70: accuracy:0.95 loss: 17.95077\n",
      "80: accuracy:0.93 loss: 22.465893\n",
      "90: accuracy:0.93 loss: 31.196577\n",
      "100: accuracy:0.94 loss: 27.64785\n",
      "100: ********* epoch 1 ********* test accuracy:0.9349 test loss: 21.792522\n",
      "110: accuracy:0.93 loss: 22.694193\n",
      "120: accuracy:0.95 loss: 21.816254\n",
      "130: accuracy:0.94 loss: 21.195288\n",
      "140: accuracy:0.93 loss: 29.558035\n",
      "150: accuracy:0.89 loss: 38.924362\n",
      "150: ********* epoch 1 ********* test accuracy:0.9396 test loss: 20.176449\n",
      "160: accuracy:0.89 loss: 29.27444\n",
      "170: accuracy:0.95 loss: 22.44251\n",
      "180: accuracy:0.96 loss: 11.488598\n",
      "190: accuracy:0.96 loss: 17.902115\n",
      "200: accuracy:0.93 loss: 21.01951\n",
      "200: ********* epoch 1 ********* test accuracy:0.9347 test loss: 20.61696\n",
      "210: accuracy:0.95 loss: 14.944484\n",
      "220: accuracy:0.9 loss: 42.260223\n",
      "230: accuracy:0.92 loss: 22.8958\n",
      "240: accuracy:0.96 loss: 10.184014\n",
      "250: accuracy:0.93 loss: 25.661621\n",
      "250: ********* epoch 1 ********* test accuracy:0.9528 test loss: 15.106853\n",
      "260: accuracy:0.94 loss: 20.679897\n",
      "270: accuracy:0.91 loss: 24.960907\n",
      "280: accuracy:0.95 loss: 12.657708\n",
      "290: accuracy:0.98 loss: 12.285361\n",
      "300: accuracy:0.95 loss: 19.203217\n",
      "300: ********* epoch 1 ********* test accuracy:0.9593 test loss: 13.397206\n",
      "310: accuracy:0.93 loss: 22.04663\n",
      "320: accuracy:0.97 loss: 10.801178\n",
      "330: accuracy:0.99 loss: 10.3073\n",
      "340: accuracy:0.93 loss: 31.782785\n",
      "350: accuracy:0.98 loss: 10.186557\n",
      "350: ********* epoch 1 ********* test accuracy:0.9573 test loss: 13.590667\n",
      "360: accuracy:0.98 loss: 7.850451\n",
      "370: accuracy:0.97 loss: 10.599452\n",
      "380: accuracy:0.98 loss: 9.538643\n",
      "390: accuracy:0.97 loss: 11.893798\n",
      "400: accuracy:0.91 loss: 30.303284\n",
      "400: ********* epoch 1 ********* test accuracy:0.9571 test loss: 13.475368\n",
      "410: accuracy:0.97 loss: 13.382782\n",
      "420: accuracy:0.96 loss: 13.443309\n",
      "430: accuracy:0.94 loss: 26.530529\n",
      "440: accuracy:0.96 loss: 13.46937\n",
      "450: accuracy:0.96 loss: 12.817852\n",
      "450: ********* epoch 1 ********* test accuracy:0.9669 test loss: 11.38957\n",
      "460: accuracy:0.97 loss: 14.230147\n",
      "470: accuracy:0.95 loss: 12.997339\n",
      "480: accuracy:0.94 loss: 14.081232\n",
      "490: accuracy:0.97 loss: 8.947994\n",
      "500: accuracy:0.92 loss: 18.55169\n",
      "500: ********* epoch 1 ********* test accuracy:0.9601 test loss: 12.71397\n",
      "510: accuracy:0.98 loss: 5.5968776\n",
      "520: accuracy:0.94 loss: 22.286243\n",
      "530: accuracy:0.96 loss: 18.41395\n",
      "540: accuracy:0.97 loss: 18.962105\n",
      "550: accuracy:0.98 loss: 3.6570332\n",
      "550: ********* epoch 1 ********* test accuracy:0.9637 test loss: 11.706385\n",
      "560: accuracy:0.98 loss: 6.0953684\n",
      "570: accuracy:0.98 loss: 11.062046\n",
      "580: accuracy:0.97 loss: 15.356392\n",
      "590: accuracy:0.94 loss: 16.765118\n",
      "600: accuracy:0.96 loss: 10.863598\n",
      "600: ********* epoch 2 ********* test accuracy:0.9636 test loss: 11.976648\n",
      "610: accuracy:0.96 loss: 7.3439455\n",
      "620: accuracy:0.97 loss: 11.737795\n",
      "630: accuracy:0.96 loss: 10.607976\n",
      "640: accuracy:0.99 loss: 2.523231\n",
      "650: accuracy:1.0 loss: 2.379758\n",
      "650: ********* epoch 2 ********* test accuracy:0.9684 test loss: 10.018596\n",
      "660: accuracy:0.98 loss: 6.0615463\n",
      "670: accuracy:0.99 loss: 4.0439715\n",
      "680: accuracy:0.97 loss: 9.450365\n",
      "690: accuracy:0.98 loss: 10.288603\n",
      "700: accuracy:0.99 loss: 3.167887\n",
      "700: ********* epoch 2 ********* test accuracy:0.9696 test loss: 10.079882\n",
      "710: accuracy:0.97 loss: 8.575112\n",
      "720: accuracy:0.98 loss: 5.280656\n",
      "730: accuracy:0.97 loss: 5.121922\n",
      "740: accuracy:0.98 loss: 9.986538\n",
      "750: accuracy:1.0 loss: 5.0289984\n",
      "750: ********* epoch 2 ********* test accuracy:0.9666 test loss: 11.448024\n",
      "760: accuracy:0.97 loss: 5.9509783\n",
      "770: accuracy:0.99 loss: 2.850772\n",
      "780: accuracy:0.99 loss: 2.994125\n",
      "790: accuracy:0.96 loss: 5.949741\n",
      "800: accuracy:0.97 loss: 9.626451\n",
      "800: ********* epoch 2 ********* test accuracy:0.9642 test loss: 11.960824\n",
      "810: accuracy:0.97 loss: 12.384168\n",
      "820: accuracy:0.94 loss: 12.425978\n",
      "830: accuracy:0.98 loss: 5.6841707\n",
      "840: accuracy:0.95 loss: 14.430613\n",
      "850: accuracy:0.96 loss: 6.486541\n",
      "850: ********* epoch 2 ********* test accuracy:0.9663 test loss: 11.596472\n",
      "860: accuracy:0.97 loss: 16.222517\n",
      "870: accuracy:0.99 loss: 4.375919\n",
      "880: accuracy:0.97 loss: 6.2018037\n",
      "890: accuracy:0.96 loss: 11.80052\n",
      "900: accuracy:0.98 loss: 4.936906\n",
      "900: ********* epoch 2 ********* test accuracy:0.9635 test loss: 11.308857\n",
      "910: accuracy:0.98 loss: 3.0307965\n",
      "920: accuracy:0.97 loss: 14.959974\n",
      "930: accuracy:0.96 loss: 8.686598\n",
      "940: accuracy:0.97 loss: 11.553264\n",
      "950: accuracy:1.0 loss: 2.4153152\n",
      "950: ********* epoch 2 ********* test accuracy:0.9721 test loss: 8.960546\n",
      "960: accuracy:0.96 loss: 9.880321\n",
      "970: accuracy:0.96 loss: 13.653718\n",
      "980: accuracy:0.96 loss: 10.006842\n",
      "990: accuracy:0.97 loss: 11.910092\n",
      "1000: accuracy:0.97 loss: 8.399623\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9704 test loss: 9.196629\n",
      "1010: accuracy:0.96 loss: 7.7888265\n",
      "1020: accuracy:0.98 loss: 8.115814\n",
      "1030: accuracy:0.99 loss: 4.1911283\n",
      "1040: accuracy:0.97 loss: 8.894683\n",
      "1050: accuracy:0.97 loss: 15.17116\n",
      "1050: ********* epoch 2 ********* test accuracy:0.9741 test loss: 8.405513\n",
      "1060: accuracy:0.99 loss: 7.874015\n",
      "1070: accuracy:1.0 loss: 2.853748\n",
      "1080: accuracy:0.98 loss: 8.995977\n",
      "1090: accuracy:0.99 loss: 3.6988213\n",
      "1100: accuracy:0.97 loss: 10.1890745\n",
      "1100: ********* epoch 2 ********* test accuracy:0.9751 test loss: 8.609058\n",
      "1110: accuracy:1.0 loss: 2.0180638\n",
      "1120: accuracy:0.95 loss: 13.907465\n",
      "1130: accuracy:0.99 loss: 5.0854044\n",
      "1140: accuracy:0.98 loss: 3.4744549\n",
      "1150: accuracy:0.98 loss: 5.429576\n",
      "1150: ********* epoch 2 ********* test accuracy:0.9655 test loss: 10.645425\n",
      "1160: accuracy:0.99 loss: 3.3776007\n",
      "1170: accuracy:0.95 loss: 10.8666525\n",
      "1180: accuracy:1.0 loss: 1.4436013\n",
      "1190: accuracy:0.99 loss: 9.307355\n",
      "1200: accuracy:0.99 loss: 1.7640916\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9738 test loss: 8.72602\n",
      "1210: accuracy:1.0 loss: 3.4080126\n",
      "1220: accuracy:0.97 loss: 8.141615\n",
      "1230: accuracy:0.98 loss: 6.424336\n",
      "1240: accuracy:0.99 loss: 3.943342\n",
      "1250: accuracy:0.99 loss: 2.503203\n",
      "1250: ********* epoch 3 ********* test accuracy:0.97 test loss: 10.144753\n",
      "1260: accuracy:1.0 loss: 2.670466\n",
      "1270: accuracy:0.99 loss: 5.238098\n",
      "1280: accuracy:0.99 loss: 4.7489176\n",
      "1290: accuracy:0.98 loss: 6.996149\n",
      "1300: accuracy:0.96 loss: 13.525875\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9725 test loss: 8.833679\n",
      "1310: accuracy:0.97 loss: 6.8893356\n",
      "1320: accuracy:0.97 loss: 5.4583993\n",
      "1330: accuracy:1.0 loss: 0.65911067\n",
      "1340: accuracy:0.96 loss: 6.1287565\n",
      "1350: accuracy:1.0 loss: 1.131411\n",
      "1350: ********* epoch 3 ********* test accuracy:0.9756 test loss: 8.253761\n",
      "1360: accuracy:0.96 loss: 11.152739\n",
      "1370: accuracy:0.99 loss: 2.1241546\n",
      "1380: accuracy:0.97 loss: 23.412727\n",
      "1390: accuracy:0.99 loss: 1.8915858\n",
      "1400: accuracy:0.96 loss: 10.211668\n",
      "1400: ********* epoch 3 ********* test accuracy:0.9745 test loss: 8.72571\n",
      "1410: accuracy:1.0 loss: 1.2664855\n",
      "1420: accuracy:0.98 loss: 5.378912\n",
      "1430: accuracy:1.0 loss: 2.0947073\n",
      "1440: accuracy:0.99 loss: 3.7948246\n",
      "1450: accuracy:1.0 loss: 2.2504523\n",
      "1450: ********* epoch 3 ********* test accuracy:0.9737 test loss: 8.644873\n",
      "1460: accuracy:0.98 loss: 7.3337293\n",
      "1470: accuracy:0.99 loss: 3.9777954\n",
      "1480: accuracy:0.98 loss: 5.4435744\n",
      "1490: accuracy:0.98 loss: 3.6912665\n",
      "1500: accuracy:0.97 loss: 7.262818\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9699 test loss: 9.824762\n",
      "1510: accuracy:0.99 loss: 4.2343087\n",
      "1520: accuracy:1.0 loss: 1.681692\n",
      "1530: accuracy:0.98 loss: 4.203801\n",
      "1540: accuracy:0.97 loss: 8.903934\n",
      "1550: accuracy:0.99 loss: 7.509461\n",
      "1550: ********* epoch 3 ********* test accuracy:0.9701 test loss: 9.806826\n",
      "1560: accuracy:0.96 loss: 11.488304\n",
      "1570: accuracy:0.99 loss: 4.696849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1580: accuracy:0.99 loss: 3.0075424\n",
      "1590: accuracy:0.99 loss: 4.497889\n",
      "1600: accuracy:0.97 loss: 5.7911177\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9715 test loss: 9.360441\n",
      "1610: accuracy:1.0 loss: 0.9703548\n",
      "1620: accuracy:0.96 loss: 8.448071\n",
      "1630: accuracy:0.98 loss: 3.7305794\n",
      "1640: accuracy:0.98 loss: 3.0101838\n",
      "1650: accuracy:0.97 loss: 11.710651\n",
      "1650: ********* epoch 3 ********* test accuracy:0.9736 test loss: 8.827088\n",
      "1660: accuracy:0.99 loss: 10.035101\n",
      "1670: accuracy:0.98 loss: 5.5451784\n",
      "1680: accuracy:0.98 loss: 4.8243637\n",
      "1690: accuracy:0.99 loss: 3.7767358\n",
      "1700: accuracy:0.97 loss: 10.99121\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9684 test loss: 11.5730915\n",
      "1710: accuracy:0.99 loss: 1.129207\n",
      "1720: accuracy:0.99 loss: 4.0856853\n",
      "1730: accuracy:0.98 loss: 4.6687617\n",
      "1740: accuracy:0.97 loss: 10.119687\n",
      "1750: accuracy:0.95 loss: 7.5996833\n",
      "1750: ********* epoch 3 ********* test accuracy:0.9737 test loss: 9.548503\n",
      "1760: accuracy:0.97 loss: 5.377203\n",
      "1770: accuracy:1.0 loss: 4.1469703\n",
      "1780: accuracy:0.98 loss: 6.537331\n",
      "1790: accuracy:0.99 loss: 2.8929622\n",
      "1800: accuracy:0.97 loss: 5.5857124\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9739 test loss: 8.963125\n",
      "1810: accuracy:0.98 loss: 3.2954738\n",
      "1820: accuracy:1.0 loss: 1.0106165\n",
      "1830: accuracy:0.99 loss: 3.081208\n",
      "1840: accuracy:0.99 loss: 1.9370569\n",
      "1850: accuracy:0.99 loss: 5.68556\n",
      "1850: ********* epoch 4 ********* test accuracy:0.972 test loss: 9.287442\n",
      "1860: accuracy:1.0 loss: 0.98793644\n",
      "1870: accuracy:0.99 loss: 3.816147\n",
      "1880: accuracy:0.98 loss: 7.448318\n",
      "1890: accuracy:0.97 loss: 5.866232\n",
      "1900: accuracy:0.98 loss: 7.93543\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9731 test loss: 9.593124\n",
      "1910: accuracy:0.97 loss: 14.782168\n",
      "1920: accuracy:0.98 loss: 3.3350668\n",
      "1930: accuracy:0.98 loss: 2.850767\n",
      "1940: accuracy:0.98 loss: 5.907519\n",
      "1950: accuracy:0.99 loss: 4.983773\n",
      "1950: ********* epoch 4 ********* test accuracy:0.9779 test loss: 7.824357\n",
      "1960: accuracy:1.0 loss: 1.5232238\n",
      "1970: accuracy:0.99 loss: 1.3169007\n",
      "1980: accuracy:0.99 loss: 5.6269236\n",
      "1990: accuracy:0.95 loss: 8.43417\n",
      "2000: accuracy:0.98 loss: 5.5940347\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9731 test loss: 8.756396\n",
      "2010: accuracy:0.98 loss: 4.0587206\n",
      "2020: accuracy:0.99 loss: 5.756909\n",
      "2030: accuracy:1.0 loss: 1.1311278\n",
      "2040: accuracy:0.98 loss: 3.2576635\n",
      "2050: accuracy:1.0 loss: 0.88650537\n",
      "2050: ********* epoch 4 ********* test accuracy:0.9754 test loss: 8.91693\n",
      "2060: accuracy:1.0 loss: 1.5019689\n",
      "2070: accuracy:0.98 loss: 3.6050968\n",
      "2080: accuracy:0.98 loss: 11.105509\n",
      "2090: accuracy:1.0 loss: 0.9398811\n",
      "2100: accuracy:0.99 loss: 2.7353642\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9762 test loss: 7.900177\n",
      "2110: accuracy:0.97 loss: 8.710318\n",
      "2120: accuracy:0.98 loss: 3.6275976\n",
      "2130: accuracy:0.98 loss: 5.5059977\n",
      "2140: accuracy:1.0 loss: 2.0167713\n",
      "2150: accuracy:0.98 loss: 4.6252103\n",
      "2150: ********* epoch 4 ********* test accuracy:0.976 test loss: 8.547149\n",
      "2160: accuracy:0.98 loss: 3.4875486\n",
      "2170: accuracy:1.0 loss: 0.91374004\n",
      "2180: accuracy:0.99 loss: 1.861565\n",
      "2190: accuracy:1.0 loss: 0.7776674\n",
      "2200: accuracy:0.98 loss: 6.6675572\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9776 test loss: 7.524154\n",
      "2210: accuracy:0.99 loss: 9.271119\n",
      "2220: accuracy:0.97 loss: 16.478308\n",
      "2230: accuracy:0.99 loss: 3.2954905\n",
      "2240: accuracy:0.99 loss: 3.068419\n",
      "2250: accuracy:0.99 loss: 2.6689234\n",
      "2250: ********* epoch 4 ********* test accuracy:0.9753 test loss: 8.858342\n",
      "2260: accuracy:1.0 loss: 0.36412305\n",
      "2270: accuracy:0.99 loss: 2.2723331\n",
      "2280: accuracy:0.98 loss: 3.9611514\n",
      "2290: accuracy:0.99 loss: 3.0632975\n",
      "2300: accuracy:0.99 loss: 4.50279\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9751 test loss: 8.728812\n",
      "2310: accuracy:1.0 loss: 1.369539\n",
      "2320: accuracy:0.99 loss: 2.131089\n",
      "2330: accuracy:0.98 loss: 3.544406\n",
      "2340: accuracy:0.99 loss: 4.2624\n",
      "2350: accuracy:0.98 loss: 3.9034617\n",
      "2350: ********* epoch 4 ********* test accuracy:0.9761 test loss: 9.422828\n",
      "2360: accuracy:1.0 loss: 1.2897367\n",
      "2370: accuracy:0.99 loss: 2.4776535\n",
      "2380: accuracy:0.99 loss: 1.6944461\n",
      "2390: accuracy:0.98 loss: 6.668594\n",
      "2400: accuracy:0.97 loss: 7.25766\n",
      "2400: ********* epoch 5 ********* test accuracy:0.9712 test loss: 10.975601\n",
      "2410: accuracy:0.98 loss: 4.4693575\n",
      "2420: accuracy:1.0 loss: 0.5608814\n",
      "2430: accuracy:0.99 loss: 1.8129057\n",
      "2440: accuracy:0.97 loss: 8.313884\n",
      "2450: accuracy:0.99 loss: 4.0345464\n",
      "2450: ********* epoch 5 ********* test accuracy:0.9773 test loss: 8.315947\n",
      "2460: accuracy:1.0 loss: 1.1313026\n",
      "2470: accuracy:0.98 loss: 2.879433\n",
      "2480: accuracy:0.97 loss: 7.6231933\n",
      "2490: accuracy:0.99 loss: 2.0223603\n",
      "2500: accuracy:0.99 loss: 1.5247829\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9806 test loss: 7.379776\n",
      "2510: accuracy:1.0 loss: 1.2307026\n",
      "2520: accuracy:0.97 loss: 8.283048\n",
      "2530: accuracy:0.99 loss: 1.8316259\n",
      "2540: accuracy:0.99 loss: 4.1391683\n",
      "2550: accuracy:1.0 loss: 0.8697328\n",
      "2550: ********* epoch 5 ********* test accuracy:0.9782 test loss: 8.36725\n",
      "2560: accuracy:0.99 loss: 1.0817332\n",
      "2570: accuracy:0.98 loss: 4.7780433\n",
      "2580: accuracy:0.97 loss: 9.2448845\n",
      "2590: accuracy:0.98 loss: 9.265609\n",
      "2600: accuracy:0.99 loss: 3.1436958\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9763 test loss: 8.509082\n",
      "2610: accuracy:1.0 loss: 0.8750748\n",
      "2620: accuracy:0.97 loss: 6.806146\n",
      "2630: accuracy:0.98 loss: 5.794333\n",
      "2640: accuracy:0.99 loss: 1.3008409\n",
      "2650: accuracy:0.98 loss: 4.863562\n",
      "2650: ********* epoch 5 ********* test accuracy:0.9754 test loss: 9.352726\n",
      "2660: accuracy:1.0 loss: 0.38581437\n",
      "2670: accuracy:0.98 loss: 6.0977287\n",
      "2680: accuracy:0.98 loss: 7.7601867\n",
      "2690: accuracy:1.0 loss: 0.9551703\n",
      "2700: accuracy:0.97 loss: 7.4283338\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9719 test loss: 9.815957\n",
      "2710: accuracy:0.98 loss: 5.7466764\n",
      "2720: accuracy:0.99 loss: 4.3679695\n",
      "2730: accuracy:0.98 loss: 4.5145497\n",
      "2740: accuracy:0.99 loss: 3.5403419\n",
      "2750: accuracy:1.0 loss: 1.0280634\n",
      "2750: ********* epoch 5 ********* test accuracy:0.9758 test loss: 9.0032015\n",
      "2760: accuracy:0.98 loss: 2.2686505\n",
      "2770: accuracy:0.98 loss: 2.993786\n",
      "2780: accuracy:0.97 loss: 9.68058\n",
      "2790: accuracy:1.0 loss: 0.5749938\n",
      "2800: accuracy:1.0 loss: 0.9997164\n",
      "2800: ********* epoch 5 ********* test accuracy:0.976 test loss: 9.079404\n",
      "2810: accuracy:0.98 loss: 15.559048\n",
      "2820: accuracy:0.97 loss: 7.757383\n",
      "2830: accuracy:0.99 loss: 1.6522022\n",
      "2840: accuracy:1.0 loss: 1.6251898\n",
      "2850: accuracy:1.0 loss: 0.32332855\n",
      "2850: ********* epoch 5 ********* test accuracy:0.9762 test loss: 9.282648\n",
      "2860: accuracy:0.99 loss: 2.1587834\n",
      "2870: accuracy:0.98 loss: 6.196461\n",
      "2880: accuracy:1.0 loss: 1.1701816\n",
      "2890: accuracy:1.0 loss: 1.1626997\n",
      "2900: accuracy:0.98 loss: 6.294269\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9778 test loss: 8.544684\n",
      "2910: accuracy:0.98 loss: 7.5693674\n",
      "2920: accuracy:0.98 loss: 5.2820864\n",
      "2930: accuracy:0.99 loss: 2.7885644\n",
      "2940: accuracy:0.98 loss: 5.3260264\n",
      "2950: accuracy:0.99 loss: 3.0406117\n",
      "2950: ********* epoch 5 ********* test accuracy:0.9702 test loss: 11.034024\n",
      "2960: accuracy:0.97 loss: 10.0336\n",
      "2970: accuracy:0.97 loss: 6.5439024\n",
      "2980: accuracy:0.96 loss: 18.910295\n",
      "2990: accuracy:1.0 loss: 1.2380923\n",
      "3000: accuracy:1.0 loss: 0.49055564\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9783 test loss: 7.938531\n",
      "3010: accuracy:1.0 loss: 1.192456\n",
      "3020: accuracy:1.0 loss: 1.0642246\n",
      "3030: accuracy:1.0 loss: 0.5143365\n",
      "3040: accuracy:1.0 loss: 1.8598225\n",
      "3050: accuracy:0.97 loss: 5.4299803\n",
      "3050: ********* epoch 6 ********* test accuracy:0.9754 test loss: 8.892097\n",
      "3060: accuracy:0.98 loss: 4.251936\n",
      "3070: accuracy:0.99 loss: 2.5371177\n",
      "3080: accuracy:0.98 loss: 6.866935\n",
      "3090: accuracy:1.0 loss: 1.7566109\n",
      "3100: accuracy:0.98 loss: 7.308218\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9764 test loss: 8.519343\n",
      "3110: accuracy:0.98 loss: 3.8000453\n",
      "3120: accuracy:0.97 loss: 3.1407106\n",
      "3130: accuracy:0.99 loss: 1.8649526\n",
      "3140: accuracy:0.99 loss: 6.090526\n",
      "3150: accuracy:0.98 loss: 4.2889442\n",
      "3150: ********* epoch 6 ********* test accuracy:0.9753 test loss: 9.259186\n",
      "3160: accuracy:0.99 loss: 1.8106829\n",
      "3170: accuracy:0.98 loss: 6.743222\n",
      "3180: accuracy:1.0 loss: 0.081222676\n",
      "3190: accuracy:0.99 loss: 2.0773194\n",
      "3200: accuracy:0.99 loss: 4.4828196\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9773 test loss: 9.175489\n",
      "3210: accuracy:0.97 loss: 7.625816\n",
      "3220: accuracy:0.99 loss: 6.1114945\n",
      "3230: accuracy:0.99 loss: 2.2260714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3240: accuracy:0.99 loss: 5.72909\n",
      "3250: accuracy:1.0 loss: 0.27844882\n",
      "3250: ********* epoch 6 ********* test accuracy:0.9765 test loss: 10.391203\n",
      "3260: accuracy:0.99 loss: 1.7056359\n",
      "3270: accuracy:0.99 loss: 2.4664304\n",
      "3280: accuracy:1.0 loss: 1.3146739\n",
      "3290: accuracy:1.0 loss: 0.60389036\n",
      "3300: accuracy:1.0 loss: 1.5180552\n",
      "3300: ********* epoch 6 ********* test accuracy:0.9753 test loss: 9.465269\n",
      "3310: accuracy:1.0 loss: 1.2206448\n",
      "3320: accuracy:1.0 loss: 0.66594815\n",
      "3330: accuracy:0.99 loss: 1.7419524\n",
      "3340: accuracy:0.99 loss: 3.5434198\n",
      "3350: accuracy:0.98 loss: 6.1963344\n",
      "3350: ********* epoch 6 ********* test accuracy:0.9784 test loss: 8.586747\n",
      "3360: accuracy:0.99 loss: 7.352934\n",
      "3370: accuracy:0.99 loss: 3.1416445\n",
      "3380: accuracy:1.0 loss: 0.8880373\n",
      "3390: accuracy:0.97 loss: 5.595838\n",
      "3400: accuracy:0.99 loss: 3.3837776\n",
      "3400: ********* epoch 6 ********* test accuracy:0.9758 test loss: 9.900097\n",
      "3410: accuracy:0.99 loss: 2.35928\n",
      "3420: accuracy:0.97 loss: 6.299825\n",
      "3430: accuracy:0.98 loss: 9.620668\n",
      "3440: accuracy:0.99 loss: 5.343108\n",
      "3450: accuracy:1.0 loss: 0.5208708\n",
      "3450: ********* epoch 6 ********* test accuracy:0.9775 test loss: 9.280438\n",
      "3460: accuracy:1.0 loss: 1.5463547\n",
      "3470: accuracy:0.99 loss: 2.9717426\n",
      "3480: accuracy:0.97 loss: 6.356671\n",
      "3490: accuracy:0.99 loss: 3.0877628\n",
      "3500: accuracy:1.0 loss: 0.8276317\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9762 test loss: 9.491353\n",
      "3510: accuracy:0.99 loss: 1.9470226\n",
      "3520: accuracy:0.99 loss: 1.2658058\n",
      "3530: accuracy:1.0 loss: 0.50326073\n",
      "3540: accuracy:0.97 loss: 8.338572\n",
      "3550: accuracy:0.99 loss: 2.8858817\n",
      "3550: ********* epoch 6 ********* test accuracy:0.9768 test loss: 9.4750595\n",
      "3560: accuracy:0.96 loss: 7.0812263\n",
      "3570: accuracy:0.99 loss: 2.5767603\n",
      "3580: accuracy:0.99 loss: 6.8772264\n",
      "3590: accuracy:0.99 loss: 6.493941\n",
      "3600: accuracy:0.99 loss: 1.145501\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9747 test loss: 10.075881\n",
      "3610: accuracy:1.0 loss: 0.59888595\n",
      "3620: accuracy:1.0 loss: 0.8486221\n",
      "3630: accuracy:0.99 loss: 2.2037318\n",
      "3640: accuracy:1.0 loss: 0.2408242\n",
      "3650: accuracy:0.99 loss: 0.85905313\n",
      "3650: ********* epoch 7 ********* test accuracy:0.9756 test loss: 9.406134\n",
      "3660: accuracy:0.99 loss: 1.8958387\n",
      "3670: accuracy:1.0 loss: 0.3798369\n",
      "3680: accuracy:0.99 loss: 1.784651\n",
      "3690: accuracy:1.0 loss: 0.54538935\n",
      "3700: accuracy:1.0 loss: 0.780236\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9796 test loss: 8.32159\n",
      "3710: accuracy:1.0 loss: 2.6418076\n",
      "3720: accuracy:1.0 loss: 1.3880504\n",
      "3730: accuracy:0.99 loss: 1.0182136\n",
      "3740: accuracy:1.0 loss: 1.0363159\n",
      "3750: accuracy:1.0 loss: 0.1974925\n",
      "3750: ********* epoch 7 ********* test accuracy:0.9785 test loss: 8.705575\n",
      "3760: accuracy:1.0 loss: 0.73390555\n",
      "3770: accuracy:0.99 loss: 4.249979\n",
      "3780: accuracy:0.99 loss: 1.6049085\n",
      "3790: accuracy:0.99 loss: 4.176062\n",
      "3800: accuracy:1.0 loss: 0.18762004\n",
      "3800: ********* epoch 7 ********* test accuracy:0.9774 test loss: 8.735742\n",
      "3810: accuracy:1.0 loss: 0.9054317\n",
      "3820: accuracy:1.0 loss: 0.5030696\n",
      "3830: accuracy:1.0 loss: 0.25843188\n",
      "3840: accuracy:1.0 loss: 0.82195604\n",
      "3850: accuracy:1.0 loss: 2.8596764\n",
      "3850: ********* epoch 7 ********* test accuracy:0.9789 test loss: 8.990641\n",
      "3860: accuracy:0.98 loss: 2.0059557\n",
      "3870: accuracy:1.0 loss: 0.35364014\n",
      "3880: accuracy:0.96 loss: 8.291827\n",
      "3890: accuracy:1.0 loss: 0.27813196\n",
      "3900: accuracy:0.99 loss: 1.0978131\n",
      "3900: ********* epoch 7 ********* test accuracy:0.9732 test loss: 10.745163\n",
      "3910: accuracy:1.0 loss: 0.4413452\n",
      "3920: accuracy:1.0 loss: 1.4260174\n",
      "3930: accuracy:0.97 loss: 9.439437\n",
      "3940: accuracy:0.99 loss: 1.2484584\n",
      "3950: accuracy:0.97 loss: 11.782683\n",
      "3950: ********* epoch 7 ********* test accuracy:0.9763 test loss: 9.116417\n",
      "3960: accuracy:0.98 loss: 4.128307\n",
      "3970: accuracy:0.97 loss: 12.770065\n",
      "3980: accuracy:0.99 loss: 3.2452197\n",
      "3990: accuracy:0.99 loss: 2.0990744\n",
      "4000: accuracy:1.0 loss: 0.56800866\n",
      "4000: ********* epoch 7 ********* test accuracy:0.9781 test loss: 8.710732\n",
      "4010: accuracy:0.99 loss: 2.2632568\n",
      "4020: accuracy:0.99 loss: 3.7975101\n",
      "4030: accuracy:0.97 loss: 15.393154\n",
      "4040: accuracy:0.99 loss: 3.8462532\n",
      "4050: accuracy:0.98 loss: 10.003645\n",
      "4050: ********* epoch 7 ********* test accuracy:0.9757 test loss: 9.930817\n",
      "4060: accuracy:0.99 loss: 1.746295\n",
      "4070: accuracy:0.99 loss: 1.4367348\n",
      "4080: accuracy:0.97 loss: 13.94226\n",
      "4090: accuracy:0.99 loss: 3.6062508\n",
      "4100: accuracy:0.99 loss: 3.753103\n",
      "4100: ********* epoch 7 ********* test accuracy:0.9767 test loss: 9.70527\n",
      "4110: accuracy:0.98 loss: 4.7147756\n",
      "4120: accuracy:0.98 loss: 8.425499\n",
      "4130: accuracy:0.99 loss: 2.1056056\n",
      "4140: accuracy:0.98 loss: 8.207577\n",
      "4150: accuracy:0.98 loss: 7.1457767\n",
      "4150: ********* epoch 7 ********* test accuracy:0.9724 test loss: 12.10627\n",
      "4160: accuracy:0.99 loss: 3.2090824\n",
      "4170: accuracy:0.97 loss: 4.721416\n",
      "4180: accuracy:0.99 loss: 2.6758406\n",
      "4190: accuracy:1.0 loss: 0.72813845\n",
      "4200: accuracy:0.99 loss: 1.0870363\n",
      "4200: ********* epoch 8 ********* test accuracy:0.9774 test loss: 9.580479\n",
      "4210: accuracy:0.99 loss: 1.7004607\n",
      "4220: accuracy:1.0 loss: 0.19477347\n",
      "4230: accuracy:1.0 loss: 1.186617\n",
      "4240: accuracy:1.0 loss: 0.41133192\n",
      "4250: accuracy:1.0 loss: 0.8135226\n",
      "4250: ********* epoch 8 ********* test accuracy:0.9769 test loss: 9.969924\n",
      "4260: accuracy:1.0 loss: 0.4278501\n",
      "4270: accuracy:0.98 loss: 4.92176\n",
      "4280: accuracy:1.0 loss: 0.39118564\n",
      "4290: accuracy:0.99 loss: 4.675746\n",
      "4300: accuracy:1.0 loss: 0.32137558\n",
      "4300: ********* epoch 8 ********* test accuracy:0.975 test loss: 10.501595\n",
      "4310: accuracy:0.98 loss: 6.077622\n",
      "4320: accuracy:0.99 loss: 2.8235717\n",
      "4330: accuracy:0.98 loss: 5.68089\n",
      "4340: accuracy:0.99 loss: 3.5910804\n",
      "4350: accuracy:0.99 loss: 2.454495\n",
      "4350: ********* epoch 8 ********* test accuracy:0.9767 test loss: 9.916198\n",
      "4360: accuracy:1.0 loss: 0.044750795\n",
      "4370: accuracy:0.99 loss: 1.4648776\n",
      "4380: accuracy:1.0 loss: 0.23293933\n",
      "4390: accuracy:0.99 loss: 1.644475\n",
      "4400: accuracy:1.0 loss: 0.54360783\n",
      "4400: ********* epoch 8 ********* test accuracy:0.976 test loss: 10.325972\n",
      "4410: accuracy:0.99 loss: 6.875802\n",
      "4420: accuracy:0.99 loss: 3.5743256\n",
      "4430: accuracy:0.98 loss: 2.8123355\n",
      "4440: accuracy:0.99 loss: 2.350961\n",
      "4450: accuracy:0.99 loss: 1.9044129\n",
      "4450: ********* epoch 8 ********* test accuracy:0.9784 test loss: 9.656935\n",
      "4460: accuracy:0.99 loss: 1.3171786\n",
      "4470: accuracy:0.99 loss: 2.4173136\n",
      "4480: accuracy:1.0 loss: 0.20567133\n",
      "4490: accuracy:0.99 loss: 3.7740455\n",
      "4500: accuracy:1.0 loss: 0.2462638\n",
      "4500: ********* epoch 8 ********* test accuracy:0.9782 test loss: 9.094315\n",
      "4510: accuracy:0.98 loss: 9.5647\n",
      "4520: accuracy:1.0 loss: 0.13657165\n",
      "4530: accuracy:1.0 loss: 0.18352693\n",
      "4540: accuracy:1.0 loss: 0.5134238\n",
      "4550: accuracy:0.99 loss: 2.3532786\n",
      "4550: ********* epoch 8 ********* test accuracy:0.9734 test loss: 11.185716\n",
      "4560: accuracy:0.98 loss: 3.8628564\n",
      "4570: accuracy:0.99 loss: 3.2558854\n",
      "4580: accuracy:1.0 loss: 1.3915685\n",
      "4590: accuracy:0.99 loss: 5.916159\n",
      "4600: accuracy:1.0 loss: 0.7343651\n",
      "4600: ********* epoch 8 ********* test accuracy:0.9734 test loss: 11.378894\n",
      "4610: accuracy:0.98 loss: 9.807615\n",
      "4620: accuracy:1.0 loss: 1.1549327\n",
      "4630: accuracy:1.0 loss: 0.12931442\n",
      "4640: accuracy:0.98 loss: 2.9984562\n",
      "4650: accuracy:0.98 loss: 6.137635\n",
      "4650: ********* epoch 8 ********* test accuracy:0.9766 test loss: 9.938352\n",
      "4660: accuracy:1.0 loss: 1.741708\n",
      "4670: accuracy:1.0 loss: 0.88894784\n",
      "4680: accuracy:0.99 loss: 2.82208\n",
      "4690: accuracy:0.97 loss: 9.479672\n",
      "4700: accuracy:0.99 loss: 1.4366897\n",
      "4700: ********* epoch 8 ********* test accuracy:0.9774 test loss: 9.775033\n",
      "4710: accuracy:0.98 loss: 3.5713806\n",
      "4720: accuracy:1.0 loss: 0.25457352\n",
      "4730: accuracy:0.99 loss: 1.9392977\n",
      "4740: accuracy:0.99 loss: 7.937125\n",
      "4750: accuracy:0.98 loss: 9.292754\n",
      "4750: ********* epoch 8 ********* test accuracy:0.975 test loss: 11.240458\n",
      "4760: accuracy:0.98 loss: 11.655869\n",
      "4770: accuracy:1.0 loss: 0.80599093\n",
      "4780: accuracy:1.0 loss: 0.62336624\n",
      "4790: accuracy:0.99 loss: 3.341423\n",
      "4800: accuracy:0.98 loss: 4.066913\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9773 test loss: 10.238619\n",
      "4810: accuracy:1.0 loss: 0.43832153\n",
      "4820: accuracy:0.99 loss: 1.2566535\n",
      "4830: accuracy:0.98 loss: 9.613926\n",
      "4840: accuracy:0.99 loss: 1.2807641\n",
      "4850: accuracy:1.0 loss: 0.134228\n",
      "4850: ********* epoch 9 ********* test accuracy:0.9789 test loss: 9.300771\n",
      "4860: accuracy:1.0 loss: 0.34818333\n",
      "4870: accuracy:1.0 loss: 0.8381655\n",
      "4880: accuracy:0.99 loss: 1.6064061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4890: accuracy:1.0 loss: 0.3095317\n",
      "4900: accuracy:0.97 loss: 8.163957\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9761 test loss: 11.306431\n",
      "4910: accuracy:1.0 loss: 0.66255057\n",
      "4920: accuracy:0.98 loss: 3.8313966\n",
      "4930: accuracy:1.0 loss: 0.69826746\n",
      "4940: accuracy:1.0 loss: 1.026401\n",
      "4950: accuracy:0.99 loss: 1.2314595\n",
      "4950: ********* epoch 9 ********* test accuracy:0.9811 test loss: 9.62827\n",
      "4960: accuracy:1.0 loss: 0.3000065\n",
      "4970: accuracy:0.99 loss: 1.2924182\n",
      "4980: accuracy:1.0 loss: 0.26114854\n",
      "4990: accuracy:0.98 loss: 4.5055947\n",
      "5000: accuracy:0.98 loss: 4.675899\n",
      "5000: ********* epoch 9 ********* test accuracy:0.9789 test loss: 10.088744\n",
      "5010: accuracy:0.99 loss: 1.7075268\n",
      "5020: accuracy:1.0 loss: 0.22523813\n",
      "5030: accuracy:1.0 loss: 0.30315584\n",
      "5040: accuracy:0.99 loss: 5.738621\n",
      "5050: accuracy:1.0 loss: 0.1724804\n",
      "5050: ********* epoch 9 ********* test accuracy:0.9796 test loss: 9.908318\n",
      "5060: accuracy:0.99 loss: 4.243024\n",
      "5070: accuracy:0.98 loss: 5.9736366\n",
      "5080: accuracy:1.0 loss: 0.55391884\n",
      "5090: accuracy:1.0 loss: 0.034273967\n",
      "5100: accuracy:0.99 loss: 4.143389\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9791 test loss: 10.014505\n",
      "5110: accuracy:0.99 loss: 0.8769259\n",
      "5120: accuracy:0.99 loss: 5.94987\n",
      "5130: accuracy:0.99 loss: 2.1317492\n",
      "5140: accuracy:0.99 loss: 3.012342\n",
      "5150: accuracy:1.0 loss: 0.17074719\n",
      "5150: ********* epoch 9 ********* test accuracy:0.9729 test loss: 12.062522\n",
      "5160: accuracy:1.0 loss: 0.25787216\n",
      "5170: accuracy:0.98 loss: 7.241717\n",
      "5180: accuracy:1.0 loss: 0.17130661\n",
      "5190: accuracy:0.99 loss: 1.5405496\n",
      "5200: accuracy:0.99 loss: 2.6635432\n",
      "5200: ********* epoch 9 ********* test accuracy:0.9739 test loss: 12.48689\n",
      "5210: accuracy:1.0 loss: 0.92784536\n",
      "5220: accuracy:0.99 loss: 1.5770316\n",
      "5230: accuracy:1.0 loss: 1.0004871\n",
      "5240: accuracy:0.98 loss: 4.2427216\n",
      "5250: accuracy:0.99 loss: 3.2579594\n",
      "5250: ********* epoch 9 ********* test accuracy:0.9787 test loss: 10.117497\n",
      "5260: accuracy:0.99 loss: 0.93670356\n",
      "5270: accuracy:0.99 loss: 1.5906075\n",
      "5280: accuracy:0.99 loss: 1.0562382\n",
      "5290: accuracy:0.99 loss: 1.5132924\n",
      "5300: accuracy:1.0 loss: 0.85018307\n",
      "5300: ********* epoch 9 ********* test accuracy:0.9759 test loss: 10.451018\n",
      "5310: accuracy:0.99 loss: 4.5640244\n",
      "5320: accuracy:0.99 loss: 2.1619706\n",
      "5330: accuracy:0.98 loss: 6.995413\n",
      "5340: accuracy:0.99 loss: 2.052827\n",
      "5350: accuracy:0.99 loss: 1.1093304\n",
      "5350: ********* epoch 9 ********* test accuracy:0.9759 test loss: 10.548781\n",
      "5360: accuracy:0.99 loss: 1.8926611\n",
      "5370: accuracy:0.99 loss: 3.7459984\n",
      "5380: accuracy:1.0 loss: 0.42982286\n",
      "5390: accuracy:0.99 loss: 1.2386669\n",
      "5400: accuracy:1.0 loss: 0.3589672\n",
      "5400: ********* epoch 10 ********* test accuracy:0.9748 test loss: 11.09001\n",
      "5410: accuracy:0.99 loss: 1.9496149\n",
      "5420: accuracy:0.98 loss: 6.018581\n",
      "5430: accuracy:1.0 loss: 0.1621605\n",
      "5440: accuracy:1.0 loss: 0.4150356\n",
      "5450: accuracy:0.99 loss: 2.9366045\n",
      "5450: ********* epoch 10 ********* test accuracy:0.9767 test loss: 11.6845875\n",
      "5460: accuracy:0.99 loss: 2.0229545\n",
      "5470: accuracy:1.0 loss: 0.1067559\n",
      "5480: accuracy:1.0 loss: 1.5798758\n",
      "5490: accuracy:1.0 loss: 1.6522589\n",
      "5500: accuracy:1.0 loss: 0.35245463\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9777 test loss: 10.484019\n",
      "5510: accuracy:0.99 loss: 4.4462004\n",
      "5520: accuracy:0.99 loss: 1.11481\n",
      "5530: accuracy:0.99 loss: 1.0688064\n",
      "5540: accuracy:1.0 loss: 0.4166957\n",
      "5550: accuracy:0.98 loss: 4.200682\n",
      "5550: ********* epoch 10 ********* test accuracy:0.976 test loss: 12.397866\n",
      "5560: accuracy:1.0 loss: 0.07480379\n",
      "5570: accuracy:0.98 loss: 4.6209373\n",
      "5580: accuracy:0.98 loss: 3.413436\n",
      "5590: accuracy:0.99 loss: 0.8635779\n",
      "5600: accuracy:1.0 loss: 0.025265029\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9793 test loss: 10.959471\n",
      "5610: accuracy:1.0 loss: 0.07403571\n",
      "5620: accuracy:0.98 loss: 2.5423412\n",
      "5630: accuracy:0.99 loss: 2.4738367\n",
      "5640: accuracy:1.0 loss: 0.4848949\n",
      "5650: accuracy:0.99 loss: 3.458023\n",
      "5650: ********* epoch 10 ********* test accuracy:0.9758 test loss: 11.568533\n",
      "5660: accuracy:0.99 loss: 1.6392616\n",
      "5670: accuracy:0.98 loss: 2.0737023\n",
      "5680: accuracy:0.98 loss: 2.589982\n",
      "5690: accuracy:0.98 loss: 4.50856\n",
      "5700: accuracy:1.0 loss: 0.15400025\n",
      "5700: ********* epoch 10 ********* test accuracy:0.9775 test loss: 12.266579\n",
      "5710: accuracy:0.99 loss: 1.7229836\n",
      "5720: accuracy:0.99 loss: 5.229178\n",
      "5730: accuracy:0.98 loss: 5.6528387\n",
      "5740: accuracy:0.98 loss: 4.1717796\n",
      "5750: accuracy:0.99 loss: 2.9782763\n",
      "5750: ********* epoch 10 ********* test accuracy:0.98 test loss: 9.727037\n",
      "5760: accuracy:0.98 loss: 8.914597\n",
      "5770: accuracy:0.97 loss: 2.5505254\n",
      "5780: accuracy:0.99 loss: 4.041715\n",
      "5790: accuracy:1.0 loss: 0.116075926\n",
      "5800: accuracy:1.0 loss: 0.97557986\n",
      "5800: ********* epoch 10 ********* test accuracy:0.9801 test loss: 8.766517\n",
      "5810: accuracy:0.98 loss: 4.6842275\n",
      "5820: accuracy:1.0 loss: 0.406083\n",
      "5830: accuracy:0.99 loss: 1.7187947\n",
      "5840: accuracy:0.99 loss: 0.9208846\n",
      "5850: accuracy:0.99 loss: 7.215431\n",
      "5850: ********* epoch 10 ********* test accuracy:0.9768 test loss: 10.7014265\n",
      "5860: accuracy:0.99 loss: 11.995817\n",
      "5870: accuracy:1.0 loss: 0.35767654\n",
      "5880: accuracy:1.0 loss: 0.7288178\n",
      "5890: accuracy:0.98 loss: 5.7648225\n",
      "5900: accuracy:0.99 loss: 2.2252057\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9722 test loss: 11.901746\n",
      "5910: accuracy:0.99 loss: 1.4457402\n",
      "5920: accuracy:0.98 loss: 3.2536058\n",
      "5930: accuracy:1.0 loss: 1.0055996\n",
      "5940: accuracy:0.99 loss: 1.950154\n",
      "5950: accuracy:0.99 loss: 1.0042148\n",
      "5950: ********* epoch 10 ********* test accuracy:0.9762 test loss: 10.825291\n",
      "5960: accuracy:1.0 loss: 0.12948579\n",
      "5970: accuracy:1.0 loss: 1.0434457\n",
      "5980: accuracy:0.99 loss: 2.6728835\n",
      "5990: accuracy:0.99 loss: 2.9911914\n",
      "6000: accuracy:0.98 loss: 2.644136\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9778 test loss: 9.4587345\n",
      "6010: accuracy:1.0 loss: 1.6465694\n",
      "6020: accuracy:1.0 loss: 0.8284092\n",
      "6030: accuracy:1.0 loss: 0.14304684\n",
      "6040: accuracy:0.98 loss: 12.010183\n",
      "6050: accuracy:1.0 loss: 0.17611285\n",
      "6050: ********* epoch 11 ********* test accuracy:0.9774 test loss: 9.89903\n",
      "6060: accuracy:0.98 loss: 6.9613857\n",
      "6070: accuracy:1.0 loss: 0.7174294\n",
      "6080: accuracy:1.0 loss: 0.5039304\n",
      "6090: accuracy:1.0 loss: 0.7214501\n",
      "6100: accuracy:1.0 loss: 0.16048634\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9798 test loss: 10.107649\n",
      "6110: accuracy:0.99 loss: 1.3132781\n",
      "6120: accuracy:1.0 loss: 1.3881917\n",
      "6130: accuracy:1.0 loss: 0.05172547\n",
      "6140: accuracy:0.99 loss: 2.7746267\n",
      "6150: accuracy:1.0 loss: 0.28603098\n",
      "6150: ********* epoch 11 ********* test accuracy:0.9783 test loss: 10.663074\n",
      "6160: accuracy:1.0 loss: 0.5741023\n",
      "6170: accuracy:1.0 loss: 0.24920508\n",
      "6180: accuracy:0.99 loss: 2.4631493\n",
      "6190: accuracy:1.0 loss: 0.68667173\n",
      "6200: accuracy:1.0 loss: 0.047624763\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9801 test loss: 9.629454\n",
      "6210: accuracy:0.99 loss: 4.773563\n",
      "6220: accuracy:0.99 loss: 2.6574614\n",
      "6230: accuracy:0.99 loss: 2.3951592\n",
      "6240: accuracy:1.0 loss: 1.0130234\n",
      "6250: accuracy:1.0 loss: 0.74266005\n",
      "6250: ********* epoch 11 ********* test accuracy:0.9781 test loss: 10.3136215\n",
      "6260: accuracy:1.0 loss: 0.5122761\n",
      "6270: accuracy:1.0 loss: 0.8074923\n",
      "6280: accuracy:1.0 loss: 0.01225145\n",
      "6290: accuracy:0.98 loss: 1.6464201\n",
      "6300: accuracy:0.99 loss: 1.7438896\n",
      "6300: ********* epoch 11 ********* test accuracy:0.9813 test loss: 9.693028\n",
      "6310: accuracy:0.99 loss: 1.7358807\n",
      "6320: accuracy:0.99 loss: 2.0791745\n",
      "6330: accuracy:0.99 loss: 6.924126\n",
      "6340: accuracy:1.0 loss: 0.020417884\n",
      "6350: accuracy:1.0 loss: 0.08260083\n",
      "6350: ********* epoch 11 ********* test accuracy:0.9783 test loss: 10.436497\n",
      "6360: accuracy:1.0 loss: 0.28164592\n",
      "6370: accuracy:0.97 loss: 4.883004\n",
      "6380: accuracy:1.0 loss: 0.1232886\n",
      "6390: accuracy:1.0 loss: 0.16489755\n",
      "6400: accuracy:0.99 loss: 2.7323244\n",
      "6400: ********* epoch 11 ********* test accuracy:0.9796 test loss: 9.695046\n",
      "6410: accuracy:0.99 loss: 1.4645996\n",
      "6420: accuracy:0.99 loss: 1.9737684\n",
      "6430: accuracy:1.0 loss: 1.2268919\n",
      "6440: accuracy:0.98 loss: 5.5123143\n",
      "6450: accuracy:1.0 loss: 0.47238442\n",
      "6450: ********* epoch 11 ********* test accuracy:0.9781 test loss: 11.609219\n",
      "6460: accuracy:0.98 loss: 4.5000615\n",
      "6470: accuracy:1.0 loss: 0.09506664\n",
      "6480: accuracy:1.0 loss: 0.26180875\n",
      "6490: accuracy:1.0 loss: 0.7866365\n",
      "6500: accuracy:0.98 loss: 5.7346015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500: ********* epoch 11 ********* test accuracy:0.9779 test loss: 10.605303\n",
      "6510: accuracy:0.98 loss: 11.347583\n",
      "6520: accuracy:1.0 loss: 0.1135892\n",
      "6530: accuracy:0.99 loss: 3.463741\n",
      "6540: accuracy:0.98 loss: 4.6114902\n",
      "6550: accuracy:1.0 loss: 0.17447527\n",
      "6550: ********* epoch 11 ********* test accuracy:0.9759 test loss: 11.640007\n",
      "6560: accuracy:0.99 loss: 1.8978513\n",
      "6570: accuracy:0.99 loss: 2.6105855\n",
      "6580: accuracy:0.99 loss: 1.2393498\n",
      "6590: accuracy:0.98 loss: 5.537546\n",
      "6600: accuracy:1.0 loss: 0.23052292\n",
      "6600: ********* epoch 12 ********* test accuracy:0.9776 test loss: 11.482827\n",
      "6610: accuracy:1.0 loss: 0.8972332\n",
      "6620: accuracy:0.99 loss: 1.4628918\n",
      "6630: accuracy:1.0 loss: 0.1422891\n",
      "6640: accuracy:0.99 loss: 8.000912\n",
      "6650: accuracy:1.0 loss: 0.57449675\n",
      "6650: ********* epoch 12 ********* test accuracy:0.9794 test loss: 11.538538\n",
      "6660: accuracy:0.98 loss: 4.425406\n",
      "6670: accuracy:1.0 loss: 0.6602522\n",
      "6680: accuracy:1.0 loss: 0.21778151\n",
      "6690: accuracy:1.0 loss: 0.48549885\n",
      "6700: accuracy:0.99 loss: 1.587171\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9773 test loss: 11.472193\n",
      "6710: accuracy:0.99 loss: 1.0882802\n",
      "6720: accuracy:0.99 loss: 3.0865753\n",
      "6730: accuracy:0.99 loss: 2.2276442\n",
      "6740: accuracy:0.99 loss: 2.3969932\n",
      "6750: accuracy:1.0 loss: 0.1563672\n",
      "6750: ********* epoch 12 ********* test accuracy:0.9786 test loss: 10.882951\n",
      "6760: accuracy:1.0 loss: 0.6471875\n",
      "6770: accuracy:1.0 loss: 1.467407\n",
      "6780: accuracy:1.0 loss: 0.5169377\n",
      "6790: accuracy:0.99 loss: 1.707032\n",
      "6800: accuracy:1.0 loss: 0.09632205\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9771 test loss: 12.130514\n",
      "6810: accuracy:0.99 loss: 2.1381829\n",
      "6820: accuracy:1.0 loss: 0.6620597\n",
      "6830: accuracy:0.99 loss: 2.419573\n",
      "6840: accuracy:0.99 loss: 4.3884945\n",
      "6850: accuracy:1.0 loss: 0.750541\n",
      "6850: ********* epoch 12 ********* test accuracy:0.9766 test loss: 13.06052\n",
      "6860: accuracy:0.98 loss: 3.2058344\n",
      "6870: accuracy:0.99 loss: 1.4148736\n",
      "6880: accuracy:1.0 loss: 0.14888965\n",
      "6890: accuracy:0.99 loss: 2.5234914\n",
      "6900: accuracy:0.99 loss: 7.063423\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9787 test loss: 11.51343\n",
      "6910: accuracy:1.0 loss: 0.1079692\n",
      "6920: accuracy:0.99 loss: 7.1483316\n",
      "6930: accuracy:1.0 loss: 1.1457615\n",
      "6940: accuracy:0.99 loss: 2.7693977\n",
      "6950: accuracy:0.98 loss: 2.4946592\n",
      "6950: ********* epoch 12 ********* test accuracy:0.9732 test loss: 13.387124\n",
      "6960: accuracy:0.99 loss: 1.7670147\n",
      "6970: accuracy:1.0 loss: 0.019060936\n",
      "6980: accuracy:1.0 loss: 0.10333898\n",
      "6990: accuracy:1.0 loss: 0.22500703\n",
      "7000: accuracy:0.99 loss: 1.9510833\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9798 test loss: 10.823868\n",
      "7010: accuracy:0.98 loss: 9.201176\n",
      "7020: accuracy:1.0 loss: 0.4012589\n",
      "7030: accuracy:0.99 loss: 2.872183\n",
      "7040: accuracy:1.0 loss: 1.0387441\n",
      "7050: accuracy:1.0 loss: 0.24029724\n",
      "7050: ********* epoch 12 ********* test accuracy:0.9781 test loss: 11.2442875\n",
      "7060: accuracy:1.0 loss: 0.053289667\n",
      "7070: accuracy:1.0 loss: 0.23410353\n",
      "7080: accuracy:0.98 loss: 10.661441\n",
      "7090: accuracy:0.98 loss: 9.019474\n",
      "7100: accuracy:1.0 loss: 0.40341204\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9784 test loss: 11.684848\n",
      "7110: accuracy:0.97 loss: 5.1182375\n",
      "7120: accuracy:1.0 loss: 0.044061724\n",
      "7130: accuracy:1.0 loss: 0.1068756\n",
      "7140: accuracy:0.99 loss: 2.8786192\n",
      "7150: accuracy:1.0 loss: 0.11798935\n",
      "7150: ********* epoch 12 ********* test accuracy:0.9813 test loss: 9.385835\n",
      "7160: accuracy:0.99 loss: 9.755168\n",
      "7170: accuracy:0.98 loss: 7.6332273\n",
      "7180: accuracy:0.99 loss: 5.9593463\n",
      "7190: accuracy:1.0 loss: 0.37575388\n",
      "7200: accuracy:0.99 loss: 1.2555965\n",
      "7200: ********* epoch 13 ********* test accuracy:0.98 test loss: 9.446112\n",
      "7210: accuracy:1.0 loss: 0.4119815\n",
      "7220: accuracy:1.0 loss: 0.8966524\n",
      "7230: accuracy:1.0 loss: 0.14208427\n",
      "7240: accuracy:1.0 loss: 0.048372284\n",
      "7250: accuracy:1.0 loss: 0.14743465\n",
      "7250: ********* epoch 13 ********* test accuracy:0.9807 test loss: 9.790206\n",
      "7260: accuracy:1.0 loss: 0.17911494\n",
      "7270: accuracy:1.0 loss: 0.04920915\n",
      "7280: accuracy:1.0 loss: 0.20589912\n",
      "7290: accuracy:0.99 loss: 1.0257077\n",
      "7300: accuracy:0.99 loss: 3.9688592\n",
      "7300: ********* epoch 13 ********* test accuracy:0.9787 test loss: 10.779983\n",
      "7310: accuracy:1.0 loss: 0.20948341\n",
      "7320: accuracy:1.0 loss: 0.2761244\n",
      "7330: accuracy:1.0 loss: 0.17123193\n",
      "7340: accuracy:1.0 loss: 0.10672069\n",
      "7350: accuracy:0.99 loss: 7.7986984\n",
      "7350: ********* epoch 13 ********* test accuracy:0.9791 test loss: 10.499124\n",
      "7360: accuracy:1.0 loss: 0.25434288\n",
      "7370: accuracy:1.0 loss: 0.2794582\n",
      "7380: accuracy:1.0 loss: 0.69769067\n",
      "7390: accuracy:1.0 loss: 0.15490294\n",
      "7400: accuracy:1.0 loss: 0.9016093\n",
      "7400: ********* epoch 13 ********* test accuracy:0.977 test loss: 11.27157\n",
      "7410: accuracy:0.99 loss: 2.3858166\n",
      "7420: accuracy:0.99 loss: 3.6768622\n",
      "7430: accuracy:0.99 loss: 4.115409\n",
      "7440: accuracy:1.0 loss: 0.9558115\n",
      "7450: accuracy:1.0 loss: 0.0018471725\n",
      "7450: ********* epoch 13 ********* test accuracy:0.9806 test loss: 10.346231\n",
      "7460: accuracy:1.0 loss: 0.15474425\n",
      "7470: accuracy:1.0 loss: 1.392894\n",
      "7480: accuracy:1.0 loss: 0.1343625\n",
      "7490: accuracy:0.99 loss: 4.543976\n",
      "7500: accuracy:1.0 loss: 0.06976609\n",
      "7500: ********* epoch 13 ********* test accuracy:0.9785 test loss: 12.178275\n",
      "7510: accuracy:0.99 loss: 2.35883\n",
      "7520: accuracy:1.0 loss: 0.28456426\n",
      "7530: accuracy:0.99 loss: 1.4325552\n",
      "7540: accuracy:0.99 loss: 5.81827\n",
      "7550: accuracy:0.99 loss: 2.3217819\n",
      "7550: ********* epoch 13 ********* test accuracy:0.9772 test loss: 12.066413\n",
      "7560: accuracy:0.98 loss: 1.4570007\n",
      "7570: accuracy:1.0 loss: 0.8680612\n",
      "7580: accuracy:1.0 loss: 0.6040594\n",
      "7590: accuracy:1.0 loss: 0.13332587\n",
      "7600: accuracy:1.0 loss: 1.249657\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9786 test loss: 11.380296\n",
      "7610: accuracy:1.0 loss: 0.033682432\n",
      "7620: accuracy:1.0 loss: 1.125601\n",
      "7630: accuracy:1.0 loss: 0.14456378\n",
      "7640: accuracy:1.0 loss: 0.7278975\n",
      "7650: accuracy:0.98 loss: 6.69458\n",
      "7650: ********* epoch 13 ********* test accuracy:0.9743 test loss: 13.9348955\n",
      "7660: accuracy:1.0 loss: 0.79795456\n",
      "7670: accuracy:1.0 loss: 0.37004864\n",
      "7680: accuracy:1.0 loss: 0.21157302\n",
      "7690: accuracy:0.99 loss: 1.5828775\n",
      "7700: accuracy:0.99 loss: 1.261131\n",
      "7700: ********* epoch 13 ********* test accuracy:0.978 test loss: 11.457104\n",
      "7710: accuracy:1.0 loss: 0.6379626\n",
      "7720: accuracy:1.0 loss: 0.3213358\n",
      "7730: accuracy:0.98 loss: 7.5808673\n",
      "7740: accuracy:0.97 loss: 12.890933\n",
      "7750: accuracy:0.99 loss: 1.2745924\n",
      "7750: ********* epoch 13 ********* test accuracy:0.978 test loss: 11.715474\n",
      "7760: accuracy:0.99 loss: 4.596999\n",
      "7770: accuracy:1.0 loss: 0.04568423\n",
      "7780: accuracy:1.0 loss: 0.61584944\n",
      "7790: accuracy:1.0 loss: 0.04469886\n",
      "7800: accuracy:0.99 loss: 3.358837\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9745 test loss: 13.474973\n",
      "7810: accuracy:1.0 loss: 0.16534965\n",
      "7820: accuracy:1.0 loss: 0.09178911\n",
      "7830: accuracy:1.0 loss: 0.13100047\n",
      "7840: accuracy:1.0 loss: 0.010585228\n",
      "7850: accuracy:1.0 loss: 1.2344341\n",
      "7850: ********* epoch 14 ********* test accuracy:0.9757 test loss: 12.016251\n",
      "7860: accuracy:1.0 loss: 1.1902032\n",
      "7870: accuracy:1.0 loss: 0.3017581\n",
      "7880: accuracy:0.99 loss: 6.5489063\n",
      "7890: accuracy:1.0 loss: 0.49420238\n",
      "7900: accuracy:1.0 loss: 0.4659835\n",
      "7900: ********* epoch 14 ********* test accuracy:0.978 test loss: 11.392353\n",
      "7910: accuracy:1.0 loss: 0.6107354\n",
      "7920: accuracy:1.0 loss: 0.88149476\n",
      "7930: accuracy:0.99 loss: 2.2351537\n",
      "7940: accuracy:1.0 loss: 0.6987311\n",
      "7950: accuracy:0.99 loss: 3.2508693\n",
      "7950: ********* epoch 14 ********* test accuracy:0.9753 test loss: 13.327514\n",
      "7960: accuracy:1.0 loss: 0.09688952\n",
      "7970: accuracy:1.0 loss: 0.48482832\n",
      "7980: accuracy:0.99 loss: 3.277557\n",
      "7990: accuracy:1.0 loss: 0.46682978\n",
      "8000: accuracy:0.99 loss: 1.6888568\n",
      "8000: ********* epoch 14 ********* test accuracy:0.9788 test loss: 11.052628\n",
      "8010: accuracy:0.99 loss: 1.2097493\n",
      "8020: accuracy:0.99 loss: 2.3956642\n",
      "8030: accuracy:1.0 loss: 0.26523954\n",
      "8040: accuracy:1.0 loss: 1.0126308\n",
      "8050: accuracy:1.0 loss: 0.15735559\n",
      "8050: ********* epoch 14 ********* test accuracy:0.981 test loss: 10.361343\n",
      "8060: accuracy:1.0 loss: 0.17302406\n",
      "8070: accuracy:1.0 loss: 0.19195034\n",
      "8080: accuracy:0.98 loss: 2.783599\n",
      "8090: accuracy:1.0 loss: 0.3683471\n",
      "8100: accuracy:0.99 loss: 3.6549563\n",
      "8100: ********* epoch 14 ********* test accuracy:0.9794 test loss: 10.483482\n",
      "8110: accuracy:0.99 loss: 3.2079022\n",
      "8120: accuracy:1.0 loss: 0.3293678\n",
      "8130: accuracy:1.0 loss: 0.5402428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8140: accuracy:0.99 loss: 1.30599\n",
      "8150: accuracy:1.0 loss: 0.0697906\n",
      "8150: ********* epoch 14 ********* test accuracy:0.9793 test loss: 11.294389\n",
      "8160: accuracy:0.99 loss: 3.521832\n",
      "8170: accuracy:1.0 loss: 0.31979635\n",
      "8180: accuracy:1.0 loss: 0.041070893\n",
      "8190: accuracy:0.99 loss: 2.496924\n",
      "8200: accuracy:1.0 loss: 0.31369665\n",
      "8200: ********* epoch 14 ********* test accuracy:0.9801 test loss: 11.312322\n",
      "8210: accuracy:0.99 loss: 2.3397653\n",
      "8220: accuracy:0.97 loss: 4.376385\n",
      "8230: accuracy:0.99 loss: 3.6699095\n",
      "8240: accuracy:1.0 loss: 0.1547966\n",
      "8250: accuracy:1.0 loss: 0.072865814\n",
      "8250: ********* epoch 14 ********* test accuracy:0.9773 test loss: 12.359419\n",
      "8260: accuracy:1.0 loss: 0.03233425\n",
      "8270: accuracy:0.99 loss: 3.6002982\n",
      "8280: accuracy:1.0 loss: 0.24858853\n",
      "8290: accuracy:0.97 loss: 10.286542\n",
      "8300: accuracy:1.0 loss: 0.10879719\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9782 test loss: 12.900394\n",
      "8310: accuracy:1.0 loss: 0.068682045\n",
      "8320: accuracy:1.0 loss: 0.32150036\n",
      "8330: accuracy:0.99 loss: 2.5916789\n",
      "8340: accuracy:0.99 loss: 2.0663185\n",
      "8350: accuracy:1.0 loss: 0.5198185\n",
      "8350: ********* epoch 14 ********* test accuracy:0.9789 test loss: 12.183784\n",
      "8360: accuracy:1.0 loss: 0.096843004\n",
      "8370: accuracy:1.0 loss: 0.88042134\n",
      "8380: accuracy:0.98 loss: 12.002067\n",
      "8390: accuracy:0.99 loss: 3.492859\n",
      "8400: accuracy:1.0 loss: 0.42214775\n",
      "8400: ********* epoch 15 ********* test accuracy:0.9744 test loss: 13.110916\n",
      "8410: accuracy:1.0 loss: 0.67146415\n",
      "8420: accuracy:1.0 loss: 0.35696235\n",
      "8430: accuracy:1.0 loss: 0.97408986\n",
      "8440: accuracy:1.0 loss: 0.5705366\n",
      "8450: accuracy:1.0 loss: 0.13538164\n",
      "8450: ********* epoch 15 ********* test accuracy:0.9795 test loss: 11.443476\n",
      "8460: accuracy:0.99 loss: 4.199991\n",
      "8470: accuracy:1.0 loss: 0.22841811\n",
      "8480: accuracy:1.0 loss: 0.72775257\n",
      "8490: accuracy:1.0 loss: 0.027555771\n",
      "8500: accuracy:0.98 loss: 6.278621\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9774 test loss: 11.243431\n",
      "8510: accuracy:1.0 loss: 0.29507455\n",
      "8520: accuracy:1.0 loss: 0.04162754\n",
      "8530: accuracy:1.0 loss: 0.071093604\n",
      "8540: accuracy:1.0 loss: 0.0062817517\n",
      "8550: accuracy:1.0 loss: 0.596977\n",
      "8550: ********* epoch 15 ********* test accuracy:0.9782 test loss: 11.515639\n",
      "8560: accuracy:1.0 loss: 0.11369263\n",
      "8570: accuracy:0.98 loss: 8.128207\n",
      "8580: accuracy:1.0 loss: 0.025070768\n",
      "8590: accuracy:0.99 loss: 2.246193\n",
      "8600: accuracy:1.0 loss: 0.009010421\n",
      "8600: ********* epoch 15 ********* test accuracy:0.9753 test loss: 14.253733\n",
      "8610: accuracy:1.0 loss: 0.016450439\n",
      "8620: accuracy:1.0 loss: 0.42400867\n",
      "8630: accuracy:1.0 loss: 0.2697776\n",
      "8640: accuracy:1.0 loss: 0.7660226\n",
      "8650: accuracy:0.98 loss: 3.1204963\n",
      "8650: ********* epoch 15 ********* test accuracy:0.9774 test loss: 12.316593\n",
      "8660: accuracy:0.98 loss: 2.393054\n",
      "8670: accuracy:1.0 loss: 0.19192763\n",
      "8680: accuracy:1.0 loss: 0.05560598\n",
      "8690: accuracy:1.0 loss: 0.6998924\n",
      "8700: accuracy:1.0 loss: 0.076831326\n",
      "8700: ********* epoch 15 ********* test accuracy:0.9814 test loss: 11.11824\n",
      "8710: accuracy:0.99 loss: 5.280304\n",
      "8720: accuracy:1.0 loss: 0.020377249\n",
      "8730: accuracy:0.99 loss: 7.156112\n",
      "8740: accuracy:1.0 loss: 0.34177867\n",
      "8750: accuracy:0.99 loss: 4.048396\n",
      "8750: ********* epoch 15 ********* test accuracy:0.9787 test loss: 12.087153\n",
      "8760: accuracy:1.0 loss: 1.3085296\n",
      "8770: accuracy:1.0 loss: 0.028181598\n",
      "8780: accuracy:1.0 loss: 0.6606835\n",
      "8790: accuracy:1.0 loss: 0.25630108\n",
      "8800: accuracy:1.0 loss: 0.29364973\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9791 test loss: 11.411638\n",
      "8810: accuracy:0.99 loss: 6.7216406\n",
      "8820: accuracy:0.98 loss: 3.3482351\n",
      "8830: accuracy:1.0 loss: 0.24223654\n",
      "8840: accuracy:1.0 loss: 0.25255507\n",
      "8850: accuracy:1.0 loss: 0.21672332\n",
      "8850: ********* epoch 15 ********* test accuracy:0.979 test loss: 12.108564\n",
      "8860: accuracy:0.99 loss: 1.7423382\n",
      "8870: accuracy:0.98 loss: 3.9203093\n",
      "8880: accuracy:1.0 loss: 0.042021528\n",
      "8890: accuracy:1.0 loss: 0.03071035\n",
      "8900: accuracy:1.0 loss: 0.64387906\n",
      "8900: ********* epoch 15 ********* test accuracy:0.9764 test loss: 13.453094\n",
      "8910: accuracy:0.98 loss: 4.5465636\n",
      "8920: accuracy:1.0 loss: 0.15230793\n",
      "8930: accuracy:1.0 loss: 0.15338744\n",
      "8940: accuracy:0.97 loss: 11.469507\n",
      "8950: accuracy:0.99 loss: 1.0333915\n",
      "8950: ********* epoch 15 ********* test accuracy:0.9722 test loss: 15.319043\n",
      "8960: accuracy:1.0 loss: 0.1062561\n",
      "8970: accuracy:0.97 loss: 4.2681775\n",
      "8980: accuracy:1.0 loss: 0.32272068\n",
      "8990: accuracy:0.98 loss: 15.858819\n",
      "9000: accuracy:1.0 loss: 0.022149663\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9758 test loss: 14.090021\n",
      "9010: accuracy:1.0 loss: 0.7288266\n",
      "9020: accuracy:0.99 loss: 5.6920185\n",
      "9030: accuracy:1.0 loss: 0.21289197\n",
      "9040: accuracy:1.0 loss: 0.048154633\n",
      "9050: accuracy:0.98 loss: 11.42298\n",
      "9050: ********* epoch 16 ********* test accuracy:0.9757 test loss: 13.951135\n",
      "9060: accuracy:0.99 loss: 4.345792\n",
      "9070: accuracy:1.0 loss: 0.39542383\n",
      "9080: accuracy:1.0 loss: 0.64725024\n",
      "9090: accuracy:0.99 loss: 0.8385112\n",
      "9100: accuracy:1.0 loss: 0.04919305\n",
      "9100: ********* epoch 16 ********* test accuracy:0.98 test loss: 11.778112\n",
      "9110: accuracy:1.0 loss: 0.04554181\n",
      "9120: accuracy:1.0 loss: 0.4759723\n",
      "9130: accuracy:1.0 loss: 0.2806109\n",
      "9140: accuracy:0.99 loss: 1.8573271\n",
      "9150: accuracy:1.0 loss: 0.015235165\n",
      "9150: ********* epoch 16 ********* test accuracy:0.9801 test loss: 12.189482\n",
      "9160: accuracy:0.99 loss: 1.8459228\n",
      "9170: accuracy:1.0 loss: 0.3433264\n",
      "9180: accuracy:1.0 loss: 0.109155014\n",
      "9190: accuracy:1.0 loss: 0.030778538\n",
      "9200: accuracy:0.99 loss: 3.6613274\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9781 test loss: 12.455488\n",
      "9210: accuracy:1.0 loss: 0.29273474\n",
      "9220: accuracy:1.0 loss: 0.14609483\n",
      "9230: accuracy:0.98 loss: 2.7075026\n",
      "9240: accuracy:0.99 loss: 1.2286072\n",
      "9250: accuracy:1.0 loss: 0.006133851\n",
      "9250: ********* epoch 16 ********* test accuracy:0.9793 test loss: 12.643689\n",
      "9260: accuracy:0.99 loss: 2.2186859\n",
      "9270: accuracy:1.0 loss: 0.086961284\n",
      "9280: accuracy:0.99 loss: 5.978377\n",
      "9290: accuracy:0.97 loss: 5.434685\n",
      "9300: accuracy:1.0 loss: 0.05870597\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9809 test loss: 12.29677\n",
      "9310: accuracy:1.0 loss: 0.3712142\n",
      "9320: accuracy:1.0 loss: 0.64938045\n",
      "9330: accuracy:1.0 loss: 0.19255206\n",
      "9340: accuracy:1.0 loss: 0.025732594\n",
      "9350: accuracy:0.99 loss: 2.501005\n",
      "9350: ********* epoch 16 ********* test accuracy:0.979 test loss: 13.020022\n",
      "9360: accuracy:1.0 loss: 0.109240666\n",
      "9370: accuracy:0.99 loss: 6.2143426\n",
      "9380: accuracy:0.98 loss: 7.4237475\n",
      "9390: accuracy:0.98 loss: 3.7917926\n",
      "9400: accuracy:0.99 loss: 3.2027853\n",
      "9400: ********* epoch 16 ********* test accuracy:0.9749 test loss: 15.599017\n",
      "9410: accuracy:0.99 loss: 4.775309\n",
      "9420: accuracy:1.0 loss: 0.058582\n",
      "9430: accuracy:0.99 loss: 1.9331151\n",
      "9440: accuracy:1.0 loss: 1.2200552\n",
      "9450: accuracy:1.0 loss: 0.022715714\n",
      "9450: ********* epoch 16 ********* test accuracy:0.9759 test loss: 14.1087475\n",
      "9460: accuracy:0.99 loss: 2.9933085\n",
      "9470: accuracy:0.99 loss: 1.4485751\n",
      "9480: accuracy:1.0 loss: 0.14994329\n",
      "9490: accuracy:0.99 loss: 3.5931983\n",
      "9500: accuracy:0.99 loss: 1.9428147\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9791 test loss: 11.90237\n",
      "9510: accuracy:1.0 loss: 0.14556918\n",
      "9520: accuracy:0.99 loss: 2.5573869\n",
      "9530: accuracy:1.0 loss: 0.13705057\n",
      "9540: accuracy:0.99 loss: 1.801308\n",
      "9550: accuracy:0.99 loss: 7.765425\n",
      "9550: ********* epoch 16 ********* test accuracy:0.9783 test loss: 13.30498\n",
      "9560: accuracy:0.99 loss: 1.1783006\n",
      "9570: accuracy:1.0 loss: 0.5937443\n",
      "9580: accuracy:0.99 loss: 6.339825\n",
      "9590: accuracy:0.98 loss: 8.778798\n",
      "9600: accuracy:1.0 loss: 0.31989896\n",
      "9600: ********* epoch 17 ********* test accuracy:0.9744 test loss: 14.765954\n",
      "9610: accuracy:0.97 loss: 4.0183935\n",
      "9620: accuracy:1.0 loss: 0.19052091\n",
      "9630: accuracy:1.0 loss: 0.08658809\n",
      "9640: accuracy:1.0 loss: 0.38737857\n",
      "9650: accuracy:0.99 loss: 0.9611116\n",
      "9650: ********* epoch 17 ********* test accuracy:0.9794 test loss: 13.436961\n",
      "9660: accuracy:0.97 loss: 4.8504024\n",
      "9670: accuracy:0.99 loss: 3.5170832\n",
      "9680: accuracy:0.99 loss: 5.4957876\n",
      "9690: accuracy:0.99 loss: 2.7966268\n",
      "9700: accuracy:0.99 loss: 3.9908323\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9745 test loss: 15.647538\n",
      "9710: accuracy:1.0 loss: 0.19729263\n",
      "9720: accuracy:1.0 loss: 0.054460105\n",
      "9730: accuracy:1.0 loss: 0.6421102\n",
      "9740: accuracy:0.99 loss: 4.9185534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9750: accuracy:1.0 loss: 0.5253672\n",
      "9750: ********* epoch 17 ********* test accuracy:0.977 test loss: 13.351046\n",
      "9760: accuracy:1.0 loss: 0.14365557\n",
      "9770: accuracy:1.0 loss: 0.11897861\n",
      "9780: accuracy:0.99 loss: 2.5965922\n",
      "9790: accuracy:0.98 loss: 6.266745\n",
      "9800: accuracy:1.0 loss: 0.0058544157\n",
      "9800: ********* epoch 17 ********* test accuracy:0.9755 test loss: 13.272182\n",
      "9810: accuracy:0.99 loss: 1.5352803\n",
      "9820: accuracy:0.99 loss: 1.4378909\n",
      "9830: accuracy:0.99 loss: 4.3637238\n",
      "9840: accuracy:0.99 loss: 0.8276287\n",
      "9850: accuracy:0.99 loss: 2.46473\n",
      "9850: ********* epoch 17 ********* test accuracy:0.9769 test loss: 13.524821\n",
      "9860: accuracy:0.99 loss: 2.813357\n",
      "9870: accuracy:0.99 loss: 2.0954506\n",
      "9880: accuracy:1.0 loss: 0.47336194\n",
      "9890: accuracy:1.0 loss: 0.032585464\n",
      "9900: accuracy:1.0 loss: 0.034521196\n",
      "9900: ********* epoch 17 ********* test accuracy:0.9762 test loss: 13.605875\n",
      "9910: accuracy:1.0 loss: 0.0041152807\n",
      "9920: accuracy:0.99 loss: 2.5764885\n",
      "9930: accuracy:1.0 loss: 0.052414216\n",
      "9940: accuracy:0.98 loss: 6.247204\n",
      "9950: accuracy:1.0 loss: 0.5786747\n",
      "9950: ********* epoch 17 ********* test accuracy:0.9769 test loss: 13.526243\n",
      "9960: accuracy:1.0 loss: 0.2072881\n",
      "9970: accuracy:1.0 loss: 0.20073324\n",
      "9980: accuracy:1.0 loss: 0.028959652\n",
      "9990: accuracy:0.99 loss: 3.7419035\n",
      "10000: accuracy:0.99 loss: 1.1194847\n",
      "10000: ********* epoch 17 ********* test accuracy:0.9755 test loss: 12.964566\n",
      "max test accuracy: 0.9814\n"
     ]
    }
   ],
   "source": [
    "# input X => 28x28 grayscale dimensions ('None' indexes images in mini-batch)\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "\n",
    "# correct answers\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights\n",
    "W1 = tf.Variable(tf.truncated_normal([28*28, 200], stddev=0.1))\n",
    "W2 = tf.Variable(tf.truncated_normal([200, 100], stddev=0.1))\n",
    "W3 = tf.Variable(tf.truncated_normal([100, 10], stddev=0.1))\n",
    "\n",
    "# biases\n",
    "b1 = tf.Variable(tf.ones([200]) / 10)\n",
    "b2 = tf.Variable(tf.ones([100]) / 10)\n",
    "b3 = tf.Variable(tf.ones([10]) / 10)\n",
    "\n",
    "# flatten images into single line\n",
    "XX = tf.reshape(X, [-1, 28*28])\n",
    "\n",
    "# model\n",
    "Y1 = tf.nn.relu(tf.matmul(XX, W1) + b1)\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)\n",
    "Y_logits = tf.matmul(Y2, W3) + b3\n",
    "Y = tf.nn.softmax(Y_logits)\n",
    "\n",
    "# cross entropy\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Y_logits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy) * 100.0\n",
    "\n",
    "# accuracy of the trained model [0, 1] ([worst, best])\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# training (learning rate is 0.003)\n",
    "train_step = tf.train.AdamOptimizer(0.003).minimize(cross_entropy)\n",
    "\n",
    "# initialization\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# call in loop to train the model 100 images at a time\n",
    "max_test_accuracy = 0.0\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    # train on batches of 100 images w/ 100 labels\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    # compute training values for visualisation\n",
    "    if update_train_data:\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: batch_X, Y_: batch_Y})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c))\n",
    "    # compute test values for visualization\n",
    "    if update_test_data:\n",
    "        global max_test_accuracy\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: mnist.test.images, Y_: mnist.test.labels})\n",
    "        if (a > max_test_accuracy):\n",
    "            max_test_accuracy = a\n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \n",
    "              \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        \n",
    "    # back-propogation training step\n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y})\n",
    "\n",
    "# text visualization of process\n",
    "for i in range(10000+1):\n",
    "    training_step(i, i % 50 == 0, i % 10 == 0)\n",
    "    \n",
    "# display the final max accuracy\n",
    "print(\"max test accuracy: \" + str(max_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decaying Learning Rate\n",
    "The code in the following cell reduces the noise of the test accuracy by employing a decaying learning rate. It starts fast and then slows down. This allows the amount of training time to remain relatively low. With 10k iterations, this maxes out with an accuracy of ~98.3% (also passed 'step' into 'feed_dict.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.14 loss: 239.11946 lr:0.0031\n",
      "0: ********* epoch 1 ********* test accuracy:0.1305 test loss: 242.16672\n",
      "10: accuracy:0.84 loss: 65.32226 lr:0.0030850375\n",
      "20: accuracy:0.89 loss: 47.39435 lr:0.0030701496\n",
      "30: accuracy:0.87 loss: 39.759422 lr:0.003055336\n",
      "40: accuracy:0.9 loss: 41.811485 lr:0.0030405961\n",
      "50: accuracy:0.92 loss: 31.148573 lr:0.00302593\n",
      "50: ********* epoch 1 ********* test accuracy:0.9003 test loss: 32.6565\n",
      "60: accuracy:0.94 loss: 25.17564 lr:0.0030113366\n",
      "70: accuracy:0.86 loss: 38.1447 lr:0.0029968163\n",
      "80: accuracy:0.88 loss: 50.50795 lr:0.0029823685\n",
      "90: accuracy:0.93 loss: 29.867804 lr:0.0029679926\n",
      "100: accuracy:0.92 loss: 40.201023 lr:0.0029536884\n",
      "100: ********* epoch 1 ********* test accuracy:0.9264 test loss: 26.498627\n",
      "110: accuracy:0.98 loss: 13.517229 lr:0.0029394557\n",
      "120: accuracy:0.96 loss: 18.678473 lr:0.0029252938\n",
      "130: accuracy:0.91 loss: 29.561497 lr:0.0029112024\n",
      "140: accuracy:0.97 loss: 16.306864 lr:0.0028971815\n",
      "150: accuracy:0.94 loss: 25.896713 lr:0.0028832306\n",
      "150: ********* epoch 1 ********* test accuracy:0.939 test loss: 20.852451\n",
      "160: accuracy:0.99 loss: 7.6610065 lr:0.002869349\n",
      "170: accuracy:0.95 loss: 15.502572 lr:0.002855537\n",
      "180: accuracy:0.97 loss: 15.766329 lr:0.0028417937\n",
      "190: accuracy:0.93 loss: 28.379457 lr:0.0028281189\n",
      "200: accuracy:0.96 loss: 11.329982 lr:0.0028145125\n",
      "200: ********* epoch 1 ********* test accuracy:0.9461 test loss: 17.94937\n",
      "210: accuracy:0.91 loss: 21.88091 lr:0.0028009736\n",
      "220: accuracy:0.94 loss: 15.556439 lr:0.0027875025\n",
      "230: accuracy:0.96 loss: 11.931231 lr:0.0027740984\n",
      "240: accuracy:0.94 loss: 13.889311 lr:0.0027607614\n",
      "250: accuracy:0.96 loss: 13.770327 lr:0.0027474908\n",
      "250: ********* epoch 1 ********* test accuracy:0.9485 test loss: 16.92006\n",
      "260: accuracy:0.94 loss: 12.503592 lr:0.0027342865\n",
      "270: accuracy:0.93 loss: 21.824854 lr:0.0027211478\n",
      "280: accuracy:0.93 loss: 18.999653 lr:0.0027080749\n",
      "290: accuracy:0.97 loss: 10.932652 lr:0.002695067\n",
      "300: accuracy:0.92 loss: 30.631388 lr:0.002682124\n",
      "300: ********* epoch 1 ********* test accuracy:0.9417 test loss: 18.102634\n",
      "310: accuracy:0.94 loss: 15.186641 lr:0.0026692455\n",
      "320: accuracy:0.96 loss: 19.466597 lr:0.0026564314\n",
      "330: accuracy:0.96 loss: 12.1917095 lr:0.0026436811\n",
      "340: accuracy:0.94 loss: 20.61821 lr:0.0026309947\n",
      "350: accuracy:0.97 loss: 7.4039965 lr:0.0026183713\n",
      "350: ********* epoch 1 ********* test accuracy:0.9602 test loss: 13.440182\n",
      "360: accuracy:0.93 loss: 16.534822 lr:0.0026058108\n",
      "370: accuracy:0.98 loss: 9.080987 lr:0.0025933129\n",
      "380: accuracy:0.96 loss: 10.648125 lr:0.0025808774\n",
      "390: accuracy:0.97 loss: 10.595926 lr:0.002568504\n",
      "400: accuracy:0.94 loss: 19.190548 lr:0.0025561925\n",
      "400: ********* epoch 1 ********* test accuracy:0.9564 test loss: 14.219865\n",
      "410: accuracy:0.94 loss: 22.062654 lr:0.002543942\n",
      "420: accuracy:0.95 loss: 13.755685 lr:0.002531753\n",
      "430: accuracy:0.97 loss: 6.2870784 lr:0.0025196243\n",
      "440: accuracy:0.96 loss: 8.996966 lr:0.0025075565\n",
      "450: accuracy:0.97 loss: 16.947973 lr:0.0024955487\n",
      "450: ********* epoch 1 ********* test accuracy:0.9631 test loss: 11.8502\n",
      "460: accuracy:0.94 loss: 17.803133 lr:0.002483601\n",
      "470: accuracy:0.95 loss: 18.843622 lr:0.0024717127\n",
      "480: accuracy:0.98 loss: 7.205346 lr:0.0024598835\n",
      "490: accuracy:0.97 loss: 12.749108 lr:0.0024481136\n",
      "500: accuracy:0.95 loss: 12.720702 lr:0.0024364025\n",
      "500: ********* epoch 1 ********* test accuracy:0.964 test loss: 11.114043\n",
      "510: accuracy:0.97 loss: 11.24418 lr:0.0024247495\n",
      "520: accuracy:0.89 loss: 27.77921 lr:0.0024131548\n",
      "530: accuracy:0.96 loss: 9.970756 lr:0.002401618\n",
      "540: accuracy:0.98 loss: 13.225901 lr:0.0023901386\n",
      "550: accuracy:1.0 loss: 2.5697172 lr:0.0023787166\n",
      "550: ********* epoch 1 ********* test accuracy:0.9635 test loss: 12.422843\n",
      "560: accuracy:0.96 loss: 7.897507 lr:0.0023673512\n",
      "570: accuracy:0.93 loss: 30.683064 lr:0.0023560429\n",
      "580: accuracy:0.98 loss: 7.88701 lr:0.0023447906\n",
      "590: accuracy:0.96 loss: 11.960937 lr:0.0023335947\n",
      "600: accuracy:0.95 loss: 14.206576 lr:0.0023224547\n",
      "600: ********* epoch 2 ********* test accuracy:0.9684 test loss: 10.47564\n",
      "610: accuracy:0.97 loss: 7.287985 lr:0.00231137\n",
      "620: accuracy:0.99 loss: 3.8306496 lr:0.002300341\n",
      "630: accuracy:0.98 loss: 7.935846 lr:0.0022893667\n",
      "640: accuracy:0.97 loss: 14.456434 lr:0.0022784472\n",
      "650: accuracy:0.98 loss: 7.4942865 lr:0.002267582\n",
      "650: ********* epoch 2 ********* test accuracy:0.9634 test loss: 11.382596\n",
      "660: accuracy:0.95 loss: 11.36018 lr:0.0022567713\n",
      "670: accuracy:1.0 loss: 1.9313529 lr:0.0022460143\n",
      "680: accuracy:0.95 loss: 13.4164095 lr:0.002235311\n",
      "690: accuracy:0.99 loss: 4.53707 lr:0.0022246612\n",
      "700: accuracy:0.97 loss: 10.1525 lr:0.0022140644\n",
      "700: ********* epoch 2 ********* test accuracy:0.9694 test loss: 9.354563\n",
      "710: accuracy:0.98 loss: 10.956157 lr:0.0022035204\n",
      "720: accuracy:0.98 loss: 14.094855 lr:0.002193029\n",
      "730: accuracy:0.96 loss: 11.385816 lr:0.00218259\n",
      "740: accuracy:0.97 loss: 8.231263 lr:0.002172203\n",
      "750: accuracy:0.99 loss: 5.414188 lr:0.0021618677\n",
      "750: ********* epoch 2 ********* test accuracy:0.9739 test loss: 8.540501\n",
      "760: accuracy:0.94 loss: 13.036661 lr:0.0021515843\n",
      "770: accuracy:0.98 loss: 8.878431 lr:0.002141352\n",
      "780: accuracy:0.98 loss: 6.430731 lr:0.0021311706\n",
      "790: accuracy:0.98 loss: 6.986781 lr:0.0021210404\n",
      "800: accuracy:0.99 loss: 3.7859862 lr:0.0021109602\n",
      "800: ********* epoch 2 ********* test accuracy:0.969 test loss: 9.399423\n",
      "810: accuracy:1.0 loss: 3.0321758 lr:0.0021009305\n",
      "820: accuracy:0.98 loss: 5.298941 lr:0.0020909507\n",
      "830: accuracy:0.98 loss: 6.8234906 lr:0.0020810207\n",
      "840: accuracy:0.98 loss: 5.5066595 lr:0.0020711406\n",
      "850: accuracy:0.96 loss: 24.100317 lr:0.0020613095\n",
      "850: ********* epoch 2 ********* test accuracy:0.9733 test loss: 8.700468\n",
      "860: accuracy:0.99 loss: 2.0274425 lr:0.0020515274\n",
      "870: accuracy:0.95 loss: 13.640666 lr:0.002041794\n",
      "880: accuracy:0.97 loss: 11.429222 lr:0.0020321093\n",
      "890: accuracy:0.99 loss: 4.7349195 lr:0.002022473\n",
      "900: accuracy:1.0 loss: 1.941278 lr:0.0020128845\n",
      "900: ********* epoch 2 ********* test accuracy:0.976 test loss: 7.676723\n",
      "910: accuracy:0.99 loss: 2.9834425 lr:0.002003344\n",
      "920: accuracy:1.0 loss: 1.0810682 lr:0.001993851\n",
      "930: accuracy:0.97 loss: 5.12124 lr:0.0019844053\n",
      "940: accuracy:1.0 loss: 1.8850867 lr:0.001975007\n",
      "950: accuracy:0.94 loss: 14.171669 lr:0.0019656552\n",
      "950: ********* epoch 2 ********* test accuracy:0.9699 test loss: 8.976541\n",
      "960: accuracy:0.98 loss: 6.9622545 lr:0.0019563502\n",
      "970: accuracy:0.97 loss: 4.1109543 lr:0.0019470915\n",
      "980: accuracy:0.99 loss: 5.7479444 lr:0.0019378791\n",
      "990: accuracy:0.96 loss: 16.490694 lr:0.0019287127\n",
      "1000: accuracy:0.98 loss: 3.3878536 lr:0.0019195919\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9696 test loss: 9.867121\n",
      "1010: accuracy:0.98 loss: 3.782329 lr:0.0019105168\n",
      "1020: accuracy:1.0 loss: 1.9182867 lr:0.0019014867\n",
      "1030: accuracy:0.98 loss: 3.2791636 lr:0.0018925016\n",
      "1040: accuracy:0.96 loss: 8.336926 lr:0.0018835615\n",
      "1050: accuracy:0.98 loss: 4.879396 lr:0.0018746661\n",
      "1050: ********* epoch 2 ********* test accuracy:0.9722 test loss: 8.424544\n",
      "1060: accuracy:1.0 loss: 1.8200502 lr:0.0018658149\n",
      "1070: accuracy:0.99 loss: 3.2438478 lr:0.0018570079\n",
      "1080: accuracy:0.96 loss: 10.261189 lr:0.0018482447\n",
      "1090: accuracy:0.97 loss: 5.1974893 lr:0.0018395253\n",
      "1100: accuracy:0.98 loss: 9.22354 lr:0.0018308494\n",
      "1100: ********* epoch 2 ********* test accuracy:0.9707 test loss: 9.214605\n",
      "1110: accuracy:0.97 loss: 9.932732 lr:0.0018222167\n",
      "1120: accuracy:0.96 loss: 14.028302 lr:0.0018136272\n",
      "1130: accuracy:0.97 loss: 7.457447 lr:0.0018050805\n",
      "1140: accuracy:0.99 loss: 4.622742 lr:0.0017965762\n",
      "1150: accuracy:0.98 loss: 12.484216 lr:0.0017881145\n",
      "1150: ********* epoch 2 ********* test accuracy:0.9729 test loss: 8.398748\n",
      "1160: accuracy:0.97 loss: 7.8018637 lr:0.001779695\n",
      "1170: accuracy:0.97 loss: 12.869503 lr:0.0017713174\n",
      "1180: accuracy:1.0 loss: 3.325485 lr:0.0017629818\n",
      "1190: accuracy:0.99 loss: 3.7560005 lr:0.0017546876\n",
      "1200: accuracy:0.99 loss: 6.2409697 lr:0.0017464348\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9741 test loss: 7.7560515\n",
      "1210: accuracy:0.97 loss: 12.359985 lr:0.0017382234\n",
      "1220: accuracy:0.98 loss: 5.711912 lr:0.0017300526\n",
      "1230: accuracy:0.99 loss: 4.0347114 lr:0.0017219227\n",
      "1240: accuracy:0.99 loss: 2.6557732 lr:0.0017138333\n",
      "1250: accuracy:0.97 loss: 5.071774 lr:0.0017057844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250: ********* epoch 3 ********* test accuracy:0.9742 test loss: 7.9554925\n",
      "1260: accuracy:0.97 loss: 5.405784 lr:0.0016977752\n",
      "1270: accuracy:0.99 loss: 3.209503 lr:0.0016898063\n",
      "1280: accuracy:0.97 loss: 11.751436 lr:0.0016818773\n",
      "1290: accuracy:0.97 loss: 10.148688 lr:0.0016739876\n",
      "1300: accuracy:0.99 loss: 4.439204 lr:0.0016661374\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9763 test loss: 7.68833\n",
      "1310: accuracy:0.97 loss: 8.858313 lr:0.0016583262\n",
      "1320: accuracy:0.97 loss: 9.519244 lr:0.0016505539\n",
      "1330: accuracy:0.99 loss: 5.8916726 lr:0.0016428205\n",
      "1340: accuracy:0.97 loss: 15.695383 lr:0.0016351256\n",
      "1350: accuracy:0.96 loss: 7.5066032 lr:0.0016274692\n",
      "1350: ********* epoch 3 ********* test accuracy:0.9748 test loss: 8.527015\n",
      "1360: accuracy:0.97 loss: 8.018057 lr:0.001619851\n",
      "1370: accuracy:1.0 loss: 1.5410709 lr:0.0016122705\n",
      "1380: accuracy:0.98 loss: 3.8918958 lr:0.0016047282\n",
      "1390: accuracy:0.98 loss: 4.973746 lr:0.0015972232\n",
      "1400: accuracy:0.97 loss: 5.6491256 lr:0.0015897558\n",
      "1400: ********* epoch 3 ********* test accuracy:0.9775 test loss: 7.3056517\n",
      "1410: accuracy:1.0 loss: 1.7060597 lr:0.0015823257\n",
      "1420: accuracy:0.98 loss: 4.658827 lr:0.0015749325\n",
      "1430: accuracy:0.99 loss: 3.814455 lr:0.0015675762\n",
      "1440: accuracy:0.99 loss: 3.4304354 lr:0.0015602567\n",
      "1450: accuracy:0.99 loss: 4.1692543 lr:0.0015529736\n",
      "1450: ********* epoch 3 ********* test accuracy:0.9786 test loss: 7.223263\n",
      "1460: accuracy:0.99 loss: 4.2369795 lr:0.0015457269\n",
      "1470: accuracy:0.97 loss: 5.8508005 lr:0.0015385163\n",
      "1480: accuracy:0.99 loss: 3.4851305 lr:0.0015313418\n",
      "1490: accuracy:0.97 loss: 6.536779 lr:0.0015242028\n",
      "1500: accuracy:0.98 loss: 4.309831 lr:0.0015170996\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9771 test loss: 7.398363\n",
      "1510: accuracy:0.97 loss: 5.472842 lr:0.0015100318\n",
      "1520: accuracy:0.99 loss: 2.9711509 lr:0.0015029992\n",
      "1530: accuracy:0.99 loss: 2.4898539 lr:0.0014960017\n",
      "1540: accuracy:0.98 loss: 5.932136 lr:0.0014890392\n",
      "1550: accuracy:1.0 loss: 1.2916142 lr:0.0014821113\n",
      "1550: ********* epoch 3 ********* test accuracy:0.9764 test loss: 7.2304845\n",
      "1560: accuracy:0.99 loss: 2.3419094 lr:0.001475218\n",
      "1570: accuracy:0.97 loss: 13.488362 lr:0.001468359\n",
      "1580: accuracy:1.0 loss: 1.3608091 lr:0.0014615343\n",
      "1590: accuracy:0.98 loss: 6.406597 lr:0.0014547437\n",
      "1600: accuracy:1.0 loss: 1.8445251 lr:0.0014479868\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9754 test loss: 8.1532955\n",
      "1610: accuracy:0.99 loss: 2.6516094 lr:0.0014412637\n",
      "1620: accuracy:1.0 loss: 1.9309961 lr:0.0014345741\n",
      "1630: accuracy:0.97 loss: 10.151574 lr:0.001427918\n",
      "1640: accuracy:0.99 loss: 4.305896 lr:0.0014212949\n",
      "1650: accuracy:1.0 loss: 0.92157173 lr:0.001414705\n",
      "1650: ********* epoch 3 ********* test accuracy:0.9757 test loss: 7.8640513\n",
      "1660: accuracy:0.97 loss: 6.7765856 lr:0.0014081479\n",
      "1670: accuracy:1.0 loss: 1.6015536 lr:0.0014016235\n",
      "1680: accuracy:0.97 loss: 8.12795 lr:0.0013951315\n",
      "1690: accuracy:0.97 loss: 5.642002 lr:0.001388672\n",
      "1700: accuracy:1.0 loss: 1.4064895 lr:0.0013822448\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9786 test loss: 7.1386776\n",
      "1710: accuracy:0.96 loss: 7.5063477 lr:0.0013758496\n",
      "1720: accuracy:1.0 loss: 2.651613 lr:0.0013694862\n",
      "1730: accuracy:1.0 loss: 0.792438 lr:0.0013631545\n",
      "1740: accuracy:1.0 loss: 2.3131585 lr:0.0013568546\n",
      "1750: accuracy:0.99 loss: 2.5596876 lr:0.001350586\n",
      "1750: ********* epoch 3 ********* test accuracy:0.9761 test loss: 7.4318156\n",
      "1760: accuracy:0.99 loss: 3.6504984 lr:0.0013443487\n",
      "1770: accuracy:0.99 loss: 2.5663528 lr:0.0013381424\n",
      "1780: accuracy:0.98 loss: 4.360244 lr:0.0013319672\n",
      "1790: accuracy:1.0 loss: 2.2931938 lr:0.0013258228\n",
      "1800: accuracy:1.0 loss: 0.35424566 lr:0.0013197089\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9796 test loss: 6.77704\n",
      "1810: accuracy:0.98 loss: 3.237827 lr:0.0013136256\n",
      "1820: accuracy:1.0 loss: 2.316403 lr:0.0013075727\n",
      "1830: accuracy:0.97 loss: 4.8058553 lr:0.0013015498\n",
      "1840: accuracy:0.97 loss: 4.02996 lr:0.0012955571\n",
      "1850: accuracy:0.98 loss: 3.363643 lr:0.0012895942\n",
      "1850: ********* epoch 4 ********* test accuracy:0.9805 test loss: 6.584736\n",
      "1860: accuracy:1.0 loss: 0.836075 lr:0.001283661\n",
      "1870: accuracy:0.99 loss: 2.8840823 lr:0.0012777575\n",
      "1880: accuracy:1.0 loss: 0.90936303 lr:0.0012718835\n",
      "1890: accuracy:0.99 loss: 3.7835345 lr:0.0012660386\n",
      "1900: accuracy:0.98 loss: 7.3291764 lr:0.001260223\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9781 test loss: 7.06971\n",
      "1910: accuracy:0.98 loss: 5.2259192 lr:0.0012544364\n",
      "1920: accuracy:0.99 loss: 2.9177527 lr:0.0012486785\n",
      "1930: accuracy:0.97 loss: 5.769504 lr:0.0012429495\n",
      "1940: accuracy:1.0 loss: 1.3513144 lr:0.0012372491\n",
      "1950: accuracy:1.0 loss: 1.3806571 lr:0.001231577\n",
      "1950: ********* epoch 4 ********* test accuracy:0.9785 test loss: 7.1025977\n",
      "1960: accuracy:1.0 loss: 1.3917978 lr:0.0012259333\n",
      "1970: accuracy:1.0 loss: 1.5742725 lr:0.0012203177\n",
      "1980: accuracy:0.98 loss: 10.799366 lr:0.00121473\n",
      "1990: accuracy:0.99 loss: 2.1254625 lr:0.0012091703\n",
      "2000: accuracy:0.99 loss: 4.0528517 lr:0.0012036383\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9782 test loss: 6.9783964\n",
      "2010: accuracy:0.99 loss: 3.1975682 lr:0.0011981339\n",
      "2020: accuracy:0.98 loss: 2.6914358 lr:0.0011926569\n",
      "2030: accuracy:0.99 loss: 1.8970737 lr:0.0011872072\n",
      "2040: accuracy:0.99 loss: 4.170719 lr:0.0011817847\n",
      "2050: accuracy:1.0 loss: 0.98986197 lr:0.0011763893\n",
      "2050: ********* epoch 4 ********* test accuracy:0.9804 test loss: 6.487009\n",
      "2060: accuracy:1.0 loss: 0.6829844 lr:0.0011710208\n",
      "2070: accuracy:0.97 loss: 3.4144084 lr:0.0011656791\n",
      "2080: accuracy:0.99 loss: 1.7419306 lr:0.0011603639\n",
      "2090: accuracy:0.99 loss: 5.2448053 lr:0.0011550754\n",
      "2100: accuracy:1.0 loss: 1.6745692 lr:0.0011498132\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9793 test loss: 6.6382713\n",
      "2110: accuracy:0.99 loss: 2.3135648 lr:0.0011445773\n",
      "2120: accuracy:1.0 loss: 0.2701859 lr:0.0011393673\n",
      "2130: accuracy:1.0 loss: 0.503977 lr:0.0011341835\n",
      "2140: accuracy:1.0 loss: 0.4740841 lr:0.0011290255\n",
      "2150: accuracy:0.99 loss: 2.5997627 lr:0.0011238932\n",
      "2150: ********* epoch 4 ********* test accuracy:0.9814 test loss: 6.1906285\n",
      "2160: accuracy:0.97 loss: 5.7103224 lr:0.0011187865\n",
      "2170: accuracy:0.99 loss: 5.805169 lr:0.0011137052\n",
      "2180: accuracy:0.99 loss: 2.1901767 lr:0.0011086494\n",
      "2190: accuracy:0.99 loss: 2.7839718 lr:0.0011036188\n",
      "2200: accuracy:0.99 loss: 6.0094132 lr:0.0010986131\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9796 test loss: 6.8765774\n",
      "2210: accuracy:0.96 loss: 21.65332 lr:0.0010936325\n",
      "2220: accuracy:1.0 loss: 1.3811748 lr:0.0010886769\n",
      "2230: accuracy:0.99 loss: 3.0498664 lr:0.0010837459\n",
      "2240: accuracy:0.99 loss: 4.0389953 lr:0.0010788393\n",
      "2250: accuracy:1.0 loss: 0.7198654 lr:0.0010739574\n",
      "2250: ********* epoch 4 ********* test accuracy:0.9779 test loss: 7.127521\n",
      "2260: accuracy:0.99 loss: 2.6637025 lr:0.0010690998\n",
      "2270: accuracy:0.98 loss: 7.17524 lr:0.0010642663\n",
      "2280: accuracy:0.98 loss: 2.240347 lr:0.001059457\n",
      "2290: accuracy:0.99 loss: 2.0318866 lr:0.0010546717\n",
      "2300: accuracy:1.0 loss: 2.005018 lr:0.0010499102\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9815 test loss: 6.710922\n",
      "2310: accuracy:0.99 loss: 5.9893756 lr:0.0010451726\n",
      "2320: accuracy:0.99 loss: 1.44751 lr:0.0010404584\n",
      "2330: accuracy:1.0 loss: 0.96440053 lr:0.001035768\n",
      "2340: accuracy:0.99 loss: 3.0561416 lr:0.0010311007\n",
      "2350: accuracy:1.0 loss: 0.6942298 lr:0.0010264568\n",
      "2350: ********* epoch 4 ********* test accuracy:0.9802 test loss: 6.4585657\n",
      "2360: accuracy:1.0 loss: 0.49261242 lr:0.0010218362\n",
      "2370: accuracy:1.0 loss: 0.3161823 lr:0.0010172385\n",
      "2380: accuracy:1.0 loss: 1.233685 lr:0.0010126637\n",
      "2390: accuracy:0.99 loss: 3.9821332 lr:0.0010081119\n",
      "2400: accuracy:1.0 loss: 1.2390885 lr:0.0010035826\n",
      "2400: ********* epoch 5 ********* test accuracy:0.9806 test loss: 6.4448752\n",
      "2410: accuracy:0.98 loss: 4.840121 lr:0.000999076\n",
      "2420: accuracy:0.99 loss: 1.5616302 lr:0.0009945917\n",
      "2430: accuracy:1.0 loss: 2.0477555 lr:0.00099013\n",
      "2440: accuracy:0.99 loss: 1.2306124 lr:0.0009856905\n",
      "2450: accuracy:0.99 loss: 2.9756937 lr:0.0009812731\n",
      "2450: ********* epoch 5 ********* test accuracy:0.9814 test loss: 6.404474\n",
      "2460: accuracy:1.0 loss: 0.14779358 lr:0.0009768778\n",
      "2470: accuracy:0.99 loss: 3.823136 lr:0.00097250426\n",
      "2480: accuracy:0.99 loss: 10.657814 lr:0.00096815266\n",
      "2490: accuracy:0.99 loss: 3.107378 lr:0.00096382276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500: accuracy:0.99 loss: 2.2855358 lr:0.0009595144\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9824 test loss: 6.087627\n",
      "2510: accuracy:0.99 loss: 2.2156007 lr:0.0009552275\n",
      "2520: accuracy:0.99 loss: 4.0672655 lr:0.000950962\n",
      "2530: accuracy:1.0 loss: 0.21640112 lr:0.00094671786\n",
      "2540: accuracy:0.99 loss: 4.0070796 lr:0.00094249484\n",
      "2550: accuracy:1.0 loss: 0.9493361 lr:0.0009382929\n",
      "2550: ********* epoch 5 ********* test accuracy:0.9806 test loss: 6.364784\n",
      "2560: accuracy:0.99 loss: 4.536926 lr:0.0009341118\n",
      "2570: accuracy:1.0 loss: 1.3138072 lr:0.0009299517\n",
      "2580: accuracy:0.99 loss: 4.9137473 lr:0.0009258123\n",
      "2590: accuracy:0.97 loss: 5.3543696 lr:0.0009216936\n",
      "2600: accuracy:1.0 loss: 0.9747217 lr:0.00091759535\n",
      "2600: ********* epoch 5 ********* test accuracy:0.982 test loss: 6.4533224\n",
      "2610: accuracy:1.0 loss: 0.5266502 lr:0.0009135176\n",
      "2620: accuracy:1.0 loss: 0.9838427 lr:0.0009094601\n",
      "2630: accuracy:1.0 loss: 1.2264178 lr:0.00090542296\n",
      "2640: accuracy:1.0 loss: 0.23740597 lr:0.0009014059\n",
      "2650: accuracy:0.99 loss: 1.7202345 lr:0.00089740887\n",
      "2650: ********* epoch 5 ********* test accuracy:0.9818 test loss: 6.1487546\n",
      "2660: accuracy:1.0 loss: 0.381185 lr:0.0008934318\n",
      "2670: accuracy:0.99 loss: 1.6315662 lr:0.0008894745\n",
      "2680: accuracy:1.0 loss: 0.17580849 lr:0.00088553707\n",
      "2690: accuracy:0.99 loss: 2.2444632 lr:0.00088161917\n",
      "2700: accuracy:1.0 loss: 0.8632855 lr:0.0008777208\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9828 test loss: 6.292722\n",
      "2710: accuracy:0.99 loss: 2.1117806 lr:0.0008738419\n",
      "2720: accuracy:1.0 loss: 1.769537 lr:0.00086998235\n",
      "2730: accuracy:0.99 loss: 2.460476 lr:0.0008661421\n",
      "2740: accuracy:0.99 loss: 2.492294 lr:0.0008623208\n",
      "2750: accuracy:1.0 loss: 0.96260077 lr:0.0008585187\n",
      "2750: ********* epoch 5 ********* test accuracy:0.98 test loss: 6.7673454\n",
      "2760: accuracy:1.0 loss: 0.4867952 lr:0.0008547356\n",
      "2770: accuracy:0.99 loss: 1.645971 lr:0.0008509713\n",
      "2780: accuracy:0.99 loss: 1.5505482 lr:0.0008472259\n",
      "2790: accuracy:0.99 loss: 10.347322 lr:0.00084349903\n",
      "2800: accuracy:1.0 loss: 0.30989537 lr:0.00083979085\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9819 test loss: 6.3963413\n",
      "2810: accuracy:1.0 loss: 1.0739744 lr:0.0008361012\n",
      "2820: accuracy:0.97 loss: 5.1648645 lr:0.00083242985\n",
      "2830: accuracy:0.99 loss: 2.6431944 lr:0.0008287768\n",
      "2840: accuracy:1.0 loss: 0.18586652 lr:0.000825142\n",
      "2850: accuracy:0.99 loss: 2.1991408 lr:0.00082152535\n",
      "2850: ********* epoch 5 ********* test accuracy:0.9796 test loss: 7.6742706\n",
      "2860: accuracy:0.99 loss: 2.1832738 lr:0.0008179267\n",
      "2870: accuracy:1.0 loss: 0.7100006 lr:0.0008143461\n",
      "2880: accuracy:1.0 loss: 0.3454652 lr:0.00081078324\n",
      "2890: accuracy:0.99 loss: 2.5828328 lr:0.0008072382\n",
      "2900: accuracy:1.0 loss: 0.9925616 lr:0.00080371083\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9818 test loss: 6.5440664\n",
      "2910: accuracy:1.0 loss: 1.0066959 lr:0.0008002011\n",
      "2920: accuracy:0.99 loss: 4.279995 lr:0.00079670886\n",
      "2930: accuracy:0.99 loss: 3.5125394 lr:0.000793234\n",
      "2940: accuracy:1.0 loss: 0.19764674 lr:0.00078977644\n",
      "2950: accuracy:1.0 loss: 1.1040967 lr:0.0007863362\n",
      "2950: ********* epoch 5 ********* test accuracy:0.9799 test loss: 6.7181334\n",
      "2960: accuracy:1.0 loss: 0.70314217 lr:0.00078291306\n",
      "2970: accuracy:0.99 loss: 1.091718 lr:0.00077950704\n",
      "2980: accuracy:0.99 loss: 3.0899568 lr:0.00077611796\n",
      "2990: accuracy:1.0 loss: 1.7282692 lr:0.00077274576\n",
      "3000: accuracy:0.99 loss: 1.2997338 lr:0.00076939043\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9793 test loss: 6.8190956\n",
      "3010: accuracy:0.99 loss: 1.6145815 lr:0.0007660518\n",
      "3020: accuracy:1.0 loss: 1.9894086 lr:0.0007627299\n",
      "3030: accuracy:0.99 loss: 4.312452 lr:0.0007594245\n",
      "3040: accuracy:1.0 loss: 0.26877964 lr:0.0007561356\n",
      "3050: accuracy:1.0 loss: 0.8513124 lr:0.0007528631\n",
      "3050: ********* epoch 6 ********* test accuracy:0.9801 test loss: 6.9696918\n",
      "3060: accuracy:0.99 loss: 1.2422912 lr:0.000749607\n",
      "3070: accuracy:0.99 loss: 4.322566 lr:0.00074636703\n",
      "3080: accuracy:1.0 loss: 0.33948418 lr:0.0007431433\n",
      "3090: accuracy:1.0 loss: 0.227501 lr:0.0007399356\n",
      "3100: accuracy:1.0 loss: 1.1938285 lr:0.0007367439\n",
      "3100: ********* epoch 6 ********* test accuracy:0.981 test loss: 6.2588396\n",
      "3110: accuracy:1.0 loss: 0.4508453 lr:0.00073356816\n",
      "3120: accuracy:1.0 loss: 0.41582164 lr:0.00073040824\n",
      "3130: accuracy:1.0 loss: 0.5029461 lr:0.00072726404\n",
      "3140: accuracy:1.0 loss: 0.3152389 lr:0.00072413555\n",
      "3150: accuracy:1.0 loss: 0.7375673 lr:0.00072102266\n",
      "3150: ********* epoch 6 ********* test accuracy:0.9826 test loss: 6.2713\n",
      "3160: accuracy:1.0 loss: 0.30680758 lr:0.0007179253\n",
      "3170: accuracy:0.99 loss: 2.658215 lr:0.0007148434\n",
      "3180: accuracy:1.0 loss: 0.65433437 lr:0.00071177684\n",
      "3190: accuracy:1.0 loss: 0.24946804 lr:0.0007087256\n",
      "3200: accuracy:1.0 loss: 0.49316528 lr:0.00070568954\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9807 test loss: 6.91183\n",
      "3210: accuracy:1.0 loss: 0.40627107 lr:0.0007026687\n",
      "3220: accuracy:0.99 loss: 4.4688077 lr:0.0006996628\n",
      "3230: accuracy:0.99 loss: 2.6918623 lr:0.00069667195\n",
      "3240: accuracy:0.99 loss: 2.6160784 lr:0.0006936961\n",
      "3250: accuracy:1.0 loss: 0.6513607 lr:0.000690735\n",
      "3250: ********* epoch 6 ********* test accuracy:0.9822 test loss: 6.517702\n",
      "3260: accuracy:0.99 loss: 2.3334472 lr:0.0006877887\n",
      "3270: accuracy:0.99 loss: 1.5295624 lr:0.0006848571\n",
      "3280: accuracy:1.0 loss: 0.58566546 lr:0.00068194006\n",
      "3290: accuracy:1.0 loss: 0.33051485 lr:0.00067903765\n",
      "3300: accuracy:0.99 loss: 1.0660032 lr:0.0006761497\n",
      "3300: ********* epoch 6 ********* test accuracy:0.983 test loss: 6.261473\n",
      "3310: accuracy:1.0 loss: 0.34459552 lr:0.00067327614\n",
      "3320: accuracy:1.0 loss: 0.7602495 lr:0.0006704169\n",
      "3330: accuracy:1.0 loss: 0.43346354 lr:0.00066757196\n",
      "3340: accuracy:0.98 loss: 3.776234 lr:0.00066474115\n",
      "3350: accuracy:0.99 loss: 2.684502 lr:0.00066192454\n",
      "3350: ********* epoch 6 ********* test accuracy:0.9824 test loss: 6.2398925\n",
      "3360: accuracy:0.99 loss: 1.4630346 lr:0.0006591219\n",
      "3370: accuracy:0.99 loss: 5.678977 lr:0.0006563333\n",
      "3380: accuracy:1.0 loss: 0.7301348 lr:0.0006535585\n",
      "3390: accuracy:1.0 loss: 0.88319534 lr:0.00065079774\n",
      "3400: accuracy:1.0 loss: 0.39514023 lr:0.00064805057\n",
      "3400: ********* epoch 6 ********* test accuracy:0.9816 test loss: 6.5124707\n",
      "3410: accuracy:0.99 loss: 1.5549695 lr:0.00064531714\n",
      "3420: accuracy:1.0 loss: 1.003603 lr:0.0006425974\n",
      "3430: accuracy:1.0 loss: 0.15917462 lr:0.0006398912\n",
      "3440: accuracy:1.0 loss: 0.5287125 lr:0.00063719845\n",
      "3450: accuracy:1.0 loss: 0.25857133 lr:0.00063451915\n",
      "3450: ********* epoch 6 ********* test accuracy:0.9819 test loss: 6.4286613\n",
      "3460: accuracy:1.0 loss: 0.7304472 lr:0.0006318532\n",
      "3470: accuracy:1.0 loss: 0.34164107 lr:0.00062920054\n",
      "3480: accuracy:1.0 loss: 0.79967713 lr:0.0006265612\n",
      "3490: accuracy:0.98 loss: 2.948537 lr:0.00062393496\n",
      "3500: accuracy:1.0 loss: 0.73181087 lr:0.0006213218\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9821 test loss: 6.774248\n",
      "3510: accuracy:1.0 loss: 0.58635265 lr:0.0006187217\n",
      "3520: accuracy:1.0 loss: 0.59414655 lr:0.0006161346\n",
      "3530: accuracy:1.0 loss: 0.47450712 lr:0.0006135603\n",
      "3540: accuracy:1.0 loss: 0.11824408 lr:0.00061099895\n",
      "3550: accuracy:1.0 loss: 0.28438792 lr:0.0006084503\n",
      "3550: ********* epoch 6 ********* test accuracy:0.9818 test loss: 6.468021\n",
      "3560: accuracy:1.0 loss: 0.47039905 lr:0.00060591445\n",
      "3570: accuracy:0.99 loss: 1.8204708 lr:0.0006033912\n",
      "3580: accuracy:1.0 loss: 0.5319667 lr:0.0006008805\n",
      "3590: accuracy:1.0 loss: 0.47897077 lr:0.0005983823\n",
      "3600: accuracy:1.0 loss: 0.28972366 lr:0.0005958966\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9824 test loss: 6.397462\n",
      "3610: accuracy:1.0 loss: 0.5195425 lr:0.0005934234\n",
      "3620: accuracy:1.0 loss: 0.9651566 lr:0.0005909624\n",
      "3630: accuracy:1.0 loss: 0.7667824 lr:0.00058851374\n",
      "3640: accuracy:1.0 loss: 0.5123764 lr:0.0005860772\n",
      "3650: accuracy:1.0 loss: 0.83299524 lr:0.00058365293\n",
      "3650: ********* epoch 7 ********* test accuracy:0.9825 test loss: 6.195563\n",
      "3660: accuracy:1.0 loss: 0.30510506 lr:0.0005812407\n",
      "3670: accuracy:1.0 loss: 0.3433219 lr:0.0005788405\n",
      "3680: accuracy:1.0 loss: 0.48408574 lr:0.0005764523\n",
      "3690: accuracy:1.0 loss: 0.48460704 lr:0.00057407597\n",
      "3700: accuracy:0.99 loss: 1.3178849 lr:0.00057171145\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9825 test loss: 6.638115\n",
      "3710: accuracy:0.99 loss: 2.9927428 lr:0.00056935876\n",
      "3720: accuracy:1.0 loss: 0.3487687 lr:0.0005670178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3730: accuracy:1.0 loss: 0.27720904 lr:0.0005646886\n",
      "3740: accuracy:1.0 loss: 0.5891169 lr:0.00056237093\n",
      "3750: accuracy:0.99 loss: 1.3630192 lr:0.00056006486\n",
      "3750: ********* epoch 7 ********* test accuracy:0.9823 test loss: 6.408538\n",
      "3760: accuracy:1.0 loss: 0.6293125 lr:0.00055777025\n",
      "3770: accuracy:1.0 loss: 0.45602113 lr:0.0005554871\n",
      "3780: accuracy:1.0 loss: 0.8667449 lr:0.0005532154\n",
      "3790: accuracy:1.0 loss: 0.18310928 lr:0.00055095495\n",
      "3800: accuracy:1.0 loss: 0.37502623 lr:0.00054870587\n",
      "3800: ********* epoch 7 ********* test accuracy:0.9821 test loss: 6.6915975\n",
      "3810: accuracy:1.0 loss: 0.3489277 lr:0.0005464679\n",
      "3820: accuracy:1.0 loss: 0.7239522 lr:0.0005442411\n",
      "3830: accuracy:1.0 loss: 0.23364753 lr:0.0005420255\n",
      "3840: accuracy:1.0 loss: 0.3170178 lr:0.0005398209\n",
      "3850: accuracy:1.0 loss: 0.2998878 lr:0.00053762726\n",
      "3850: ********* epoch 7 ********* test accuracy:0.9827 test loss: 6.473717\n",
      "3860: accuracy:1.0 loss: 0.34040162 lr:0.0005354446\n",
      "3870: accuracy:1.0 loss: 0.2934652 lr:0.00053327275\n",
      "3880: accuracy:1.0 loss: 0.2182506 lr:0.00053111184\n",
      "3890: accuracy:0.99 loss: 0.8525082 lr:0.00052896165\n",
      "3900: accuracy:1.0 loss: 0.49514672 lr:0.0005268222\n",
      "3900: ********* epoch 7 ********* test accuracy:0.982 test loss: 6.4672904\n",
      "3910: accuracy:1.0 loss: 0.20433353 lr:0.0005246934\n",
      "3920: accuracy:1.0 loss: 0.5343513 lr:0.0005225753\n",
      "3930: accuracy:1.0 loss: 0.33377948 lr:0.0005204676\n",
      "3940: accuracy:1.0 loss: 0.34514368 lr:0.00051837054\n",
      "3950: accuracy:0.97 loss: 3.9525266 lr:0.0005162839\n",
      "3950: ********* epoch 7 ********* test accuracy:0.9822 test loss: 6.452628\n",
      "3960: accuracy:1.0 loss: 0.3252353 lr:0.0005142077\n",
      "3970: accuracy:1.0 loss: 0.40042013 lr:0.0005121418\n",
      "3980: accuracy:1.0 loss: 0.118137084 lr:0.00051008625\n",
      "3990: accuracy:0.99 loss: 1.4642718 lr:0.00050804095\n",
      "4000: accuracy:1.0 loss: 0.36505607 lr:0.0005060059\n",
      "4000: ********* epoch 7 ********* test accuracy:0.9824 test loss: 6.7295117\n",
      "4010: accuracy:1.0 loss: 0.17328842 lr:0.00050398085\n",
      "4020: accuracy:1.0 loss: 0.50282717 lr:0.00050196605\n",
      "4030: accuracy:1.0 loss: 0.17699525 lr:0.00049996114\n",
      "4040: accuracy:1.0 loss: 0.22198623 lr:0.0004979664\n",
      "4050: accuracy:1.0 loss: 1.2019439 lr:0.0004959815\n",
      "4050: ********* epoch 7 ********* test accuracy:0.9821 test loss: 6.9646487\n",
      "4060: accuracy:1.0 loss: 0.92775244 lr:0.0004940065\n",
      "4070: accuracy:1.0 loss: 0.16944689 lr:0.0004920414\n",
      "4080: accuracy:0.99 loss: 1.0752494 lr:0.0004900861\n",
      "4090: accuracy:1.0 loss: 0.1260051 lr:0.00048814056\n",
      "4100: accuracy:1.0 loss: 0.6125437 lr:0.00048620466\n",
      "4100: ********* epoch 7 ********* test accuracy:0.9826 test loss: 6.675968\n",
      "4110: accuracy:1.0 loss: 0.20711255 lr:0.00048427854\n",
      "4120: accuracy:0.99 loss: 1.2330734 lr:0.00048236188\n",
      "4130: accuracy:1.0 loss: 1.0811756 lr:0.00048045485\n",
      "4140: accuracy:1.0 loss: 0.477444 lr:0.00047855728\n",
      "4150: accuracy:1.0 loss: 0.67299104 lr:0.0004766693\n",
      "4150: ********* epoch 7 ********* test accuracy:0.9825 test loss: 6.7575455\n",
      "4160: accuracy:1.0 loss: 0.9176324 lr:0.0004747906\n",
      "4170: accuracy:1.0 loss: 0.28327283 lr:0.00047292135\n",
      "4180: accuracy:1.0 loss: 0.12187215 lr:0.0004710614\n",
      "4190: accuracy:1.0 loss: 0.83266366 lr:0.00046921076\n",
      "4200: accuracy:1.0 loss: 0.27619973 lr:0.00046736925\n",
      "4200: ********* epoch 8 ********* test accuracy:0.9821 test loss: 6.6486373\n",
      "4210: accuracy:1.0 loss: 0.2404636 lr:0.00046553704\n",
      "4220: accuracy:1.0 loss: 0.21423504 lr:0.00046371386\n",
      "4230: accuracy:1.0 loss: 0.6308943 lr:0.0004618999\n",
      "4240: accuracy:1.0 loss: 0.29056314 lr:0.00046009486\n",
      "4250: accuracy:1.0 loss: 0.29687417 lr:0.00045829892\n",
      "4250: ********* epoch 8 ********* test accuracy:0.9819 test loss: 6.692613\n",
      "4260: accuracy:1.0 loss: 0.78349996 lr:0.0004565119\n",
      "4270: accuracy:1.0 loss: 0.30001438 lr:0.0004547338\n",
      "4280: accuracy:1.0 loss: 0.4110456 lr:0.0004529645\n",
      "4290: accuracy:1.0 loss: 0.35982618 lr:0.00045120405\n",
      "4300: accuracy:0.99 loss: 2.3623626 lr:0.00044945246\n",
      "4300: ********* epoch 8 ********* test accuracy:0.9836 test loss: 6.55738\n",
      "4310: accuracy:1.0 loss: 0.101332165 lr:0.00044770952\n",
      "4320: accuracy:1.0 loss: 0.3740704 lr:0.00044597537\n",
      "4330: accuracy:1.0 loss: 0.23709309 lr:0.00044424974\n",
      "4340: accuracy:1.0 loss: 0.23912226 lr:0.00044253285\n",
      "4350: accuracy:1.0 loss: 0.1734784 lr:0.00044082443\n",
      "4350: ********* epoch 8 ********* test accuracy:0.9829 test loss: 6.5820208\n",
      "4360: accuracy:1.0 loss: 0.31422225 lr:0.0004391246\n",
      "4370: accuracy:1.0 loss: 0.51409864 lr:0.00043743316\n",
      "4380: accuracy:1.0 loss: 0.16047601 lr:0.00043575026\n",
      "4390: accuracy:1.0 loss: 0.06518977 lr:0.00043407566\n",
      "4400: accuracy:1.0 loss: 0.25400814 lr:0.0004324095\n",
      "4400: ********* epoch 8 ********* test accuracy:0.9833 test loss: 6.5215244\n",
      "4410: accuracy:1.0 loss: 0.16153242 lr:0.00043075153\n",
      "4420: accuracy:1.0 loss: 0.6243615 lr:0.00042910196\n",
      "4430: accuracy:1.0 loss: 0.30505645 lr:0.0004274605\n",
      "4440: accuracy:1.0 loss: 0.25353757 lr:0.00042582734\n",
      "4450: accuracy:1.0 loss: 0.12038335 lr:0.00042420224\n",
      "4450: ********* epoch 8 ********* test accuracy:0.9832 test loss: 6.507539\n",
      "4460: accuracy:1.0 loss: 0.082704104 lr:0.00042258532\n",
      "4470: accuracy:1.0 loss: 0.2394734 lr:0.00042097637\n",
      "4480: accuracy:1.0 loss: 0.46777362 lr:0.00041937552\n",
      "4490: accuracy:1.0 loss: 0.14529683 lr:0.0004177826\n",
      "4500: accuracy:1.0 loss: 0.412193 lr:0.0004161977\n",
      "4500: ********* epoch 8 ********* test accuracy:0.9832 test loss: 6.582533\n",
      "4510: accuracy:1.0 loss: 0.19254057 lr:0.00041462062\n",
      "4520: accuracy:1.0 loss: 0.10094993 lr:0.0004130515\n",
      "4530: accuracy:1.0 loss: 0.078578115 lr:0.0004114901\n",
      "4540: accuracy:1.0 loss: 0.20728981 lr:0.0004099365\n",
      "4550: accuracy:1.0 loss: 0.1088362 lr:0.0004083907\n",
      "4550: ********* epoch 8 ********* test accuracy:0.9834 test loss: 6.666035\n",
      "4560: accuracy:1.0 loss: 0.24392432 lr:0.00040685257\n",
      "4570: accuracy:1.0 loss: 0.41933638 lr:0.00040532218\n",
      "4580: accuracy:1.0 loss: 0.6963961 lr:0.00040379935\n",
      "4590: accuracy:1.0 loss: 0.3199075 lr:0.00040228417\n",
      "4600: accuracy:1.0 loss: 0.70677793 lr:0.00040077648\n",
      "4600: ********* epoch 8 ********* test accuracy:0.9834 test loss: 6.580122\n",
      "4610: accuracy:1.0 loss: 0.19886054 lr:0.0003992764\n",
      "4620: accuracy:1.0 loss: 0.1485635 lr:0.00039778373\n",
      "4630: accuracy:1.0 loss: 0.082104556 lr:0.00039629856\n",
      "4640: accuracy:1.0 loss: 0.3823067 lr:0.00039482073\n",
      "4650: accuracy:1.0 loss: 0.21748447 lr:0.00039335035\n",
      "4650: ********* epoch 8 ********* test accuracy:0.9833 test loss: 6.7807636\n",
      "4660: accuracy:1.0 loss: 0.40443525 lr:0.0003918872\n",
      "4670: accuracy:1.0 loss: 0.159107 lr:0.00039043147\n",
      "4680: accuracy:0.99 loss: 2.4208307 lr:0.0003889829\n",
      "4690: accuracy:1.0 loss: 0.0844889 lr:0.00038754163\n",
      "4700: accuracy:0.99 loss: 1.5250881 lr:0.00038610745\n",
      "4700: ********* epoch 8 ********* test accuracy:0.9828 test loss: 6.8057275\n",
      "4710: accuracy:1.0 loss: 0.2508759 lr:0.00038468052\n",
      "4720: accuracy:1.0 loss: 0.17082171 lr:0.00038326066\n",
      "4730: accuracy:1.0 loss: 0.078450665 lr:0.00038184793\n",
      "4740: accuracy:1.0 loss: 0.12512097 lr:0.00038044216\n",
      "4750: accuracy:1.0 loss: 0.5627036 lr:0.0003790435\n",
      "4750: ********* epoch 8 ********* test accuracy:0.9839 test loss: 6.73896\n",
      "4760: accuracy:1.0 loss: 0.91583383 lr:0.00037765174\n",
      "4770: accuracy:1.0 loss: 0.085619755 lr:0.00037626692\n",
      "4780: accuracy:1.0 loss: 0.0805108 lr:0.00037488903\n",
      "4790: accuracy:1.0 loss: 0.26003522 lr:0.000373518\n",
      "4800: accuracy:1.0 loss: 0.21297187 lr:0.00037215385\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9837 test loss: 6.6147923\n",
      "4810: accuracy:1.0 loss: 0.27818906 lr:0.00037079645\n",
      "4820: accuracy:0.99 loss: 1.9380972 lr:0.0003694459\n",
      "4830: accuracy:1.0 loss: 0.063124195 lr:0.000368102\n",
      "4840: accuracy:1.0 loss: 0.46145335 lr:0.00036676484\n",
      "4850: accuracy:0.99 loss: 1.7412059 lr:0.00036543433\n",
      "4850: ********* epoch 9 ********* test accuracy:0.9824 test loss: 6.8728237\n",
      "4860: accuracy:1.0 loss: 0.25361952 lr:0.0003641105\n",
      "4870: accuracy:1.0 loss: 0.25322482 lr:0.00036279322\n",
      "4880: accuracy:1.0 loss: 0.24737038 lr:0.00036148255\n",
      "4890: accuracy:1.0 loss: 0.13235432 lr:0.00036017838\n",
      "4900: accuracy:1.0 loss: 0.43445396 lr:0.00035888076\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9829 test loss: 6.7448893\n",
      "4910: accuracy:1.0 loss: 0.29062635 lr:0.00035758957\n",
      "4920: accuracy:0.99 loss: 2.7485151 lr:0.00035630487\n",
      "4930: accuracy:1.0 loss: 0.21068637 lr:0.0003550265\n",
      "4940: accuracy:1.0 loss: 0.09601598 lr:0.00035375459\n",
      "4950: accuracy:1.0 loss: 0.04459565 lr:0.00035248895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4950: ********* epoch 9 ********* test accuracy:0.9822 test loss: 6.896934\n",
      "4960: accuracy:1.0 loss: 0.2551557 lr:0.00035122968\n",
      "4970: accuracy:0.99 loss: 0.92501295 lr:0.00034997665\n",
      "4980: accuracy:1.0 loss: 0.27541828 lr:0.00034872993\n",
      "4990: accuracy:0.99 loss: 1.2614886 lr:0.00034748935\n",
      "5000: accuracy:1.0 loss: 0.22037958 lr:0.00034625502\n",
      "5000: ********* epoch 9 ********* test accuracy:0.983 test loss: 6.9390492\n",
      "5010: accuracy:1.0 loss: 0.07622694 lr:0.00034502678\n",
      "5020: accuracy:1.0 loss: 0.13588467 lr:0.00034380468\n",
      "5030: accuracy:1.0 loss: 0.088062525 lr:0.00034258873\n",
      "5040: accuracy:1.0 loss: 0.09467772 lr:0.0003413788\n",
      "5050: accuracy:1.0 loss: 0.11270957 lr:0.0003401749\n",
      "5050: ********* epoch 9 ********* test accuracy:0.9839 test loss: 6.8952107\n",
      "5060: accuracy:1.0 loss: 0.17331961 lr:0.00033897703\n",
      "5070: accuracy:1.0 loss: 0.22848544 lr:0.00033778517\n",
      "5080: accuracy:1.0 loss: 0.19036108 lr:0.0003365992\n",
      "5090: accuracy:1.0 loss: 0.4543654 lr:0.00033541917\n",
      "5100: accuracy:1.0 loss: 0.025937177 lr:0.00033424495\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9834 test loss: 6.7039475\n",
      "5110: accuracy:1.0 loss: 0.38679957 lr:0.0003330767\n",
      "5120: accuracy:1.0 loss: 0.07846202 lr:0.0003319142\n",
      "5130: accuracy:0.99 loss: 2.1614888 lr:0.00033075755\n",
      "5140: accuracy:1.0 loss: 0.07221946 lr:0.0003296066\n",
      "5150: accuracy:1.0 loss: 0.42150155 lr:0.00032846146\n",
      "5150: ********* epoch 9 ********* test accuracy:0.9828 test loss: 6.9054885\n",
      "5160: accuracy:1.0 loss: 0.067219324 lr:0.000327322\n",
      "5170: accuracy:1.0 loss: 0.23659262 lr:0.00032618825\n",
      "5180: accuracy:1.0 loss: 0.06005047 lr:0.0003250601\n",
      "5190: accuracy:1.0 loss: 0.046007838 lr:0.00032393762\n",
      "5200: accuracy:1.0 loss: 0.10959066 lr:0.0003228207\n",
      "5200: ********* epoch 9 ********* test accuracy:0.9829 test loss: 6.939024\n",
      "5210: accuracy:1.0 loss: 0.03516096 lr:0.00032170943\n",
      "5220: accuracy:1.0 loss: 0.03481998 lr:0.00032060363\n",
      "5230: accuracy:1.0 loss: 0.528471 lr:0.0003195034\n",
      "5240: accuracy:1.0 loss: 0.09675503 lr:0.00031840857\n",
      "5250: accuracy:1.0 loss: 0.12692863 lr:0.00031731924\n",
      "5250: ********* epoch 9 ********* test accuracy:0.9834 test loss: 6.899219\n",
      "5260: accuracy:1.0 loss: 0.07768267 lr:0.00031623538\n",
      "5270: accuracy:1.0 loss: 0.08733395 lr:0.00031515688\n",
      "5280: accuracy:1.0 loss: 0.08000745 lr:0.0003140838\n",
      "5290: accuracy:1.0 loss: 0.18657875 lr:0.00031301603\n",
      "5300: accuracy:1.0 loss: 0.138737 lr:0.00031195363\n",
      "5300: ********* epoch 9 ********* test accuracy:0.9834 test loss: 6.9139013\n",
      "5310: accuracy:1.0 loss: 0.14893152 lr:0.0003108965\n",
      "5320: accuracy:1.0 loss: 0.17783447 lr:0.00030984465\n",
      "5330: accuracy:1.0 loss: 0.22724694 lr:0.00030879804\n",
      "5340: accuracy:1.0 loss: 0.12802358 lr:0.00030775668\n",
      "5350: accuracy:1.0 loss: 0.16934942 lr:0.00030672047\n",
      "5350: ********* epoch 9 ********* test accuracy:0.9814 test loss: 6.858193\n",
      "5360: accuracy:1.0 loss: 0.60865766 lr:0.00030568946\n",
      "5370: accuracy:1.0 loss: 0.14879397 lr:0.00030466355\n",
      "5380: accuracy:1.0 loss: 0.32775977 lr:0.00030364282\n",
      "5390: accuracy:1.0 loss: 0.1527204 lr:0.00030262713\n",
      "5400: accuracy:1.0 loss: 0.34508744 lr:0.00030161653\n",
      "5400: ********* epoch 10 ********* test accuracy:0.9829 test loss: 6.9772706\n",
      "5410: accuracy:1.0 loss: 0.64563835 lr:0.00030061096\n",
      "5420: accuracy:1.0 loss: 0.8106394 lr:0.00029961043\n",
      "5430: accuracy:1.0 loss: 0.2366069 lr:0.00029861485\n",
      "5440: accuracy:1.0 loss: 0.09733556 lr:0.00029762427\n",
      "5450: accuracy:1.0 loss: 0.120291576 lr:0.0002966386\n",
      "5450: ********* epoch 10 ********* test accuracy:0.9837 test loss: 6.8933554\n",
      "5460: accuracy:1.0 loss: 0.23990537 lr:0.00029565787\n",
      "5470: accuracy:1.0 loss: 0.23385811 lr:0.00029468202\n",
      "5480: accuracy:1.0 loss: 0.29982656 lr:0.00029371103\n",
      "5490: accuracy:1.0 loss: 0.05658475 lr:0.0002927449\n",
      "5500: accuracy:1.0 loss: 0.29331073 lr:0.00029178357\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9824 test loss: 7.008571\n",
      "5510: accuracy:1.0 loss: 0.08553704 lr:0.00029082707\n",
      "5520: accuracy:1.0 loss: 0.6391762 lr:0.0002898753\n",
      "5530: accuracy:1.0 loss: 0.13523051 lr:0.0002889283\n",
      "5540: accuracy:1.0 loss: 0.4355348 lr:0.00028798598\n",
      "5550: accuracy:1.0 loss: 0.24414997 lr:0.00028704843\n",
      "5550: ********* epoch 10 ********* test accuracy:0.9839 test loss: 6.964154\n",
      "5560: accuracy:1.0 loss: 0.3165589 lr:0.0002861155\n",
      "5570: accuracy:1.0 loss: 0.19001481 lr:0.00028518727\n",
      "5580: accuracy:1.0 loss: 0.3335223 lr:0.0002842636\n",
      "5590: accuracy:1.0 loss: 0.2127226 lr:0.00028334462\n",
      "5600: accuracy:1.0 loss: 0.30794847 lr:0.00028243018\n",
      "5600: ********* epoch 10 ********* test accuracy:0.983 test loss: 6.946516\n",
      "5610: accuracy:1.0 loss: 0.15018556 lr:0.0002815203\n",
      "5620: accuracy:1.0 loss: 0.045233898 lr:0.00028061497\n",
      "5630: accuracy:1.0 loss: 0.119069524 lr:0.00027971415\n",
      "5640: accuracy:1.0 loss: 0.0687701 lr:0.0002788178\n",
      "5650: accuracy:1.0 loss: 0.12537634 lr:0.00027792598\n",
      "5650: ********* epoch 10 ********* test accuracy:0.9832 test loss: 6.8384995\n",
      "5660: accuracy:1.0 loss: 0.30012092 lr:0.00027703855\n",
      "5670: accuracy:1.0 loss: 0.07184046 lr:0.0002761556\n",
      "5680: accuracy:1.0 loss: 0.040159214 lr:0.00027527698\n",
      "5690: accuracy:1.0 loss: 0.22001088 lr:0.00027440282\n",
      "5700: accuracy:1.0 loss: 0.3642081 lr:0.00027353296\n",
      "5700: ********* epoch 10 ********* test accuracy:0.9834 test loss: 6.8797774\n",
      "5710: accuracy:1.0 loss: 0.20066513 lr:0.00027266747\n",
      "5720: accuracy:1.0 loss: 0.24259114 lr:0.00027180626\n",
      "5730: accuracy:1.0 loss: 0.10018241 lr:0.00027094936\n",
      "5740: accuracy:1.0 loss: 0.08690971 lr:0.00027009676\n",
      "5750: accuracy:1.0 loss: 0.123731375 lr:0.00026924838\n",
      "5750: ********* epoch 10 ********* test accuracy:0.9836 test loss: 6.864199\n",
      "5760: accuracy:0.99 loss: 1.7060142 lr:0.00026840428\n",
      "5770: accuracy:1.0 loss: 0.03503281 lr:0.00026756435\n",
      "5780: accuracy:1.0 loss: 0.06054386 lr:0.00026672863\n",
      "5790: accuracy:1.0 loss: 0.12211003 lr:0.00026589705\n",
      "5800: accuracy:1.0 loss: 0.1522614 lr:0.00026506966\n",
      "5800: ********* epoch 10 ********* test accuracy:0.9829 test loss: 6.911454\n",
      "5810: accuracy:1.0 loss: 0.3848452 lr:0.00026424637\n",
      "5820: accuracy:1.0 loss: 0.099139266 lr:0.00026342718\n",
      "5830: accuracy:1.0 loss: 0.7621189 lr:0.00026261207\n",
      "5840: accuracy:1.0 loss: 0.23995656 lr:0.00026180106\n",
      "5850: accuracy:1.0 loss: 0.08054593 lr:0.00026099404\n",
      "5850: ********* epoch 10 ********* test accuracy:0.983 test loss: 7.037097\n",
      "5860: accuracy:1.0 loss: 0.20318854 lr:0.0002601911\n",
      "5870: accuracy:1.0 loss: 0.22489367 lr:0.00025939214\n",
      "5880: accuracy:1.0 loss: 0.13409223 lr:0.0002585972\n",
      "5890: accuracy:1.0 loss: 0.41605622 lr:0.00025780615\n",
      "5900: accuracy:1.0 loss: 0.12654415 lr:0.00025701913\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9831 test loss: 6.9367466\n",
      "5910: accuracy:1.0 loss: 0.3389105 lr:0.00025623597\n",
      "5920: accuracy:1.0 loss: 0.09373867 lr:0.00025545675\n",
      "5930: accuracy:1.0 loss: 0.07365683 lr:0.0002546814\n",
      "5940: accuracy:1.0 loss: 0.2172538 lr:0.00025390994\n",
      "5950: accuracy:1.0 loss: 0.14666668 lr:0.0002531423\n",
      "5950: ********* epoch 10 ********* test accuracy:0.9833 test loss: 6.9016466\n",
      "5960: accuracy:1.0 loss: 0.0739396 lr:0.00025237846\n",
      "5970: accuracy:1.0 loss: 0.14592853 lr:0.0002516185\n",
      "5980: accuracy:1.0 loss: 0.17629585 lr:0.0002508623\n",
      "5990: accuracy:1.0 loss: 0.13950317 lr:0.00025010988\n",
      "6000: accuracy:1.0 loss: 0.36919487 lr:0.00024936118\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9839 test loss: 6.9649353\n",
      "6010: accuracy:1.0 loss: 0.60648936 lr:0.00024861627\n",
      "6020: accuracy:1.0 loss: 0.053895473 lr:0.000247875\n",
      "6030: accuracy:1.0 loss: 0.05894989 lr:0.0002471375\n",
      "6040: accuracy:1.0 loss: 0.13090476 lr:0.00024640362\n",
      "6050: accuracy:1.0 loss: 0.17483632 lr:0.00024567347\n",
      "6050: ********* epoch 11 ********* test accuracy:0.9829 test loss: 6.9364233\n",
      "6060: accuracy:1.0 loss: 0.14093165 lr:0.0002449469\n",
      "6070: accuracy:1.0 loss: 0.2703287 lr:0.00024422398\n",
      "6080: accuracy:1.0 loss: 0.13439974 lr:0.00024350465\n",
      "6090: accuracy:1.0 loss: 0.103215836 lr:0.00024278893\n",
      "6100: accuracy:1.0 loss: 0.08900099 lr:0.00024207676\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9833 test loss: 7.0180893\n",
      "6110: accuracy:1.0 loss: 0.12750098 lr:0.00024136817\n",
      "6120: accuracy:1.0 loss: 0.19472036 lr:0.00024066307\n",
      "6130: accuracy:1.0 loss: 0.09848884 lr:0.00023996152\n",
      "6140: accuracy:1.0 loss: 0.18811522 lr:0.00023926346\n",
      "6150: accuracy:1.0 loss: 0.1616953 lr:0.00023856887\n",
      "6150: ********* epoch 11 ********* test accuracy:0.9832 test loss: 7.024672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6160: accuracy:1.0 loss: 0.10490968 lr:0.00023787777\n",
      "6170: accuracy:1.0 loss: 0.32278606 lr:0.0002371901\n",
      "6180: accuracy:1.0 loss: 0.06578855 lr:0.00023650585\n",
      "6190: accuracy:1.0 loss: 0.08300308 lr:0.00023582505\n",
      "6200: accuracy:1.0 loss: 0.026202984 lr:0.0002351476\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9824 test loss: 7.153768\n",
      "6210: accuracy:1.0 loss: 0.08789939 lr:0.00023447353\n",
      "6220: accuracy:1.0 loss: 0.23710376 lr:0.00023380286\n",
      "6230: accuracy:1.0 loss: 0.033850882 lr:0.0002331355\n",
      "6240: accuracy:1.0 loss: 0.09419339 lr:0.0002324715\n",
      "6250: accuracy:1.0 loss: 0.06845573 lr:0.00023181079\n",
      "6250: ********* epoch 11 ********* test accuracy:0.9834 test loss: 7.0665946\n",
      "6260: accuracy:1.0 loss: 0.06737302 lr:0.00023115339\n",
      "6270: accuracy:1.0 loss: 0.04489597 lr:0.00023049922\n",
      "6280: accuracy:1.0 loss: 0.059678786 lr:0.00022984837\n",
      "6290: accuracy:1.0 loss: 0.12233178 lr:0.00022920076\n",
      "6300: accuracy:1.0 loss: 0.058271345 lr:0.00022855637\n",
      "6300: ********* epoch 11 ********* test accuracy:0.9838 test loss: 7.0747733\n",
      "6310: accuracy:1.0 loss: 0.1264674 lr:0.00022791518\n",
      "6320: accuracy:1.0 loss: 0.086417094 lr:0.00022727723\n",
      "6330: accuracy:1.0 loss: 0.043269403 lr:0.00022664241\n",
      "6340: accuracy:1.0 loss: 0.16835228 lr:0.0002260108\n",
      "6350: accuracy:1.0 loss: 0.051081814 lr:0.0002253823\n",
      "6350: ********* epoch 11 ********* test accuracy:0.9837 test loss: 7.116718\n",
      "6360: accuracy:1.0 loss: 0.046788085 lr:0.00022475695\n",
      "6370: accuracy:1.0 loss: 0.23380765 lr:0.00022413471\n",
      "6380: accuracy:1.0 loss: 0.08334668 lr:0.00022351561\n",
      "6390: accuracy:1.0 loss: 0.105358 lr:0.00022289957\n",
      "6400: accuracy:1.0 loss: 0.064410776 lr:0.00022228662\n",
      "6400: ********* epoch 11 ********* test accuracy:0.9834 test loss: 7.1365\n",
      "6410: accuracy:1.0 loss: 0.07695395 lr:0.00022167669\n",
      "6420: accuracy:1.0 loss: 0.12268891 lr:0.00022106984\n",
      "6430: accuracy:1.0 loss: 0.12095889 lr:0.000220466\n",
      "6440: accuracy:1.0 loss: 0.27312404 lr:0.00021986515\n",
      "6450: accuracy:1.0 loss: 0.043014448 lr:0.00021926734\n",
      "6450: ********* epoch 11 ********* test accuracy:0.9833 test loss: 7.072038\n",
      "6460: accuracy:1.0 loss: 0.14808194 lr:0.00021867247\n",
      "6470: accuracy:1.0 loss: 0.036268 lr:0.0002180806\n",
      "6480: accuracy:1.0 loss: 0.04496233 lr:0.00021749166\n",
      "6490: accuracy:1.0 loss: 0.04997488 lr:0.0002169057\n",
      "6500: accuracy:1.0 loss: 0.09471701 lr:0.0002163226\n",
      "6500: ********* epoch 11 ********* test accuracy:0.9831 test loss: 7.1070857\n",
      "6510: accuracy:1.0 loss: 0.026452646 lr:0.00021574246\n",
      "6520: accuracy:1.0 loss: 0.03069456 lr:0.00021516517\n",
      "6530: accuracy:1.0 loss: 0.04276414 lr:0.0002145908\n",
      "6540: accuracy:1.0 loss: 0.13314483 lr:0.00021401927\n",
      "6550: accuracy:1.0 loss: 0.09011912 lr:0.0002134506\n",
      "6550: ********* epoch 11 ********* test accuracy:0.9835 test loss: 7.029487\n",
      "6560: accuracy:1.0 loss: 0.14532398 lr:0.00021288476\n",
      "6570: accuracy:1.0 loss: 0.08694654 lr:0.00021232176\n",
      "6580: accuracy:1.0 loss: 0.104210675 lr:0.00021176154\n",
      "6590: accuracy:1.0 loss: 0.27591634 lr:0.00021120414\n",
      "6600: accuracy:1.0 loss: 0.09224503 lr:0.0002106495\n",
      "6600: ********* epoch 12 ********* test accuracy:0.9837 test loss: 7.018126\n",
      "6610: accuracy:1.0 loss: 0.06491268 lr:0.00021009764\n",
      "6620: accuracy:1.0 loss: 0.07030923 lr:0.00020954851\n",
      "6630: accuracy:1.0 loss: 0.06885301 lr:0.00020900214\n",
      "6640: accuracy:1.0 loss: 0.17441413 lr:0.00020845848\n",
      "6650: accuracy:1.0 loss: 0.2076106 lr:0.00020791756\n",
      "6650: ********* epoch 12 ********* test accuracy:0.9837 test loss: 6.9551544\n",
      "6660: accuracy:1.0 loss: 0.17686135 lr:0.00020737931\n",
      "6670: accuracy:1.0 loss: 0.02903163 lr:0.00020684375\n",
      "6680: accuracy:1.0 loss: 0.12347756 lr:0.00020631085\n",
      "6690: accuracy:1.0 loss: 0.19342288 lr:0.00020578061\n",
      "6700: accuracy:1.0 loss: 0.05888869 lr:0.00020525305\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9833 test loss: 6.904874\n",
      "6710: accuracy:1.0 loss: 0.010198784 lr:0.00020472809\n",
      "6720: accuracy:1.0 loss: 0.3502958 lr:0.00020420577\n",
      "6730: accuracy:1.0 loss: 0.13682681 lr:0.00020368604\n",
      "6740: accuracy:1.0 loss: 0.11537946 lr:0.00020316891\n",
      "6750: accuracy:1.0 loss: 0.042811245 lr:0.00020265434\n",
      "6750: ********* epoch 12 ********* test accuracy:0.9831 test loss: 7.0242963\n",
      "6760: accuracy:1.0 loss: 0.15082791 lr:0.00020214237\n",
      "6770: accuracy:1.0 loss: 0.19001901 lr:0.00020163291\n",
      "6780: accuracy:1.0 loss: 0.25726777 lr:0.00020112604\n",
      "6790: accuracy:1.0 loss: 0.12531492 lr:0.00020062165\n",
      "6800: accuracy:1.0 loss: 0.16494709 lr:0.0002001198\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9838 test loss: 7.112628\n",
      "6810: accuracy:1.0 loss: 0.058595452 lr:0.00019962045\n",
      "6820: accuracy:1.0 loss: 0.046812512 lr:0.0001991236\n",
      "6830: accuracy:1.0 loss: 0.052118517 lr:0.0001986292\n",
      "6840: accuracy:1.0 loss: 0.066180795 lr:0.0001981373\n",
      "6850: accuracy:1.0 loss: 0.19355655 lr:0.00019764784\n",
      "6850: ********* epoch 12 ********* test accuracy:0.9835 test loss: 7.0431643\n",
      "6860: accuracy:1.0 loss: 0.053410064 lr:0.00019716081\n",
      "6870: accuracy:1.0 loss: 0.18204497 lr:0.00019667622\n",
      "6880: accuracy:1.0 loss: 0.10358761 lr:0.00019619406\n",
      "6890: accuracy:1.0 loss: 0.01757548 lr:0.00019571428\n",
      "6900: accuracy:1.0 loss: 0.2000048 lr:0.0001952369\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9837 test loss: 7.052096\n",
      "6910: accuracy:1.0 loss: 0.029600577 lr:0.0001947619\n",
      "6920: accuracy:1.0 loss: 0.14206876 lr:0.00019428927\n",
      "6930: accuracy:1.0 loss: 0.062543765 lr:0.00019381901\n",
      "6940: accuracy:1.0 loss: 0.15955259 lr:0.00019335106\n",
      "6950: accuracy:1.0 loss: 0.032780863 lr:0.00019288549\n",
      "6950: ********* epoch 12 ********* test accuracy:0.9831 test loss: 7.060028\n",
      "6960: accuracy:1.0 loss: 0.41553372 lr:0.00019242222\n",
      "6970: accuracy:1.0 loss: 0.21528374 lr:0.00019196127\n",
      "6980: accuracy:1.0 loss: 0.10026287 lr:0.0001915026\n",
      "6990: accuracy:1.0 loss: 0.04065997 lr:0.00019104625\n",
      "7000: accuracy:1.0 loss: 0.066567495 lr:0.00019059214\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9834 test loss: 7.0946164\n",
      "7010: accuracy:1.0 loss: 0.08258852 lr:0.00019014032\n",
      "7020: accuracy:1.0 loss: 0.31607106 lr:0.00018969073\n",
      "7030: accuracy:1.0 loss: 0.07158908 lr:0.00018924341\n",
      "7040: accuracy:1.0 loss: 0.04969071 lr:0.0001887983\n",
      "7050: accuracy:1.0 loss: 0.035250507 lr:0.00018835542\n",
      "7050: ********* epoch 12 ********* test accuracy:0.9833 test loss: 7.169088\n",
      "7060: accuracy:1.0 loss: 0.09840087 lr:0.00018791473\n",
      "7070: accuracy:1.0 loss: 0.031128185 lr:0.00018747627\n",
      "7080: accuracy:1.0 loss: 0.048494678 lr:0.00018703997\n",
      "7090: accuracy:1.0 loss: 0.083076835 lr:0.00018660587\n",
      "7100: accuracy:1.0 loss: 0.06861704 lr:0.00018617391\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9833 test loss: 7.169808\n",
      "7110: accuracy:1.0 loss: 0.20937555 lr:0.00018574412\n",
      "7120: accuracy:1.0 loss: 0.2420745 lr:0.00018531646\n",
      "7130: accuracy:1.0 loss: 0.009303279 lr:0.00018489096\n",
      "7140: accuracy:1.0 loss: 0.058404535 lr:0.00018446756\n",
      "7150: accuracy:1.0 loss: 0.13062525 lr:0.00018404625\n",
      "7150: ********* epoch 12 ********* test accuracy:0.9836 test loss: 7.17884\n",
      "7160: accuracy:1.0 loss: 0.06988951 lr:0.00018362708\n",
      "7170: accuracy:1.0 loss: 0.055602834 lr:0.00018320998\n",
      "7180: accuracy:1.0 loss: 0.014848474 lr:0.00018279499\n",
      "7190: accuracy:1.0 loss: 0.081658855 lr:0.00018238203\n",
      "7200: accuracy:1.0 loss: 0.023967229 lr:0.00018197116\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9836 test loss: 7.178646\n",
      "7210: accuracy:1.0 loss: 0.05317179 lr:0.00018156233\n",
      "7220: accuracy:1.0 loss: 0.04545532 lr:0.00018115554\n",
      "7230: accuracy:1.0 loss: 0.07205856 lr:0.00018075077\n",
      "7240: accuracy:1.0 loss: 0.05790556 lr:0.00018034803\n",
      "7250: accuracy:1.0 loss: 0.21963793 lr:0.00017994728\n",
      "7250: ********* epoch 13 ********* test accuracy:0.9834 test loss: 7.256028\n",
      "7260: accuracy:1.0 loss: 0.102493316 lr:0.00017954854\n",
      "7270: accuracy:1.0 loss: 0.043368056 lr:0.0001791518\n",
      "7280: accuracy:1.0 loss: 0.052638438 lr:0.00017875704\n",
      "7290: accuracy:1.0 loss: 0.10564824 lr:0.00017836422\n",
      "7300: accuracy:1.0 loss: 0.05127912 lr:0.00017797339\n",
      "7300: ********* epoch 13 ********* test accuracy:0.9831 test loss: 7.288678\n",
      "7310: accuracy:1.0 loss: 0.17906396 lr:0.00017758447\n",
      "7320: accuracy:1.0 loss: 0.16733627 lr:0.00017719754\n",
      "7330: accuracy:1.0 loss: 0.17324197 lr:0.0001768125\n",
      "7340: accuracy:1.0 loss: 0.24840137 lr:0.0001764294\n",
      "7350: accuracy:1.0 loss: 0.3064979 lr:0.0001760482\n",
      "7350: ********* epoch 13 ********* test accuracy:0.9831 test loss: 7.2618337\n",
      "7360: accuracy:1.0 loss: 0.06912546 lr:0.00017566892\n",
      "7370: accuracy:1.0 loss: 0.050924152 lr:0.00017529153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7380: accuracy:1.0 loss: 0.10747405 lr:0.000174916\n",
      "7390: accuracy:1.0 loss: 0.19510347 lr:0.00017454235\n",
      "7400: accuracy:1.0 loss: 0.13520934 lr:0.00017417056\n",
      "7400: ********* epoch 13 ********* test accuracy:0.9832 test loss: 7.2717786\n",
      "7410: accuracy:1.0 loss: 0.047064517 lr:0.00017380065\n",
      "7420: accuracy:1.0 loss: 0.048044592 lr:0.00017343255\n",
      "7430: accuracy:1.0 loss: 0.18533497 lr:0.0001730663\n",
      "7440: accuracy:1.0 loss: 0.083964825 lr:0.0001727019\n",
      "7450: accuracy:1.0 loss: 0.09939411 lr:0.00017233929\n",
      "7450: ********* epoch 13 ********* test accuracy:0.9836 test loss: 7.3883576\n",
      "7460: accuracy:1.0 loss: 0.15239774 lr:0.00017197849\n",
      "7470: accuracy:1.0 loss: 0.043523487 lr:0.00017161951\n",
      "7480: accuracy:1.0 loss: 0.022529691 lr:0.0001712623\n",
      "7490: accuracy:1.0 loss: 0.09610679 lr:0.00017090689\n",
      "7500: accuracy:1.0 loss: 0.050656747 lr:0.00017055322\n",
      "7500: ********* epoch 13 ********* test accuracy:0.9832 test loss: 7.3212566\n",
      "7510: accuracy:1.0 loss: 0.080132455 lr:0.00017020135\n",
      "7520: accuracy:1.0 loss: 0.05079281 lr:0.0001698512\n",
      "7530: accuracy:1.0 loss: 0.036438055 lr:0.00016950283\n",
      "7540: accuracy:1.0 loss: 0.13012838 lr:0.00016915618\n",
      "7550: accuracy:1.0 loss: 0.13786665 lr:0.00016881127\n",
      "7550: ********* epoch 13 ********* test accuracy:0.9834 test loss: 7.3302464\n",
      "7560: accuracy:1.0 loss: 0.1198875 lr:0.00016846808\n",
      "7570: accuracy:1.0 loss: 0.06682399 lr:0.00016812659\n",
      "7580: accuracy:1.0 loss: 0.045449965 lr:0.0001677868\n",
      "7590: accuracy:1.0 loss: 0.10377835 lr:0.00016744871\n",
      "7600: accuracy:1.0 loss: 0.072185546 lr:0.0001671123\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9841 test loss: 7.271263\n",
      "7610: accuracy:1.0 loss: 0.11449138 lr:0.0001667776\n",
      "7620: accuracy:1.0 loss: 0.0350047 lr:0.00016644453\n",
      "7630: accuracy:1.0 loss: 0.06397046 lr:0.00016611312\n",
      "7640: accuracy:1.0 loss: 0.05510935 lr:0.0001657834\n",
      "7650: accuracy:1.0 loss: 0.13323924 lr:0.00016545529\n",
      "7650: ********* epoch 13 ********* test accuracy:0.9835 test loss: 7.3309345\n",
      "7660: accuracy:1.0 loss: 0.056416437 lr:0.00016512885\n",
      "7670: accuracy:1.0 loss: 0.034357704 lr:0.00016480399\n",
      "7680: accuracy:1.0 loss: 0.11828447 lr:0.00016448079\n",
      "7690: accuracy:1.0 loss: 0.111186944 lr:0.0001641592\n",
      "7700: accuracy:1.0 loss: 0.2558769 lr:0.0001638392\n",
      "7700: ********* epoch 13 ********* test accuracy:0.9836 test loss: 7.3751526\n",
      "7710: accuracy:1.0 loss: 0.055707477 lr:0.0001635208\n",
      "7720: accuracy:1.0 loss: 0.073650576 lr:0.000163204\n",
      "7730: accuracy:1.0 loss: 0.060947828 lr:0.00016288875\n",
      "7740: accuracy:1.0 loss: 0.069320984 lr:0.0001625751\n",
      "7750: accuracy:1.0 loss: 0.03014014 lr:0.00016226301\n",
      "7750: ********* epoch 13 ********* test accuracy:0.9838 test loss: 7.3436923\n",
      "7760: accuracy:1.0 loss: 0.20987469 lr:0.00016195247\n",
      "7770: accuracy:1.0 loss: 0.033098865 lr:0.00016164347\n",
      "7780: accuracy:1.0 loss: 0.15125497 lr:0.00016133604\n",
      "7790: accuracy:1.0 loss: 0.12748303 lr:0.00016103011\n",
      "7800: accuracy:1.0 loss: 0.010878874 lr:0.00016072573\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9831 test loss: 7.4216456\n",
      "7810: accuracy:1.0 loss: 0.04156711 lr:0.00016042285\n",
      "7820: accuracy:1.0 loss: 0.06267956 lr:0.0001601215\n",
      "7830: accuracy:1.0 loss: 0.02183224 lr:0.00015982163\n",
      "7840: accuracy:1.0 loss: 0.07910145 lr:0.00015952328\n",
      "7850: accuracy:1.0 loss: 0.04651371 lr:0.0001592264\n",
      "7850: ********* epoch 14 ********* test accuracy:0.9833 test loss: 7.370363\n",
      "7860: accuracy:1.0 loss: 0.059985828 lr:0.000158931\n",
      "7870: accuracy:1.0 loss: 0.041697297 lr:0.00015863709\n",
      "7880: accuracy:1.0 loss: 0.043895617 lr:0.00015834463\n",
      "7890: accuracy:1.0 loss: 0.123619616 lr:0.00015805365\n",
      "7900: accuracy:1.0 loss: 0.14441934 lr:0.0001577641\n",
      "7900: ********* epoch 14 ********* test accuracy:0.9838 test loss: 7.3802137\n",
      "7910: accuracy:1.0 loss: 0.20981306 lr:0.000157476\n",
      "7920: accuracy:1.0 loss: 0.12837662 lr:0.00015718934\n",
      "7930: accuracy:1.0 loss: 0.16200304 lr:0.0001569041\n",
      "7940: accuracy:1.0 loss: 0.022318104 lr:0.00015662028\n",
      "7950: accuracy:1.0 loss: 0.057854537 lr:0.00015633789\n",
      "7950: ********* epoch 14 ********* test accuracy:0.9832 test loss: 7.403971\n",
      "7960: accuracy:1.0 loss: 0.025090039 lr:0.0001560569\n",
      "7970: accuracy:1.0 loss: 0.116616905 lr:0.00015577732\n",
      "7980: accuracy:1.0 loss: 0.19326834 lr:0.00015549913\n",
      "7990: accuracy:1.0 loss: 0.073263824 lr:0.00015522234\n",
      "8000: accuracy:1.0 loss: 0.028205032 lr:0.00015494692\n",
      "8000: ********* epoch 14 ********* test accuracy:0.9836 test loss: 7.322465\n",
      "8010: accuracy:1.0 loss: 0.029827671 lr:0.00015467286\n",
      "8020: accuracy:1.0 loss: 0.058090173 lr:0.00015440017\n",
      "8030: accuracy:1.0 loss: 0.13326643 lr:0.00015412885\n",
      "8040: accuracy:1.0 loss: 0.034157805 lr:0.0001538589\n",
      "8050: accuracy:1.0 loss: 0.049494162 lr:0.00015359027\n",
      "8050: ********* epoch 14 ********* test accuracy:0.9831 test loss: 7.3947787\n",
      "8060: accuracy:1.0 loss: 0.036164388 lr:0.00015332298\n",
      "8070: accuracy:1.0 loss: 0.07873808 lr:0.00015305703\n",
      "8080: accuracy:1.0 loss: 0.07317874 lr:0.00015279242\n",
      "8090: accuracy:1.0 loss: 0.096752375 lr:0.00015252912\n",
      "8100: accuracy:1.0 loss: 0.09419406 lr:0.00015226711\n",
      "8100: ********* epoch 14 ********* test accuracy:0.9833 test loss: 7.414604\n",
      "8110: accuracy:1.0 loss: 0.26074314 lr:0.00015200643\n",
      "8120: accuracy:1.0 loss: 0.041070886 lr:0.00015174704\n",
      "8130: accuracy:1.0 loss: 0.046014894 lr:0.00015148896\n",
      "8140: accuracy:1.0 loss: 0.13283892 lr:0.00015123216\n",
      "8150: accuracy:1.0 loss: 0.041305367 lr:0.00015097663\n",
      "8150: ********* epoch 14 ********* test accuracy:0.9832 test loss: 7.414569\n",
      "8160: accuracy:1.0 loss: 0.025109312 lr:0.00015072238\n",
      "8170: accuracy:1.0 loss: 0.0055942456 lr:0.00015046942\n",
      "8180: accuracy:1.0 loss: 0.052473716 lr:0.0001502177\n",
      "8190: accuracy:1.0 loss: 0.07523019 lr:0.00014996724\n",
      "8200: accuracy:1.0 loss: 0.0116658565 lr:0.000149718\n",
      "8200: ********* epoch 14 ********* test accuracy:0.9832 test loss: 7.4178348\n",
      "8210: accuracy:1.0 loss: 0.059480865 lr:0.00014947006\n",
      "8220: accuracy:1.0 loss: 0.09284645 lr:0.00014922331\n",
      "8230: accuracy:1.0 loss: 0.035910517 lr:0.00014897781\n",
      "8240: accuracy:1.0 loss: 0.043531146 lr:0.00014873352\n",
      "8250: accuracy:1.0 loss: 0.105270684 lr:0.00014849048\n",
      "8250: ********* epoch 14 ********* test accuracy:0.9835 test loss: 7.5200834\n",
      "8260: accuracy:1.0 loss: 0.040290616 lr:0.00014824864\n",
      "8270: accuracy:1.0 loss: 0.0754788 lr:0.00014800798\n",
      "8280: accuracy:1.0 loss: 0.11213261 lr:0.00014776854\n",
      "8290: accuracy:1.0 loss: 0.044604145 lr:0.00014753031\n",
      "8300: accuracy:1.0 loss: 0.042018328 lr:0.00014729325\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9838 test loss: 7.4784346\n",
      "8310: accuracy:1.0 loss: 0.040390655 lr:0.00014705736\n",
      "8320: accuracy:1.0 loss: 0.0416988 lr:0.00014682265\n",
      "8330: accuracy:1.0 loss: 0.044299673 lr:0.00014658915\n",
      "8340: accuracy:1.0 loss: 0.0062071164 lr:0.00014635678\n",
      "8350: accuracy:1.0 loss: 0.069402345 lr:0.00014612557\n",
      "8350: ********* epoch 14 ********* test accuracy:0.9831 test loss: 7.509389\n",
      "8360: accuracy:1.0 loss: 0.07998487 lr:0.00014589551\n",
      "8370: accuracy:1.0 loss: 0.020618942 lr:0.00014566659\n",
      "8380: accuracy:1.0 loss: 0.14533034 lr:0.00014543885\n",
      "8390: accuracy:1.0 loss: 0.03323391 lr:0.00014521222\n",
      "8400: accuracy:1.0 loss: 0.1081147 lr:0.00014498673\n",
      "8400: ********* epoch 15 ********* test accuracy:0.9836 test loss: 7.5086913\n",
      "8410: accuracy:1.0 loss: 0.06894096 lr:0.00014476234\n",
      "8420: accuracy:1.0 loss: 0.23961426 lr:0.00014453911\n",
      "8430: accuracy:1.0 loss: 0.08419126 lr:0.00014431696\n",
      "8440: accuracy:1.0 loss: 0.048491254 lr:0.00014409592\n",
      "8450: accuracy:1.0 loss: 0.0050893244 lr:0.000143876\n",
      "8450: ********* epoch 15 ********* test accuracy:0.983 test loss: 7.4451737\n",
      "8460: accuracy:1.0 loss: 0.014534003 lr:0.00014365718\n",
      "8470: accuracy:1.0 loss: 0.031789653 lr:0.00014343942\n",
      "8480: accuracy:1.0 loss: 0.12559354 lr:0.00014322277\n",
      "8490: accuracy:1.0 loss: 0.025545442 lr:0.00014300719\n",
      "8500: accuracy:1.0 loss: 0.06526383 lr:0.0001427927\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9831 test loss: 7.5592227\n",
      "8510: accuracy:1.0 loss: 0.09406886 lr:0.00014257927\n",
      "8520: accuracy:1.0 loss: 0.08696854 lr:0.0001423669\n",
      "8530: accuracy:1.0 loss: 0.021184608 lr:0.0001421556\n",
      "8540: accuracy:1.0 loss: 0.047860157 lr:0.00014194535\n",
      "8550: accuracy:1.0 loss: 0.049482763 lr:0.00014173615\n",
      "8550: ********* epoch 15 ********* test accuracy:0.984 test loss: 7.5209312\n",
      "8560: accuracy:1.0 loss: 0.11978278 lr:0.00014152798\n",
      "8570: accuracy:1.0 loss: 0.024270477 lr:0.00014132085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8580: accuracy:1.0 loss: 0.05669255 lr:0.00014111475\n",
      "8590: accuracy:1.0 loss: 0.04060779 lr:0.00014090972\n",
      "8600: accuracy:1.0 loss: 0.001812724 lr:0.00014070567\n",
      "8600: ********* epoch 15 ********* test accuracy:0.9839 test loss: 7.447438\n",
      "8610: accuracy:1.0 loss: 0.015443654 lr:0.00014050264\n",
      "8620: accuracy:1.0 loss: 0.020420946 lr:0.00014030063\n",
      "8630: accuracy:1.0 loss: 0.033626236 lr:0.00014009964\n",
      "8640: accuracy:1.0 loss: 0.041507743 lr:0.00013989964\n",
      "8650: accuracy:1.0 loss: 0.056514256 lr:0.00013970064\n",
      "8650: ********* epoch 15 ********* test accuracy:0.9838 test loss: 7.475617\n",
      "8660: accuracy:1.0 loss: 0.04213275 lr:0.00013950263\n",
      "8670: accuracy:1.0 loss: 0.07328377 lr:0.00013930563\n",
      "8680: accuracy:1.0 loss: 0.06326685 lr:0.00013910959\n",
      "8690: accuracy:1.0 loss: 0.031148639 lr:0.00013891452\n",
      "8700: accuracy:1.0 loss: 0.08127371 lr:0.00013872042\n",
      "8700: ********* epoch 15 ********* test accuracy:0.9837 test loss: 7.521419\n",
      "8710: accuracy:1.0 loss: 0.03916387 lr:0.00013852732\n",
      "8720: accuracy:1.0 loss: 0.07862659 lr:0.00013833516\n",
      "8730: accuracy:1.0 loss: 0.06191658 lr:0.00013814395\n",
      "8740: accuracy:1.0 loss: 0.060963713 lr:0.00013795371\n",
      "8750: accuracy:1.0 loss: 0.043130383 lr:0.00013776444\n",
      "8750: ********* epoch 15 ********* test accuracy:0.9828 test loss: 7.52926\n",
      "8760: accuracy:1.0 loss: 0.047782525 lr:0.00013757608\n",
      "8770: accuracy:1.0 loss: 0.087831326 lr:0.00013738866\n",
      "8780: accuracy:1.0 loss: 0.27184513 lr:0.00013720218\n",
      "8790: accuracy:1.0 loss: 0.09251957 lr:0.00013701664\n",
      "8800: accuracy:1.0 loss: 0.14404176 lr:0.00013683202\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9833 test loss: 7.6019607\n",
      "8810: accuracy:1.0 loss: 0.10310105 lr:0.0001366483\n",
      "8820: accuracy:1.0 loss: 0.06676647 lr:0.00013646553\n",
      "8830: accuracy:1.0 loss: 0.011957106 lr:0.00013628365\n",
      "8840: accuracy:1.0 loss: 0.14986484 lr:0.0001361027\n",
      "8850: accuracy:1.0 loss: 0.063082166 lr:0.00013592263\n",
      "8850: ********* epoch 15 ********* test accuracy:0.9835 test loss: 7.6413827\n",
      "8860: accuracy:1.0 loss: 0.081290275 lr:0.00013574347\n",
      "8870: accuracy:1.0 loss: 0.088487945 lr:0.00013556518\n",
      "8880: accuracy:1.0 loss: 0.2683962 lr:0.00013538782\n",
      "8890: accuracy:1.0 loss: 0.062469535 lr:0.00013521132\n",
      "8900: accuracy:1.0 loss: 0.03218924 lr:0.00013503569\n",
      "8900: ********* epoch 15 ********* test accuracy:0.9836 test loss: 7.6280856\n",
      "8910: accuracy:1.0 loss: 0.048196178 lr:0.00013486095\n",
      "8920: accuracy:1.0 loss: 0.024926793 lr:0.00013468708\n",
      "8930: accuracy:1.0 loss: 0.04523227 lr:0.00013451408\n",
      "8940: accuracy:1.0 loss: 0.049639773 lr:0.00013434194\n",
      "8950: accuracy:1.0 loss: 0.004207396 lr:0.00013417065\n",
      "8950: ********* epoch 15 ********* test accuracy:0.9838 test loss: 7.5912995\n",
      "8960: accuracy:1.0 loss: 0.094518736 lr:0.00013400023\n",
      "8970: accuracy:1.0 loss: 0.0075349063 lr:0.00013383066\n",
      "8980: accuracy:1.0 loss: 0.057966024 lr:0.00013366193\n",
      "8990: accuracy:1.0 loss: 0.035398595 lr:0.00013349403\n",
      "9000: accuracy:1.0 loss: 0.06734123 lr:0.00013332699\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9836 test loss: 7.5903506\n",
      "9010: accuracy:1.0 loss: 0.105132446 lr:0.00013316076\n",
      "9020: accuracy:1.0 loss: 0.023615517 lr:0.00013299537\n",
      "9030: accuracy:1.0 loss: 0.09264742 lr:0.00013283081\n",
      "9040: accuracy:1.0 loss: 0.066451825 lr:0.00013266708\n",
      "9050: accuracy:1.0 loss: 0.014114956 lr:0.00013250414\n",
      "9050: ********* epoch 16 ********* test accuracy:0.9837 test loss: 7.5743794\n",
      "9060: accuracy:1.0 loss: 0.09902546 lr:0.00013234201\n",
      "9070: accuracy:1.0 loss: 0.10501084 lr:0.0001321807\n",
      "9080: accuracy:1.0 loss: 0.051287144 lr:0.00013202021\n",
      "9090: accuracy:1.0 loss: 0.053277507 lr:0.00013186052\n",
      "9100: accuracy:1.0 loss: 0.05839403 lr:0.00013170161\n",
      "9100: ********* epoch 16 ********* test accuracy:0.9838 test loss: 7.5719686\n",
      "9110: accuracy:1.0 loss: 0.023795519 lr:0.0001315435\n",
      "9120: accuracy:1.0 loss: 0.058776345 lr:0.00013138616\n",
      "9130: accuracy:1.0 loss: 0.06930628 lr:0.00013122964\n",
      "9140: accuracy:1.0 loss: 0.034200974 lr:0.00013107387\n",
      "9150: accuracy:1.0 loss: 0.063209936 lr:0.0001309189\n",
      "9150: ********* epoch 16 ********* test accuracy:0.9834 test loss: 7.605841\n",
      "9160: accuracy:1.0 loss: 0.1287844 lr:0.00013076467\n",
      "9170: accuracy:1.0 loss: 0.013091424 lr:0.00013061125\n",
      "9180: accuracy:1.0 loss: 0.05411908 lr:0.00013045857\n",
      "9190: accuracy:1.0 loss: 0.023722451 lr:0.00013030665\n",
      "9200: accuracy:1.0 loss: 0.05348626 lr:0.0001301555\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9835 test loss: 7.740573\n",
      "9210: accuracy:1.0 loss: 0.044415172 lr:0.00013000511\n",
      "9220: accuracy:1.0 loss: 0.0742179 lr:0.00012985546\n",
      "9230: accuracy:1.0 loss: 0.037679162 lr:0.00012970655\n",
      "9240: accuracy:1.0 loss: 0.03155594 lr:0.00012955838\n",
      "9250: accuracy:1.0 loss: 0.022924114 lr:0.00012941097\n",
      "9250: ********* epoch 16 ********* test accuracy:0.9833 test loss: 7.6076455\n",
      "9260: accuracy:1.0 loss: 0.18351223 lr:0.00012926427\n",
      "9270: accuracy:1.0 loss: 0.02757326 lr:0.00012911831\n",
      "9280: accuracy:1.0 loss: 0.011625051 lr:0.00012897309\n",
      "9290: accuracy:1.0 loss: 0.14262678 lr:0.00012882857\n",
      "9300: accuracy:1.0 loss: 0.060325693 lr:0.0001286848\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9831 test loss: 7.6581492\n",
      "9310: accuracy:1.0 loss: 0.04933992 lr:0.00012854174\n",
      "9320: accuracy:1.0 loss: 0.0669208 lr:0.00012839938\n",
      "9330: accuracy:1.0 loss: 0.036173932 lr:0.00012825773\n",
      "9340: accuracy:1.0 loss: 0.05002713 lr:0.00012811681\n",
      "9350: accuracy:1.0 loss: 0.025675539 lr:0.00012797657\n",
      "9350: ********* epoch 16 ********* test accuracy:0.9841 test loss: 7.62453\n",
      "9360: accuracy:1.0 loss: 0.038549677 lr:0.00012783703\n",
      "9370: accuracy:1.0 loss: 0.103907675 lr:0.0001276982\n",
      "9380: accuracy:1.0 loss: 0.04443248 lr:0.00012756005\n",
      "9390: accuracy:1.0 loss: 0.012027238 lr:0.0001274226\n",
      "9400: accuracy:1.0 loss: 0.07075901 lr:0.00012728582\n",
      "9400: ********* epoch 16 ********* test accuracy:0.9835 test loss: 7.6515837\n",
      "9410: accuracy:1.0 loss: 0.02990597 lr:0.00012714973\n",
      "9420: accuracy:1.0 loss: 0.056780543 lr:0.00012701433\n",
      "9430: accuracy:1.0 loss: 0.050132494 lr:0.0001268796\n",
      "9440: accuracy:1.0 loss: 0.043204375 lr:0.00012674552\n",
      "9450: accuracy:1.0 loss: 0.10137735 lr:0.00012661214\n",
      "9450: ********* epoch 16 ********* test accuracy:0.9838 test loss: 7.68868\n",
      "9460: accuracy:1.0 loss: 0.0053915936 lr:0.00012647941\n",
      "9470: accuracy:1.0 loss: 0.022717781 lr:0.00012634734\n",
      "9480: accuracy:1.0 loss: 0.021225676 lr:0.00012621593\n",
      "9490: accuracy:1.0 loss: 0.06873678 lr:0.00012608517\n",
      "9500: accuracy:1.0 loss: 0.11504621 lr:0.00012595509\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9838 test loss: 7.6979322\n",
      "9510: accuracy:1.0 loss: 0.07007632 lr:0.00012582564\n",
      "9520: accuracy:1.0 loss: 0.2913029 lr:0.00012569682\n",
      "9530: accuracy:1.0 loss: 0.04724336 lr:0.00012556865\n",
      "9540: accuracy:1.0 loss: 0.11317134 lr:0.00012544113\n",
      "9550: accuracy:1.0 loss: 0.037751388 lr:0.00012531425\n",
      "9550: ********* epoch 16 ********* test accuracy:0.9826 test loss: 7.7167177\n",
      "9560: accuracy:1.0 loss: 0.04273124 lr:0.00012518799\n",
      "9570: accuracy:1.0 loss: 0.0360901 lr:0.00012506236\n",
      "9580: accuracy:1.0 loss: 0.1371481 lr:0.00012493736\n",
      "9590: accuracy:1.0 loss: 0.013787388 lr:0.000124813\n",
      "9600: accuracy:1.0 loss: 0.07056074 lr:0.00012468924\n",
      "9600: ********* epoch 17 ********* test accuracy:0.983 test loss: 7.8383503\n",
      "9610: accuracy:1.0 loss: 0.10926655 lr:0.0001245661\n",
      "9620: accuracy:1.0 loss: 0.036886644 lr:0.00012444357\n",
      "9630: accuracy:1.0 loss: 0.052115716 lr:0.00012432167\n",
      "9640: accuracy:1.0 loss: 0.091260746 lr:0.00012420036\n",
      "9650: accuracy:1.0 loss: 0.045266822 lr:0.00012407966\n",
      "9650: ********* epoch 17 ********* test accuracy:0.9836 test loss: 7.820107\n",
      "9660: accuracy:1.0 loss: 0.047182128 lr:0.00012395956\n",
      "9670: accuracy:1.0 loss: 0.21274689 lr:0.00012384006\n",
      "9680: accuracy:1.0 loss: 0.12876312 lr:0.00012372115\n",
      "9690: accuracy:1.0 loss: 0.041726507 lr:0.00012360285\n",
      "9700: accuracy:1.0 loss: 0.037342705 lr:0.00012348512\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9834 test loss: 7.779674\n",
      "9710: accuracy:1.0 loss: 0.0070567746 lr:0.000123368\n",
      "9720: accuracy:1.0 loss: 0.055272672 lr:0.00012325145\n",
      "9730: accuracy:1.0 loss: 0.29855767 lr:0.00012313548\n",
      "9740: accuracy:1.0 loss: 0.21987018 lr:0.00012302009\n",
      "9750: accuracy:1.0 loss: 0.013424833 lr:0.00012290529\n",
      "9750: ********* epoch 17 ********* test accuracy:0.9832 test loss: 7.799627\n",
      "9760: accuracy:1.0 loss: 0.026357431 lr:0.00012279104\n",
      "9770: accuracy:1.0 loss: 0.038222015 lr:0.00012267736\n",
      "9780: accuracy:1.0 loss: 0.02306817 lr:0.00012256426\n",
      "9790: accuracy:1.0 loss: 0.0334239 lr:0.00012245172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9800: accuracy:1.0 loss: 0.047215074 lr:0.00012233976\n",
      "9800: ********* epoch 17 ********* test accuracy:0.9837 test loss: 7.785306\n",
      "9810: accuracy:1.0 loss: 0.018477364 lr:0.00012222832\n",
      "9820: accuracy:1.0 loss: 0.05610091 lr:0.00012211746\n",
      "9830: accuracy:1.0 loss: 0.048528697 lr:0.00012200714\n",
      "9840: accuracy:1.0 loss: 0.053001195 lr:0.00012189739\n",
      "9850: accuracy:1.0 loss: 0.04856781 lr:0.00012178817\n",
      "9850: ********* epoch 17 ********* test accuracy:0.9833 test loss: 7.7736845\n",
      "9860: accuracy:1.0 loss: 0.049522527 lr:0.000121679506\n",
      "9870: accuracy:1.0 loss: 0.037264515 lr:0.00012157137\n",
      "9880: accuracy:1.0 loss: 0.02084042 lr:0.000121463796\n",
      "9890: accuracy:1.0 loss: 0.024743695 lr:0.000121356745\n",
      "9900: accuracy:1.0 loss: 0.012920622 lr:0.000121250225\n",
      "9900: ********* epoch 17 ********* test accuracy:0.9832 test loss: 7.792797\n",
      "9910: accuracy:1.0 loss: 0.07268228 lr:0.00012114423\n",
      "9920: accuracy:1.0 loss: 0.016947575 lr:0.000121038785\n",
      "9930: accuracy:1.0 loss: 0.029773688 lr:0.00012093385\n",
      "9940: accuracy:1.0 loss: 0.034910493 lr:0.00012082944\n",
      "9950: accuracy:1.0 loss: 0.08791222 lr:0.00012072555\n",
      "9950: ********* epoch 17 ********* test accuracy:0.9834 test loss: 7.8653297\n",
      "9960: accuracy:1.0 loss: 0.031149544 lr:0.000120622186\n",
      "9970: accuracy:1.0 loss: 0.046375375 lr:0.00012051933\n",
      "9980: accuracy:1.0 loss: 0.03559791 lr:0.00012041699\n",
      "9990: accuracy:1.0 loss: 0.03477008 lr:0.000120315155\n",
      "10000: accuracy:1.0 loss: 0.02197769 lr:0.00012021384\n",
      "10000: ********* epoch 17 ********* test accuracy:0.9831 test loss: 7.825657\n",
      "max test accuracy: 0.9841\n"
     ]
    }
   ],
   "source": [
    "# input X => 28x28 grayscale dimensions ('None' indexes images in mini-batch)\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "\n",
    "# correct answers\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights\n",
    "W1 = tf.Variable(tf.truncated_normal([28*28, 200], stddev=0.1))\n",
    "W2 = tf.Variable(tf.truncated_normal([200, 100], stddev=0.1))\n",
    "W3 = tf.Variable(tf.truncated_normal([100, 10], stddev=0.1))\n",
    "\n",
    "# biases\n",
    "b1 = tf.Variable(tf.ones([200]) / 10)\n",
    "b2 = tf.Variable(tf.ones([100]) / 10)\n",
    "b3 = tf.Variable(tf.ones([10]) / 10)\n",
    "\n",
    "# flatten images into single line\n",
    "XX = tf.reshape(X, [-1, 28*28])\n",
    "\n",
    "# model\n",
    "Y1 = tf.nn.relu(tf.matmul(XX, W1) + b1)\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)\n",
    "Y_logits = tf.matmul(Y2, W3) + b3\n",
    "Y = tf.nn.softmax(Y_logits)\n",
    "\n",
    "# cross entropy\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Y_logits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy) * 100.0\n",
    "\n",
    "# accuracy of the trained model [0, 1] ([worst, best])\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# training (lr decays from 0.003 to 0.0001)\n",
    "step = tf.placeholder(tf.int32)\n",
    "learn_rate = 0.0001 + tf.train.exponential_decay(0.003, step, 2000, 1 / math.e)\n",
    "train_step = tf.train.AdamOptimizer(learn_rate).minimize(cross_entropy)\n",
    "\n",
    "# initialization\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# call in loop to train the model 100 images at a time\n",
    "max_test_accuracy = 0.0\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    # train on batches of 100 images w/ 100 labels\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    # compute training values for visualisation\n",
    "    if update_train_data:\n",
    "        a, c, l = sess.run([accuracy, cross_entropy, learn_rate],\n",
    "                           feed_dict={X: batch_X, Y_: batch_Y, step: i})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c) + \" lr:\" + str(l))\n",
    "    # compute test values for visualization\n",
    "    if update_test_data:\n",
    "        global max_test_accuracy\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: mnist.test.images, Y_: mnist.test.labels})\n",
    "        if (a > max_test_accuracy):\n",
    "            max_test_accuracy = a\n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \n",
    "              \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        \n",
    "    # back-propogation training step\n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y, step: i})\n",
    "\n",
    "# text visualization of process\n",
    "for i in range(10000+1):\n",
    "    training_step(i, i % 50 == 0, i % 10 == 0)\n",
    "    \n",
    "# display the final max accuracy\n",
    "print(\"max test accuracy: \" + str(max_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout and Overfitting\n",
    "The following cell contains code modifications to take dropout and overfitting concepts into account. Instead of only improving the training cross entropy, the test cross entropy is taken into account as well. This produces a result with an accuracy of ~98.4%.\n",
    "\n",
    "## Overfitting\n",
    "This occurs in very high data iterations, and it is typically a sign that the training is no longer having a positive effect on recognition of test or real world data. It overfits to the reduction of training data cross entropy. It can be resolved through a regularization technique called dropout.\n",
    "\n",
    "## Dropout\n",
    "In dropout, some percentage of neurons are randomly dropped out of the network. The probability a neuron is to remain is referred to as \"pkeep.\" Values of 50% to 75% are typical values for pkeep. The output of the remaining neurons are boosted to ensure no layer shifts occur. In performance testing, pkeep is simply set to a value of 1. Tensorflow has a dropout function for use in the output layer of neurons. It randomly zeros out some outputs and boosts the remaining ones by 1/pkeep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input X => 28x28 grayscale dimensions ('None' indexes images in mini-batch)\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "\n",
    "# correct answers\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights\n",
    "W1 = tf.Variable(tf.truncated_normal([28*28, 200], stddev=0.1))\n",
    "W2 = tf.Variable(tf.truncated_normal([200, 100], stddev=0.1))\n",
    "W3 = tf.Variable(tf.truncated_normal([100, 10], stddev=0.1))\n",
    "\n",
    "# biases\n",
    "b1 = tf.Variable(tf.ones([200]) / 10)\n",
    "b2 = tf.Variable(tf.ones([100]) / 10)\n",
    "b3 = tf.Variable(tf.ones([10]) / 10)\n",
    "\n",
    "# flatten images into single line\n",
    "XX = tf.reshape(X, [-1, 28*28])\n",
    "\n",
    "# probability neuron stays in network (0.75 for training, 1 for testing)\n",
    "test_keep = 1\n",
    "train_keep = 0.75\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "# model\n",
    "Y1 = tf.nn.relu(tf.matmul(XX, W1) + b1)\n",
    "Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1d, W2) + b2)\n",
    "Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "Y_logits = tf.matmul(Y2d, W3) + b3\n",
    "Y = tf.nn.softmax(Y_logits)\n",
    "\n",
    "# cross entropy\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Y_logits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy) * 100.0\n",
    "\n",
    "# accuracy of the trained model [0, 1] ([worst, best])\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# matplotlib visualisation\n",
    "all_weights = tf.concat([tf.reshape(W1, [-1]), tf.reshape(W2, [-1]), tf.reshape(W3, [-1])], 0)\n",
    "all_biases  = tf.concat([tf.reshape(b1, [-1]), tf.reshape(b2, [-1]), tf.reshape(b3, [-1])], 0)\n",
    "I = tensorflowvisu.tf_format_mnist_images(X, Y, Y_)\n",
    "It = tensorflowvisu.tf_format_mnist_images(X, Y, Y_, 1000, lines=25)\n",
    "datavis = tensorflowvisu.MnistDataVis()\n",
    "\n",
    "# training (lr decays from 0.003 to 0.0001)\n",
    "step = tf.placeholder(tf.int32)\n",
    "learn_rate = 0.0001 + tf.train.exponential_decay(0.003, step, 2000, 1 / math.e)\n",
    "train_step = tf.train.AdamOptimizer(learn_rate).minimize(cross_entropy)\n",
    "\n",
    "# initialization\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# call in loop to train the model 100 images at a time\n",
    "#max_test_accuracy = 0.0\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    # train on batches of 100 images w/ 100 labels\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    # compute training values for visualisation\n",
    "    if update_train_data:\n",
    "        a, c, im, w, b, l = sess.run([accuracy, cross_entropy, I, all_weights, all_biases, learn_rate],\n",
    "                           feed_dict={X: batch_X, Y_: batch_Y, pkeep: test_keep, step: i})\n",
    "        #print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c) + \" lr:\" + str(l))\n",
    "        datavis.append_training_curves_data(i, a, c)\n",
    "        datavis.update_image1(im)\n",
    "        datavis.append_data_histograms(i, w, b)\n",
    "    # compute test values for visualization\n",
    "    if update_test_data:\n",
    "        #global max_test_accuracy\n",
    "        a, c, im = sess.run([accuracy, cross_entropy, It],\n",
    "                        feed_dict={X: mnist.test.images, Y_: mnist.test.labels, pkeep: test_keep})\n",
    "        #if (a > max_test_accuracy):\n",
    "        #    max_test_accuracy = a\n",
    "        #print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \n",
    "        #      \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        datavis.append_test_curves_data(i, a, c)\n",
    "        datavis.update_image2(im)\n",
    "        \n",
    "    # back-propogation training step\n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y, pkeep: train_keep, step: i})\n",
    "\n",
    "# text visualization of process\n",
    "#for i in range(10000+1):\n",
    "#    training_step(i, i % 50 == 0, i % 10 == 0)\n",
    "datavis.animate(training_step, iterations=10000+1, train_data_update_freq=40,\n",
    "                test_data_update_freq=200, more_tests_at_start=True)\n",
    "    \n",
    "# display the final max accuracy\n",
    "#print(\"max test accuracy: \" + str(max_test_accuracy))\n",
    "print(\"max test accuracy: \" + str(datavis.get_max_test_accuracy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notes\n",
    "Neural net above is successful, but 98% can't be broken in any significant way. Overfitting is also still a problem, even after changing the learning technique of the NN through dropout and learning rate modifications. The N.N. still learns poorly. The N.N. is not capable (in its present shape) of improving its performance (this is a conclusion drawn after modifying the N.N. through degrees of freedom constraints, dropout application, and training on large amounts of data). Convolutional networks would work better for this case, as the images above had to be flattened into a single row (1-D). Convolutional N.N. allow for the retention of dimensions. These will be explored in the next project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
